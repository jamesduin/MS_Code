%%
%% This is file `thesis-test.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% nuthesis.dtx  (with options: `thesis-test')
%% 

\documentclass[ms]{nuthesis}
%% Needed to typset the math in this sample
\usepackage{amsmath}
\usepackage{amsfonts}
%% Let's use a different font
\usepackage[sc,osf]{mathpazo}

%% prevents placing floats before a section
\usepackage[section]{placeins}

%% Makes things look better
\usepackage{microtype}

%% Makes things look better
\usepackage{booktabs}

%% Gives us extra list environments
\usepackage{paralist}

%% Be able to include graphicsx
\usepackage{graphicx}

%% I like darker colors
\usepackage{color}
\definecolor{dark-red}{rgb}{0.6,0,0}
\definecolor{dark-green}{rgb}{0,0.6,0}
\definecolor{dark-blue}{rgb}{0,0,0.6}

%% If you use hyperref, you need to load memhfixc *after* it.
%% See the memoir docs for details.
\usepackage[%
pdfauthor={James D. Duin},
pdftitle={Hierachical Active Learning Bioinformatics Application},
pdfsubject={BioDataset},
pdfkeywords={semi-supervised learning, active learning, hierarchical labeling, cost analysis},
linkcolor=dark-blue,
pagecolor=dark-green,
citecolor=dark-blue,
urlcolor=dark-red,
%colorlinks=true,
backref,
plainpages=false,% This helps to fix the issue with hyperref with page numbering
pdfpagelabels% This helps to fix the issue with hyperref with page numbering
]{hyperref}

%% Needed by memoir to fix things with hyperref
\usepackage{memhfixc}
\begin{document}
%% Start formating the first few special pages
%% frontmatter is needed to set the page numbering correctly
\frontmatter

\title{Hierachical Active Learning Bioinformatics Application}
\author{James D. Duin}
\adviser{Professor Stephen Scott}
\adviserAbstract{Stephen Scott}
\major{Computer Science}
\degreemonth{February}
\degreeyear{2017}
%%
%% For most people the defaults will be correct, so they are commented
%% out. To manually set these, just uncomment and make the needed
%% changes.
%% \college{Your college}
%% \city{Your City}
%%
%% For most people the following can be changed with a class
%% option. To manually set these, just uncomment the following and
%% make the needed changes.
%% \doctype{Thesis or Dissertation}
%% \degree{Your degree}
%% \degreeabbreviation{Your degree abbr.}
%%
%% Now that we know everything we need, we can generate the title page
%% itself.
%%
\maketitle
%% You have a maximum of 350, which includes your title, name, etc.
\begin{abstract}
  \par I made the classifiers to predict the BioDataset, I made the
  plots for the Active Passive curves, I made the plots for the Fixed
  Fine Ratio experiments with various costs.
\end{abstract}

%% Optional
%\begin{copyrightpage}
%This file may be distributed and/or modified under the conditions of
%the \LaTeX{} Project Public License, either version 1.3c of this license
%or (at your option) any later version.  The latest version of this
%license is in:
%\begin{center}
%   \url{http://www.latex-project.org/lppl.txt}
%\end{center}
%and version 1.3c or later is part of all distributions of \LaTeX version
%2006/05/20 or later.
%\end{copyrightpage}

%% Optional
\begin{dedication}
  This thesis is dedicated to my parents Paul and Vicki Duin and fiancee Anna Spady.
\end{dedication}

%% Optional
\begin{acknowledgments}
  I would like to thank my advisor Dr. Stephen Scott for guidance in selecting this
  research topic, Yugi Mo for his work in developing the HAL methodology, and Dr.
  Douglas Downey for his work on this topic. I would like to thank Juan Cui, Jiang Shu,
  Kevin Chiang for assistance accessing and understanding the protein dataset that is
  the subject of the paper.
\end{acknowledgments}

%% Optional
%\begin{grantinfo}
%  I'm not funded by any grants.
%\end{grantinfo}
%%% The ToC is required
%%% Uncomment these if need be

%% The ToC is required
\tableofcontents
%% Uncomment these if need be
%\listoffigures
%\listoftables

%%   mainmatter is needed after the ToC, (LoF, and LoT) to set the
%%   page numbering correctly for the main body
\mainmatter

%% Thesis goes here
\chapter{Introduction}\label{chap:aenied}
\section{Machine Learning}
\par Machine Learning algorithms are defined as computer programs that learn from experience E
with respect to some class of tasks T and performance measure P, if their performance at
tasks in T, as measured by P, improves with experience E [4]. In the context of this paper,
the machine learning algorithm that is used is a support vector machine (SVM) implementation
by libSvm [3]. The performance measure is the classification of protein instances according to a
label, for example, originated in the mitochondria or not.  The experience is the number of
training instances in the training set. The dataset is initially partitioned into training
and test sets. The algorithm improves its classifier structure based on the features of
each instance by iterating through all instances in the training set. When training has
been completed, the classifier is then tested on the test set and the number of instances
that the classifier correctly or incorrectly labels determines its accuracy, precision
and recall scores. In our protein dataset there are 20,098 proteins with 449 features each
relating to their structure. In our experiments an SVM classifier is applied to the dataset
with the goal of achieving high precision scores for the label “mitochondrion”, that is,
if the protein originates in the mitochondria or not.


\section{Hierarchical Bioinformatics Data Set}
\par The protein dataset is labeled according to where it originates in the cell.
At the root is “mitochondrion”, then there is the sub level labels for if its native
to the mitochondria or if it has a separate target compartment specification. The complete
tree along with the number of instances belonging to the each label is included in \textit{Figure} \ref{fig:Mito_tree}.
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.25\columnwidth]{fig/Mito_tree}
    \label{fig:Mito_tree}
    \caption{The protein dataset is labeled according to where it originates in the cell. At the
    root is “mitochondrion”, then there is the sub level labels for if its native to the mitochondria
    or if it has a separate target compartment specification. The complete tree along with the number
    of instances belonging to the each label is included in \ref{fig:Mito_tree}.}
\end{figure}
\FloatBarrier
\section{Coarse Grained vs Fine Grained Trade Off}
\par The classifier that does not take advantage of any of the fine grained labels works off of the root
labels for each instance and does not train separate classifiers for the fine grained labels. This
classifier is referred to as the coarse grained classifier. The classifier that does use fine grained
labels, and trains a separate classifier for each label, then combines them to generate a root level
label is referred to as the fine grained classifier. It can be demonstrated through a dummy example
that for certain datasets, a fine grained approach to the root level classifier can achieve higher
levels of precision for the same level of recall. Such a dataset is shown in \ref{fig:union}. The
classifiers for this dataset can be thought of as a function of axis parallel rectangular boxes.
For the course grained to have high recall and return all of the positive circle instances, it must
encompass the entire dataset and incidentally return all of the negative diamond instances as
positive also. A fine grained approach is preferable for the dummy dataset pictured. It is the
intention of this study to demonstrate that the fine grained classification approach for a root
level classifier will achieve higher levels of precision for the same level of recall when
applied to the protein dataset.
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.25\columnwidth]{fig/union}
    \label{fig:union}
    \caption{Demonstration of a dataset that would benefit from multiple fine grained
    learners for each circle type. In order for the coarse grain learner to have high
    recall, precision must be scarified and a large amount of false positives returned.
    By combining fine grained classifiers the same level of recall can be achieved with a
    higher level of precision because none of the false positive diamonds will be returned}
\end{figure}
\FloatBarrier


\chapter{Background and Related Work}
\section{Active Learning}
\par Active Learning relates to the coarse grained vs fine grained tradeoff because it is
reasonable to assume that fine grained labels may not be as readily available as coarse grained
labels, and thus have a higher cost. An active learning approach is used to determine how
many fine grained labels to purchase in order to minimize the total cost to train the
algorithm an maximize the precision and recall scores. The following equations for
precision, recall, and a weighted F score are shown below in equations 1 through 3.
\break
$Precision eqn$ \break
$Recall eqn$ \break
$F0_5 eqn$ \break
\break
\par The goal in an active learning approach is to maximize the F measure where β equals 0.5 [2].
The F-0.5 measure gives more weight to precision, as opposed to recall, so it gives
incentive to purchase enough fine grained labels to increase the F-0.5 measure. The
coarse grained labels will cost less than fine grained labels, but the increase in
the F-0.5 measure justifies the increase in cost up to a certain point. The F-0.5
measure is used in the results section of this paper. ... That's why we use PRauc in the results.

\section{Other Papers cited by Yugi}
Describe some of the other papers that Yugi cited.

\section{Hierarchical Active Learning}
\par The Hierarchical Active Learning algorithm (HAL) is shown diagrammatically in Figure 3.
Multiple fine grained classifiers are trained at each level of the Hierarchy of the dataset.
Queries to the oracle are performed purchase the most cost effective labels to add to the
training sets of the classifiers. The active learning cycle continues until a cost budget has
been reached. The benefit of an active learning approach is to maximize the F-0.5 measure
for a given cost budget. It was the goal of this study to apply the HAL algorithm to the
protein dataset, however this is not achieved at this time. An existing application of
HAL is briefly discussed in the following section.
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/AL2}
    \label{fig:AL2}
    \caption{Diagram of HAL approach}
\end{figure}
\FloatBarrier



\section{Application to Dispatch Dataset}
\par HAL was applied to a Dispatch dataset. This dataset contains 375,026 manually labeled
hierarchical names across 1,384 newspaper articles [2]. This is a clear example where
fine grained labels have a higher cost since it is easier for a person to manually
determine if the article pertains to an organization or not, rather than if it pertains
to a railroad or a zoo, which would be sub labels of the organization root. The first
analysis step was to determine that the F-0.5 measure is increased by using fine
grained classifiers. The results are shown in Figure 4. The highest F-0.5 measure for
a given iteration of purchases of training instances is obtained by using the active
learning approach with all fine-grained labels. The passive learning curves were
 generated by selecting batches of instances randomly rather than querying the
 oracle for a specific label type that offers that most gain in classifier accuracy.
 The active learning curves did take advantage of querying for specific labels in
 order to maximize gain in classifier accuracy. ... add other data sets.
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.25\columnwidth]{fig/draft-richmond}
    \label{fig:draft-richmond}
    \caption{Application of HAL demonstrating the benefit of Actively selecting the type
    of labels to purchase for instances rather than randomly selecting labels to purchase,
    as in the Passive curves.}
\end{figure}
\FloatBarrier
The next analysis step is to apply a given ratio of fine grained vs coarse grained
labels to purchase at each batch request to the oracle. The results of varying the
percentage of fine grained labels purchased are shown in Figure 5. The figure shows
that even a small amount of fine grained labels purchased, that is, 20perc provides a
significant increase in the F-0.5 measure for a given iteration. ...  pr auc results.
Add explanation of Bandit approach and results.



\chapter{Bio HAL Application}\label{chap:math}
\section{Separation of training sets}
\section{Training and Testing Coarse Grain and Fine Grain Classifiers}
\section{Passive SVM Rbf kernel vs Logistic Reg}
add text'example cite'\cite{Merialdo2001}


\section{Active vs Passive curves}
\par The following plots were obtained with a round batch size of 100
and a starter set of 1040 instances out of the total 2098 instances.
The plots are the average of 10 folds, for each fold a test set of 2010
containing representatives of each class was held out, out of the remaining
18088, the starter set was selected which again contained representatives of
each class. Coarse and fine classifiers share the same starter set. During
 each round coarse and fine classifiers are trained on their corresponding
 sets, metrics are outputted on the held out test set, then confidence estimates
 are ran on the remaining eligible instances. Eligible instances are kept in
 separate sets for coarse and fine, 100 of the most uncertain instances are removed
 from each eligible set and added to its corresponding coarse or fine set to be
 trained on for the next round.
\FloatBarrier
\subsection{Plots for Logistic Regression Active vs Passive curves}
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassivePRLR}
    \label{fig:ActiveVsPassivePRLR}
    \caption{The PR AUC curves for rounds with the Logistic
Regression classifier conforms to expectations, with active-fine having
the highest performance. Active-coarse outperforms passive-coarse. Passive-fine
doesn't outperform the coarse classifiers until rnd 100. }
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassiveROCLR}
    \label{fig:ActiveVsPassiveROCLR}
    \caption{The ROC AUC curves for rounds with the
Logistic Regression classifier. The active curves beat out the passive
curves for both coarse and fine. Coarse roc starts with an advantage over
fine as in the PR curves. Both converge to the same rate after roc auc level after 80.}
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassiveAccLR}
    \label{fig:ActiveVsPassiveAccLR}
    \caption{The accuracy of the fine classifiers stays at
roughly the same rate throughout the rounds, this is due to an effective
weighting scheme for the fine grained classifiers. The active coarse accuracy
drops towards the end due to an increase in false positives as more negative
instances are added in the later rounds.}
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassiveF1LR}
    \label{fig:ActiveVsPassiveF1LR}
    \caption{The F-measure of the the fine classifiers increases
throughout the rounds as more true positives are predicted. The active coarse
again decreases at later rounds due to increased false positives.}
\end{figure}

\FloatBarrier
\subsection{Plots for SVM Active vs Passive curves}
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassivePRSVM}
    \label{fig:ActiveVsPassivePRSVM}
    \caption{The PR AUC curves for rounds with SVM show little
advantage for fine. The results are slightly different than the ones shown
on 2/14 due to fixing a bug with the code that wasn't performing the
preprocessing scaling for the SVM case at the same stage as it was being
done for the logistic regression classifier.}
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassiveROCSVM}
    \label{fig:ActiveVsPassiveROCSVM}
    \caption{The ROC curves show more of an advantage for coarse classifiers.}
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassiveAccSVM}
    \label{fig:ActiveVsPassiveAccSVM}
    \caption{The accuracy for the coarse decreases sharply due
to coarse predicting steadily more false positives, behaving similar to the
Log Reg case. Fine accuracy is higher due to predicting less false positives
than coarse. Fine also predicts less true positives, compare apx. 37 to apx.
60 t.p. for coarse at round 60.}
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/ActiveVsPassiveF1SVM}
    \label{fig:ActiveVsPassiveF1SVM}
    \caption{The F-measure favors coarse, and trends to the same level
    for both coarse and fine.}
\end{figure}

\FloatBarrier
\section{Plots for FFR experiments}
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/FFR_PR_Cost1}
    \label{fig:FFR_PR_Cost1}
    \caption{The round size is changed to 160 and a fine has a cost of 1.
The 0p5 round for instance, corresponds to 0.5 of the total budget being used on fine,
so 80 goes to fine and 80 goes to coarse. For the 0p0 round, none of the budget is
used for fine and it has the worst performance. After around 0p3 the gains in performance
are marginal. The performance increases with the green 1p0 curve outperforming the
rest. The results are an average of 10 folds.}
\end{figure}

\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/FFR_PR_Cost16}
    \label{fig:FFR_PR_Cost16}
    \caption{The round size is again 160 and fine has a cost of 16. The worst
    performing curve is again 0p0 with no fine instances, but the benefit of fine
    instances is marginal. For the 0p5 case 5 instances are purchased for fine and
    80 are purchased for coarse. For the 1p0 case 16 instances are purchase for
    fine and 144 instances are purchased for coarse.}
\end{figure}
\FloatBarrier






\chapter{Conclusions and Future Work}\label{chap:math}
\par I should probably do the Bandit experiments.










%% backmatter is needed at the end of the main body of your thesis to
%% set up page numbering correctly for the remainder of the thesis
\backmatter

%% Start the correct formatting for the appendices
\appendix

\chapter{Tuning the fine grained classes}
\par All the data and results for tuning the fine grained classes.

%% Bibliography goes here (You better have one)
%% BibTeX is your friend
\bibliographystyle{plain}
\bibliography{nuthesis}
%% Pull in all the entries in the bibtex file. Is is a useful trick to
%% check all your references.
\nocite{*}


%% Index go here (if you have one)

\end{document}
\endinput
%%
%% End of file `thesis-test.tex'.
