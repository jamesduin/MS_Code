%%
%% This is file `thesis-test.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% nuthesis.dtx  (with options: `thesis-test')
%% 

\documentclass[ms]{nuthesis}
%% Needed to typset the math in this sample
\usepackage{amsmath}
\usepackage{amsfonts}
%% Let's use a different font
\usepackage[sc,osf]{mathpazo}

%% prevents placing floats before a section
\usepackage[section]{placeins}

%% Makes things look better
\usepackage{microtype}

%% Makes things look better
\usepackage{booktabs}

%% Gives us extra list environments
\usepackage{paralist}

%% Be able to include graphicsx
\usepackage{graphicx}

%% Be able to use subfigure
\usepackage{caption}
\usepackage{subcaption}

%% I like darker colors
\usepackage{color}
\definecolor{dark-red}{rgb}{0.6,0,0}
\definecolor{dark-green}{rgb}{0,0.6,0}
\definecolor{dark-blue}{rgb}{0,0,0.6}

%% If you use hyperref, you need to load memhfixc *after* it.
%% See the memoir docs for details.
\usepackage[%
pdfauthor={James D. Duin},
pdftitle={Hierachical Active Learning Bioinformatics Application},
pdfsubject={BioDataset},
pdfkeywords={semi-supervised learning, active learning, hierarchical labeling, cost analysis},
linkcolor=dark-blue,
pagecolor=dark-green,
citecolor=dark-blue,
urlcolor=dark-red,
%colorlinks=true,
backref,
plainpages=false,% This helps to fix the issue with hyperref with page numbering
pdfpagelabels% This helps to fix the issue with hyperref with page numbering
]{hyperref}

%% Needed by memoir to fix things with hyperref
\usepackage{memhfixc}
\begin{document}
%% Start formating the first few special pages
%% frontmatter is needed to set the page numbering correctly
\frontmatter

\title{Hierachical Active Learning Bioinformatics Application}
\author{James D. Duin}
\adviser{Professor Stephen Scott}
\adviserAbstract{Stephen Scott}
\major{Computer Science}
\degreemonth{February}
\degreeyear{2017}
%%
%% For most people the defaults will be correct, so they are commented
%% out. To manually set these, just uncomment and make the needed
%% changes.
%% \college{Your college}
%% \city{Your City}
%%
%% For most people the following can be changed with a class
%% option. To manually set these, just uncomment the following and
%% make the needed changes.
%% \doctype{Thesis or Dissertation}
%% \degree{Your degree}
%% \degreeabbreviation{Your degree abbr.}
%%
%% Now that we know everything we need, we can generate the title page
%% itself.
%%
\maketitle
%% You have a maximum of 350, which includes your title, name, etc.
\begin{abstract}
  \par I made the classifiers to predict the BioDataset, I made the
  plots for the Active Passive curves, I made the plots for the Fixed
  Fine Ratio experiments with various costs.
\end{abstract}

%% Optional
%\begin{copyrightpage}
%This file may be distributed and/or modified under the conditions of
%the \LaTeX{} Project Public License, either version 1.3c of this license
%or (at your option) any later version.  The latest version of this
%license is in:
%\begin{center}
%   \url{http://www.latex-project.org/lppl.txt}
%\end{center}
%and version 1.3c or later is part of all distributions of \LaTeX version
%2006/05/20 or later.
%\end{copyrightpage}

%% Optional
\begin{dedication}
  This thesis is dedicated to my parents Paul and Vicki Duin and fiancee Anna Spady.
\end{dedication}

%% Optional
\begin{acknowledgments}
  I would like to thank my advisor Dr. Stephen Scott for guidance in selecting this
  research topic, Yugi Mo for his work in developing the HAL methodology, and Dr.
  Douglas Downey for his work on this topic. I would like to thank Juan Cui, Jiang Shu,
  Kevin Chiang for assistance accessing and understanding the protein dataset that is
  the subject of the paper.
\end{acknowledgments}

%% Optional
%\begin{grantinfo}
%  I'm not funded by any grants.
%\end{grantinfo}
%%% The ToC is required
%%% Uncomment these if need be

%% The ToC is required
\tableofcontents
%% Uncomment these if need be
%\listoffigures
%\listoftables

%%   mainmatter is needed after the ToC, (LoF, and LoT) to set the
%%   page numbering correctly for the main body
\mainmatter

%% Thesis goes here
\chapter{Introduction}\label{chap:aenied}
\section{Machine Learning}
\par Machine Learning algorithms are defined as computer programs that learn from experience E
with respect to some class of tasks T and performance measure P, if their performance at
tasks in T, as measured by P, improves with experience E [4]. In the context of this paper,
the machine learning algorithm that is used is a support vector machine (SVM) implementation
by libSvm [3]. The performance measure is the classification of protein instances according to a
label, for example, originated in the mitochondria or not.  The experience is the number of
training instances in the training set. The dataset is initially partitioned into training
and test sets. The algorithm improves its classifier structure based on the features of
each instance by iterating through all instances in the training set. When training has
been completed, the classifier is then tested on the test set and the number of instances
that the classifier correctly or incorrectly labels determines its accuracy, precision
and recall scores. In our protein dataset there are 20,098 proteins with 449 features each
relating to their structure. In our experiments an SVM classifier is applied to the dataset
with the goal of achieving high precision scores for the label “mitochondrion”, that is,
if the protein originates in the mitochondria or not.


\section{Hierarchical Bioinformatics Data Set}
\par The protein dataset is labeled according to where it originates in the cell.
At the root is “mitochondrion”, then there is the sub level labels for if its native
to the mitochondria or if it has a separate target compartment specification. The complete
tree along with the number of instances belonging to the each label is included in \textit{Figure} \ref{fig:Mito_tree}.

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/Mito_tree}
    \label{fig:Mito_tree}
    \caption{The protein dataset is labeled according to where it originates in the cell. At the
    root is “mitochondrion”, then there is the sub level labels for if its native to the mitochondria
    or if it has a separate target compartment specification. The complete tree along with the number
    of instances belonging to the each label is included in \ref{fig:Mito_tree}.}
\end{figure}
\FloatBarrier

\section{Coarse Grained vs Fine Grained Trade Off}
\par The classifier that does not take advantage of any of the fine grained labels works off of the root
labels for each instance and does not train separate classifiers for the fine grained labels. This
classifier is referred to as the coarse grained classifier. The classifier that does use fine grained
labels, and trains a separate classifier for each label, then combines them to generate a root level
label is referred to as the fine grained classifier. It can be demonstrated through a dummy example
that for certain datasets, a fine grained approach to the root level classifier can achieve higher
levels of precision for the same level of recall. Such a dataset is shown in \ref{fig:union}. The
classifiers for this dataset can be thought of as a function of axis parallel rectangular boxes.
For the course grained to have high recall and return all of the positive circle instances, it must
encompass the entire dataset and incidentally return all of the negative diamond instances as
positive also. A fine grained approach is preferable for the dummy dataset pictured. It is the
intention of this study to demonstrate that the fine grained classification approach for a root
level classifier will achieve higher levels of precision for the same level of recall when
applied to the protein dataset.

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/union}
    \label{fig:union}
    \caption{Demonstration of a dataset that would benefit from multiple fine grained
    learners for each circle type. In order for the coarse grain learner to have high
    recall, precision must be scarified and a large amount of false positives returned.
    By combining fine grained classifiers the same level of recall can be achieved with a
    higher level of precision because none of the false positive diamonds will be returned}
\end{figure}
\FloatBarrier


\chapter{Background and Related Work}
\section{Active Learning}
\par Active Learning relates to the coarse grained vs fine grained tradeoff because it is
reasonable to assume that fine grained labels may not be as readily available as coarse grained
labels, and thus have a higher cost. An active learning approach is used to determine how
many fine grained labels to purchase in order to minimize the total cost to train the
algorithm an maximize the precision and recall scores. The following equations for
precision, recall, and a weighted F score are shown below in equations 1 through 3.
\break
$Precision eqn$ \break
$Recall eqn$ \break
$F0_5 eqn$ \break
\break
\par The goal in an active learning approach is to maximize the F measure where β equals 0.5 [2].
The F-0.5 measure gives more weight to precision, as opposed to recall, so it gives
incentive to purchase enough fine grained labels to increase the F-0.5 measure. The
coarse grained labels will cost less than fine grained labels, but the increase in
the F-0.5 measure justifies the increase in cost up to a certain point. The F-0.5
measure is used in the results section of this paper. ... That's why we use PRauc in the results.

\section{Other Papers cited by Yugi}
Describe some of the other papers that Yugi cited.

\section{Hierarchical Active Learning}
\par The Hierarchical Active Learning algorithm (HAL) is shown diagrammatically in Figure 3.
Multiple fine grained classifiers are trained at each level of the Hierarchy of the dataset.
Queries to the oracle are performed purchase the most cost effective labels to add to the
training sets of the classifiers. The active learning cycle continues until a cost budget has
been reached. The benefit of an active learning approach is to maximize the F-0.5 measure
for a given cost budget. It was the goal of this study to apply the HAL algorithm to the
protein dataset, however this is not achieved at this time. An existing application of
HAL is briefly discussed in the following section.


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=0.5\columnwidth]{fig/AL2}
    \label{fig:AL2}
    \caption{Diagram of HAL approach}
\end{figure}
\FloatBarrier



\section{Application to Dispatch Dataset}
\par HAL was applied to a Dispatch dataset. This dataset contains 375,026 manually labeled
hierarchical names across 1,384 newspaper articles [2]. This is a clear example where
fine grained labels have a higher cost since it is easier for a person to manually
determine if the article pertains to an organization or not, rather than if it pertains
to a railroad or a zoo, which would be sub labels of the organization root. The first
analysis step was to determine that the F-0.5 measure is increased by using fine
grained classifiers. The results are shown in Figure 4. The highest F-0.5 measure for
a given iteration of purchases of training instances is obtained by using the active
learning approach with all fine-grained labels. The passive learning curves were
 generated by selecting batches of instances randomly rather than querying the
 oracle for a specific label type that offers that most gain in classifier accuracy.
 The active learning curves did take advantage of querying for specific labels in
 order to maximize gain in classifier accuracy. ... add other data sets.

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/draft-richmond}
    \label{fig:draft-richmond}
    \caption{Application of HAL demonstrating the benefit of Actively selecting the type
    of labels to purchase for instances rather than randomly selecting labels to purchase,
    as in the Passive curves.}
\end{figure}
\FloatBarrier


\par The next analysis step is to apply a given ratio of fine grained vs coarse grained
labels to purchase at each batch request to the oracle. The results of varying the
percentage of fine grained labels purchased are shown in Figure 5. The figure shows
that even a small amount of fine grained labels purchased, that is, 20perc provides a
significant increase in the F-0.5 measure for a given iteration. ...  pr auc results.
Add explanation of Bandit approach and results.



\chapter{Bio HAL Application}\label{chap:math}
\section{Training and Testing Coarse Grain and Fine Grain Classifiers}
\par The first step is to examine the dataset.



\FloatBarrier
\begin{table}[!htb]
\begin{subtable}{.15\linewidth}
  \centering
  \begin{tabular}{|l||l|}\toprule
    Classes & All \\ \midrule
    0 & 19136 \\
    1 & 13 \\
    2 & 185 \\
    3 & 324 \\
    4 & 190 \\
    5 & 11 \\
    6 & 104 \\
    7 & 59 \\
    8 & 76 \\
    Total & 20098 \\
    Shape & 450 \\
 \bottomrule
  \end{tabular}
  \label{tab:ClassesAll}
  \caption{Classes}
\end{subtable}%
\begin{subtable}{.85\linewidth}
\centering
  \begin{tabular}{|l||l||l||l||l||l||l||l||l||l||l|}\toprule
    All & Folds & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\ \midrule
    1 & 2010 & 1914 & 1 & 19 & 32 & 19 & 1 & 11 & 6 & 7 \\
    2 & 2010 & 1914 & 1 & 19 & 32 & 19 & 1 & 11 & 6 & 7 \\
    3 & 2010 & 1914 & 1 & 19 & 32 & 19 & 1 & 11 & 5 & 8 \\
    4 & 2010 & 1914 & 1 & 19 & 32 & 19 & 1 & 10 & 6 & 8 \\
    5 & 2010 & 1914 & 1 & 18 & 33 & 19 & 1 & 10 & 6 & 8 \\
    6 & 2010 & 1914 & 1 & 18 & 33 & 19 & 1 & 10 & 6 & 8 \\
    7 & 2010 & 1913 & 2 & 18 & 33 & 19 & 1 & 10 & 6 & 8 \\
    8 & 2010 & 1913 & 2 & 18 & 33 & 19 & 1 & 10 & 6 & 8 \\
    9 & 2009 & 1913 & 2 & 18 & 32 & 19 & 2 & 10 & 6 & 7 \\
    10 & 2009 & 1913 & 1 & 19 & 32 & 19 & 1 & 11 & 6 & 7 \\ \midrule
    Total & 20098 & 19136 & 13 & 185 & 324 & 190 & 11 & 104 & 59 & 76 \\
 \bottomrule
  \end{tabular}
  \label{tab:partitions}
  \caption{Folds}
  \end{subtable}
  \caption{This is what the dataset looks like there are 20098 instances total with 450 features each. This is what the folds of the dataset look like.}
\end{table}
\FloatBarrier








\par Next the dataset is partitioned into 10 folds, each fold contains a representative proportion
of each of the classes, the instances are added to each partition at random. The total partitioning
looks like Table \ref{tab:partitions}


\par Then 9 of the folds are compressed into the test set and the fold held out
is the test set the totals of each class in the train and test set for fold 1 is shown in
Table \ref{tab:TrainTest}.


\FloatBarrier
\begin{table}[h]
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l||l||l||l|}\toprule
    train & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    Total & 18088 & 17222 & 12 & 166 & 292 & 171 & 10 & 93 & 53 & 69 \\ \midrule
    test & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
    Total & 2010 & 1914 & 1 & 19 & 32 & 19 & 1 & 11 & 6 & 7 \\
 \bottomrule
  \end{tabular}
  \caption{This is what the train and test set look like.}
  \label{tab:TrainTest}
\end{table}
\FloatBarrier

\par coarse just used marked all positives as 1 and ran a binary classifer.
\par fine grained classifiers trained 8 separate classifiers for the 8 fine grained
classes, so for fine grain classifier for 1, all other fine grained classes are marked as 0
in addition to all the coarse instances being marked as 0.
\par Also the Pr auc and Roc auc are the primary metrics for determining the performance
of the classifier.
\par The next step is to determine what classifier can be applied to 'learn' the classes of
this dataset.
\par Because the experiment will involve running multiple rounds with increasing the instances to
 be trained on iteratively I tested each classifier against the full dataset and then a reduced
 dataset with one fifth of the negative instances.



\par I tried using SVM. Throughout this project I used the python library sci-kit learn \cite{scikit-learn}.
The support vector machine implemented by this library has the following default parameters. SVC C=1.0,
kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache-size=200,
class-weight=None, verbose=False, max-iter=-1, decision-function-shape=None, random-state=None.
\par I tried different scaling methods (min max scaler, std scaler), I settled on std scaler.



\par I tried different feature select measures, I settled on 75 perc feature select.



\par I tried different different C costs, kernels, decision function shape, gamma, tolerance
settled on classif = svm.SVC(C=1.0, kernel='rbf',decisionfunctionshape='ovo',gamma=0.0025,
 tol=0.00001)



\par I left the class weight as balanced for this part, the results did not show much advantage
for using the fine grained classifier.


\par next I tried using a Logistic regression classifier.
\par I tried different scaling methods, min max scaler, std scaler, I settled on min max scaler.
\par I tried different feature select measures, decided to use all of the features.
\par I tried different C costs, tolerances, and class weights. I settled on C=0.1, tol = 0.00001,
and weight equal to the balanced, adjusted via a scaling line.
\par I also further tuned the fine grained classifiers starting from the initial scaling from the
line an then multiplying that by a ratio.
I got this vector of ratios
[0.87, 0.4, 0.78, 0.65, 3.48, 0.78, 1.74, 0.87]

\break

\section{Passive SVM Rbf kernel vs Logistic Reg}
\par This shows the advantage to fine grained labels to justify the experiment.
\FloatBarrier
\begin{table}[h]
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
    coarse-pr & fine-pr & coarse-roc & fine-roc & coarse-acc & fine-acc & coarse-f1 & fine-f1 \\ \midrule
    0.898 & 0.901 & 0.905 & 0.896 & 0.767 & 0.945 & 0.259 & 0.474 \\
    0.870 & 0.869 & 0.847 & 0.846 & 0.803 & 0.944 & 0.272 & 0.456 \\
    0.897 & 0.907 & 0.895 & 0.901 & 0.792 & 0.947 & 0.287 & 0.500 \\
    0.864 & 0.866 & 0.852 & 0.848 & 0.778 & 0.943 & 0.256 & 0.430 \\
    0.855 & 0.865 & 0.859 & 0.859 & 0.795 & 0.947 & 0.269 & 0.451 \\
    0.867 & 0.869 & 0.874 & 0.865 & 0.785 & 0.939 & 0.263 & 0.417 \\
    0.871 & 0.887 & 0.873 & 0.881 & 0.784 & 0.940 & 0.269 & 0.442 \\
    0.835 & 0.845 & 0.843 & 0.842 & 0.794 & 0.940 & 0.258 & 0.388 \\
    0.870 & 0.878 & 0.869 & 0.871 & 0.784 & 0.939 & 0.262 & 0.417 \\
    0.873 & 0.873 & 0.891 & 0.890 & 0.786 & 0.933 & 0.279 & 0.368 \\
    avg 0.870 & avg 0.876 & avg 0.871 & avg 0.870 & avg 0.787 & avg 0.942 & avg 0.268 & avg 0.434 \\ \bottomrule
  \end{tabular}
  \caption{Here are the results for the logistic regression passive 10 folds.}
  \label{tab:logReg}
\end{table}
\FloatBarrier

\FloatBarrier
\begin{table}[h]
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
    coarse-tn & fine-tn & coarse-fn & fine-fn & coarse-fp & fine-fp & coarse-tp & fine-tp \\ \midrule
    1460 & 1849 & 454 & 65 & 14 & 46 & 82 & 50 \\
    1540 & 1851 & 374 & 63 & 22 & 49 & 74 & 47 \\
    1508 & 1851 & 406 & 63 & 12 & 43 & 84 & 53 \\
    1486 & 1853 & 428 & 61 & 19 & 53 & 77 & 43 \\
    1521 & 1859 & 393 & 55 & 20 & 52 & 76 & 44 \\
    1501 & 1843 & 413 & 71 & 19 & 52 & 77 & 44 \\
    1496 & 1841 & 417 & 72 & 17 & 49 & 80 & 48 \\
    1524 & 1852 & 389 & 61 & 25 & 59 & 72 & 38 \\
    1498 & 1841 & 415 & 72 & 18 & 51 & 77 & 44 \\
    1497 & 1836 & 416 & 77 & 13 & 57 & 83 & 39 \\
    avg 1503.1 & avg 1847.6 & avg 410.5 & avg 66.0 & avg 17.9 & avg 51.1 & avg 78.2 & avg 45.0 \\ \bottomrule
  \end{tabular}
  \caption{Here are the results for the logistic regression confusion matrices. The main source of the advantage
  for fine is from the decreased amount of false negatives.}
  \label{tab:logReg}
\end{table}
\FloatBarrier


\FloatBarrier
\begin{table}[h]
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
    coarse-pr & fine-pr & coarse-roc & fine-roc & coarse-acc & fine-acc & coarse-f1 & fine-f1 \\ \midrule
    0.904 & 0.912 & 0.883 & 0.891 & 0.940 & 0.957 & 0.508 & 0.491 \\
    0.885 & 0.889 & 0.863 & 0.870 & 0.941 & 0.957 & 0.494 & 0.456 \\
    0.886 & 0.887 & 0.851 & 0.858 & 0.944 & 0.959 & 0.477 & 0.461 \\
    0.893 & 0.884 & 0.874 & 0.858 & 0.935 & 0.956 & 0.472 & 0.467 \\
    0.879 & 0.874 & 0.856 & 0.856 & 0.931 & 0.953 & 0.420 & 0.390 \\
    0.882 & 0.878 & 0.859 & 0.858 & 0.934 & 0.956 & 0.436 & 0.418 \\
    0.898 & 0.892 & 0.882 & 0.869 & 0.942 & 0.959 & 0.473 & 0.458 \\
    0.889 & 0.893 & 0.872 & 0.869 & 0.938 & 0.958 & 0.461 & 0.472 \\
    0.913 & 0.913 & 0.901 & 0.907 & 0.942 & 0.960 & 0.491 & 0.491 \\
    0.898 & 0.899 & 0.892 & 0.882 & 0.938 & 0.959 & 0.446 & 0.465 \\
    avg 0.893 & avg 0.892 & avg 0.873 & avg 0.872 & avg 0.939 & avg 0.957 & avg 0.468 & avg 0.457 \\ \bottomrule
  \end{tabular}
  \caption{Here are the results for the SVM passive 10 folds.}
  \label{tab:SVM}
\end{table}
\FloatBarrier

\FloatBarrier
\begin{table}[h]
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
    coarse-tn & fine-tn & coarse-fn & fine-fn & coarse-fp & fine-fp & coarse-tp & fine-tp \\ \midrule
    1828 & 1881 & 86 & 33 & 34 & 54 & 62 & 42 \\
    1833 & 1888 & 81 & 26 & 38 & 60 & 58 & 36 \\
    1847 & 1893 & 67 & 21 & 45 & 61 & 51 & 35 \\
    1822 & 1882 & 92 & 32 & 38 & 57 & 58 & 39 \\
    1822 & 1886 & 92 & 28 & 46 & 66 & 50 & 30 \\
    1827 & 1889 & 87 & 25 & 45 & 64 & 51 & 32 \\
    1842 & 1892 & 71 & 21 & 45 & 62 & 52 & 35 \\
    1833 & 1887 & 80 & 26 & 44 & 59 & 53 & 38 \\
    1836 & 1888 & 77 & 25 & 39 & 56 & 56 & 39 \\
    1835 & 1890 & 78 & 23 & 46 & 60 & 50 & 36 \\
    avg 1832.5 & avg 1887.6 & avg 81.1 & avg 26.0 & avg 42.0 & avg 59.9 & avg 54.1 & avg 36.2 \\ \bottomrule
  \end{tabular}
  \caption{Here are the results for the SVM confusion matrices. Here the fine returns less false negatives
  than the coarse, but not as many true positives compared to coarse.}
  \label{tab:SVM}
\end{table}
\FloatBarrier



\FloatBarrier
\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/LogReg_FindThreshold_RocCurve_coarse}
        \caption{Log Reg ROC Curves - coarse}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/LogReg_FindThreshold_RocCurve_fine}
        \caption{Log Reg ROC Curves - fine}
    \end{subfigure}
    \caption{Fine has a higher accuracy than coarse at the default threshold.}
\end{figure*}
\FloatBarrier


\FloatBarrier
\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/LogReg_FindThreshold_PrCurve_coarse}
        \caption{Log Reg Pr Curves - coarse}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/LogReg_FindThreshold_PrCurve_fine}
        \caption{Log Reg Pr Curves - fine}
    \end{subfigure}
    \caption{The fine threshold occurs at a point on the pr-curve associated with a higher
    f-measure than the coarse curves.}
\end{figure*}
\FloatBarrier




\FloatBarrier
\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/SVM_FindThreshold_RocCurve_coarse}
        \caption{SVM ROC Curves - coarse}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/SVM_FindThreshold_RocCurve_fine}
        \caption{SVM ROC Curves - fine}
    \end{subfigure}
    \caption{SVM results are similar between coarse and fine.}
\end{figure*}
\FloatBarrier


\FloatBarrier
\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/SVM_FindThreshold_PrCurve_coarse}
        \caption{SVM Pr Curves - coarse}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/SVM_FindThreshold_PrCurve_fine}
        \caption{SVM Pr Curves - fine}
    \end{subfigure}
    \caption{SVM results for PR-curves and F-measure have coarse and fine picking
    different parts of the curves for their respective thresholds, coarse f1 avg is
    slightly higher at 0.468 compared to 0.457 for fine.}
\end{figure*}
\FloatBarrier









\FloatBarrier
\begin{table}[h]
  \small
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l||l||l||l||l|}\toprule
    Titles & fld1 & fld2 & fld3 & fld4 & fld5 & fld6 & fld7 & fld8 & fld9 & fld10 & Avg \\ \midrule
    fineErr & 111 & 112 & 106 & 114 & 107 & 123 & 121 & 120 & 123 & 134 & 117.1 \\
    coarseErr & 468 & 396 & 418 & 447 & 413 & 432 & 434 & 414 & 433 & 429 & 428.4 \\
    total & 579 & 508 & 524 & 561 & 520 & 555 & 555 & 534 & 556 & 563 & 545.5 \\
    jaccardInd & 0.15 & 0.2 & 0.16 & 0.16 & 0.17 & 0.19 & 0.18 & 0.19 & 0.19 & 0.2 & 0.18 \\
    InterSect &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 63 & 62 & 62 & 61 & 55 & 69 & 69 & 60 & 69 & 79 & 64.9 \\
    1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 2 & 1 & 0.80 \\
    2 & 5 & 10 & 5 & 10 & 6 & 3 & 5 & 10 & 6 & 5 & 6.5 \\
    3 & 5 & 10 & 4 & 3 & 8 & 10 & 8 & 8 & 6 & 4 & 6.6 \\
    4 & 3 & 0 & 1 & 1 & 2 & 4 & 1 & 3 & 1 & 2 & 1.8 \\
    5 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.10 \\
    6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.00 \\
    7 & 1 & 2 & 2 & 3 & 2 & 1 & 1 & 2 & 3 & 1 & 1.8 \\
    8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.10 \\
    Total & 77 & 84 & 74 & 79 & 75 & 88 & 85 & 85 & 87 & 92 & 82.6 \\
    Coarse &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 454 & 374 & 406 & 428 & 393 & 413 & 417 & 389 & 415 & 416 & 410.5 \\
    1 & 0 & 0 & 0 & 1 & 1 & 1 & 2 & 1 & 2 & 1 & 0.90 \\
    2 & 5 & 10 & 5 & 10 & 6 & 3 & 5 & 10 & 6 & 5 & 6.5 \\
    3 & 5 & 10 & 4 & 3 & 8 & 10 & 8 & 8 & 6 & 4 & 6.6 \\
    4 & 3 & 0 & 1 & 1 & 2 & 4 & 1 & 3 & 1 & 2 & 1.8 \\
    5 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.10 \\
    6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.00 \\
    7 & 1 & 2 & 2 & 4 & 2 & 1 & 1 & 2 & 3 & 1 & 1.9 \\
    8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.10 \\
    Total & 468 & 396 & 418 & 447 & 413 & 432 & 434 & 414 & 433 & 429 & 428.4 \\
    Fine &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 65 & 63 & 63 & 61 & 55 & 71 & 72 & 61 & 72 & 77 & 66.0 \\
    1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 2 & 1 & 0.90 \\
    2 & 11 & 16 & 13 & 16 & 16 & 11 & 10 & 15 & 15 & 13 & 13.6 \\
    3 & 18 & 20 & 11 & 21 & 17 & 17 & 17 & 23 & 15 & 21 & 18.0 \\
    4 & 6 & 3 & 7 & 5 & 6 & 10 & 11 & 10 & 8 & 7 & 7.3 \\
    5 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0.50 \\
    6 & 3 & 3 & 8 & 3 & 2 & 5 & 5 & 4 & 4 & 4 & 4.1 \\
    7 & 6 & 5 & 4 & 5 & 5 & 5 & 4 & 3 & 4 & 5 & 4.6 \\
    8 & 0 & 2 & 0 & 2 & 4 & 2 & 0 & 3 & 3 & 5 & 2.1 \\
    Total & 111 & 112 & 106 & 114 & 107 & 123 & 121 & 120 & 123 & 134 & 117.1 \\ \bottomrule
  \end{tabular}
  \caption{The jaccard results for Logistic Regression. Of the 117 instances in the
  Fine error set, 82 match with instances in the Coarse error set. The Jaccard index is 0.18,
  this compares to a higher Jaccard index of 0.43 in the SVM results. The difference is mainly
  due to the higher number of instances in the Coarse error set in the Log Reg results, 428 compared
  to 123. The error sets and the intersection sets contain representative samples of all the classes.}
  \label{tab:SVM}
\end{table}
\FloatBarrier



\FloatBarrier
\begin{table}[h]
  \small
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l||l||l||l||l|}\toprule
    Titles & fld1 & fld2 & fld3 & fld4 & fld5 & fld6 & fld7 & fld8 & fld9 & fld10 & Avg \\ \midrule
    fineErr & 87 & 86 & 82 & 89 & 94 & 89 & 83 & 85 & 81 & 83 & 85.9 \\
    coarseErr & 120 & 119 & 112 & 130 & 138 & 132 & 116 & 124 & 116 & 124 & 123.1 \\
    total & 207 & 205 & 194 & 219 & 232 & 221 & 199 & 209 & 197 & 207 & 209.0 \\
    jaccardInd & 0.41 & 0.4 & 0.47 & 0.42 & 0.41 & 0.41 & 0.47 & 0.45 & 0.44 & 0.45 & 0.43 \\
    InterSect &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 27 & 23 & 19 & 27 & 22 & 22 & 20 & 23 & 22 & 19 & 22.4 \\
    1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0.90 \\
    2 & 11 & 12 & 13 & 12 & 14 & 12 & 15 & 12 & 12 & 12 & 12.5 \\
    3 & 11 & 9 & 14 & 10 & 10 & 15 & 12 & 16 & 12 & 15 & 12.4 \\
    4 & 3 & 8 & 7 & 7 & 10 & 10 & 8 & 8 & 4 & 8 & 7.3 \\
    5 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0.50 \\
    6 & 2 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 0.50 \\
    7 & 2 & 4 & 3 & 5 & 4 & 3 & 4 & 2 & 4 & 4 & 3.5 \\
    8 & 3 & 1 & 4 & 3 & 5 & 1 & 4 & 2 & 3 & 4 & 3.0 \\
    Total & 60 & 59 & 62 & 65 & 67 & 64 & 64 & 65 & 60 & 64 & 63.0 \\
    Coarse &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 86 & 81 & 67 & 92 & 92 & 87 & 71 & 80 & 77 & 78 & 81.1 \\
    1 & 1 & 1 & 1 & 1 & 1 & 0 & 2 & 1 & 2 & 1 & 1.1 \\
    2 & 11 & 12 & 14 & 12 & 14 & 12 & 15 & 12 & 12 & 12 & 12.6 \\
    3 & 11 & 10 & 14 & 10 & 10 & 16 & 12 & 17 & 12 & 15 & 12.7 \\
    4 & 4 & 9 & 8 & 7 & 10 & 10 & 8 & 8 & 4 & 9 & 7.7 \\
    5 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0.50 \\
    6 & 2 & 0 & 0 & 0 & 0 & 2 & 0 & 2 & 1 & 0 & 0.70 \\
    7 & 2 & 4 & 3 & 5 & 5 & 3 & 4 & 2 & 4 & 4 & 3.6 \\
    8 & 3 & 1 & 4 & 3 & 5 & 2 & 4 & 2 & 3 & 4 & 3.1 \\
    Total & 120 & 119 & 112 & 130 & 138 & 132 & 116 & 124 & 116 & 124 & 123.1 \\
    Fine &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 33 & 26 & 21 & 32 & 28 & 25 & 21 & 26 & 25 & 23 & 26.0 \\
    1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0.90 \\
    2 & 15 & 18 & 16 & 16 & 15 & 18 & 17 & 16 & 16 & 17 & 16.4 \\
    3 & 18 & 15 & 19 & 19 & 22 & 19 & 17 & 23 & 19 & 19 & 19.0 \\
    4 & 7 & 11 & 11 & 9 & 12 & 12 & 12 & 11 & 8 & 10 & 10.3 \\
    5 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 & 0.60 \\
    6 & 4 & 6 & 3 & 1 & 3 & 3 & 4 & 2 & 1 & 3 & 3.0 \\
    7 & 2 & 6 & 3 & 6 & 5 & 5 & 6 & 3 & 4 & 4 & 4.4 \\
    8 & 7 & 2 & 7 & 4 & 7 & 7 & 5 & 3 & 6 & 5 & 5.3 \\
    Total & 87 & 86 & 82 & 89 & 94 & 89 & 83 & 85 & 81 & 83 & 85.9 \\ \bottomrule
  \end{tabular}
  \caption{The jaccard results for SVM. The fine error set has a total of 86 instances, with
  63 of those matching with the coarse set. Similar to the Log Reg jaccard results,
  the error sets and the intersection sets contain representative samples of all the classes.}
  \label{tab:SVM}
\end{table}
\FloatBarrier


\par Comparing the Log Reg coarse error set to the SVM coarse error set had
an intersection of 0.

\par Comparing the Log Reg fine error set to the SVM fine error set had
an intersection of 0.

\par Comparing the Log Reg fine error set to the SVM coarse error set had
an intersection of 0.

\par Comparing the Log Reg coarse error set to the SVM fine error set had
an intersection of 0.



\section{Active vs Passive curves}
\par The following plots were obtained with a round batch size of 100
and a starter set of 1040 instances out of the total 2098 instances.
The plots are the average of 10 folds, for each fold a test set of 2010
containing representatives of each class was held out, out of the remaining
18088, the starter set was selected which again contained representatives of
each class. Coarse and fine classifiers share the same starter set. During
 each round coarse and fine classifiers are trained on their corresponding
 sets, metrics are outputted on the held out test set, then confidence estimates
 are ran on the remaining eligible instances. Eligible instances are kept in
 separate sets for coarse and fine, 100 of the most uncertain instances are removed
 from each eligible set and added to its corresponding coarse or fine set to be
 trained on for the next round.


\FloatBarrier
\subsection{Plots for Logistic Regression Active vs Passive curves}
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassivePRLR}
    \label{fig:ActiveVsPassivePRLR}
    \caption{The PR AUC curves for rounds with the Logistic
Regression classifier conforms to expectations, with active-fine having
the highest performance. Active-coarse outperforms passive-coarse. Passive-fine
doesn't outperform the coarse classifiers until rnd 100. }
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassiveROCLR}
    \label{fig:ActiveVsPassiveROCLR}
    \caption{The ROC AUC curves for rounds with the
Logistic Regression classifier. The active curves beat out the passive
curves for both coarse and fine. Coarse roc starts with an advantage over
fine as in the PR curves. Both converge to the same rate after roc auc level after 80.}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassiveAccLR}
    \label{fig:ActiveVsPassiveAccLR}
    \caption{The accuracy of the fine classifiers stays at
roughly the same rate throughout the rounds, this is due to an effective
weighting scheme for the fine grained classifiers. The active coarse accuracy
drops towards the end due to an increase in false positives as more negative
instances are added in the later rounds.}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassiveF1LR}
    \label{fig:ActiveVsPassiveF1LR}
    \caption{The F-measure of the the fine classifiers increases
throughout the rounds as more true positives are predicted. The active coarse
again decreases at later rounds due to increased false positives.}
\end{figure}
\FloatBarrier









\FloatBarrier
\subsection{Plots for SVM Active vs Passive curves}
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassivePRSVM}
    \label{fig:ActiveVsPassivePRSVM}
    \caption{The PR AUC curves for rounds with SVM show little
advantage for fine. The results are slightly different than the ones shown
on 2/14 due to fixing a bug with the code that wasn't performing the
preprocessing scaling for the SVM case at the same stage as it was being
done for the logistic regression classifier.}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassiveROCSVM}
    \label{fig:ActiveVsPassiveROCSVM}
    \caption{The ROC curves show more of an advantage for coarse classifiers.}
\end{figure}
\FloatBarrier


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassiveAccSVM}
    \label{fig:ActiveVsPassiveAccSVM}
    \caption{The accuracy for the coarse decreases sharply due
to coarse predicting steadily more false positives, behaving similar to the
Log Reg case. Fine accuracy is higher due to predicting less false positives
than coarse. Fine also predicts less true positives, compare apx. 37 to apx.
60 t.p. for coarse at round 60.}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/ActiveVsPassiveF1SVM}
    \label{fig:ActiveVsPassiveF1SVM}
    \caption{The F-measure favors coarse, and trends to the same level
    for both coarse and fine.}
\end{figure}
\FloatBarrier


\break

\FloatBarrier
\section{Plots for FFR experiments}
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost1_rnds0_171}
    \label{fig:FFR_PR_Cost1}
    \caption{The strategy is changed from purchasing a set number of instances
    per round to having a set budget per round and spending a portion of that budget
    on fine and coarse grained labels. For this curve the fine and coarse grain labels
    both have a cost of 1. The purple 1.0 curve shows that if only fine grained labels
    are purchased, the highest performing PR-AUC can be obtained. The results are an
    average of 10 folds.}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost16_rnds0_171}
    \label{fig:FFR_PR_Cost16_rnds0_171}
    \caption{The fine cost is increased to 16. The budget for each iteration is 100, and
    for the case of the 0.5 curve, 50 instances are bought for coarse and 3.125 instances are
    bought for fine. The remainder 0.125 is then turned into a 0.125 chance for any round to
    purchase an extra fine label. The round size for the FFR 1.0 curve is very small, with only
    7 labels purchased per iteration. The cost is to high for the fine label advantage to offset
    the decreased number of instances purchased.}
\end{figure}
\FloatBarrier


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost16_rnds0_500}
    \label{fig:FFR_PR_Cost16_rnds0_500}
    \caption{This shows the iterations continuing through round 500, the curves
    with the higher fine rates eventually settle to the same end point that the
    curves with the high rates of coarse labels purchased achieved at previous
    iterations.}
\end{figure}
\FloatBarrier



\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost8_rnds0_171}
    \label{fig:FFR_PR_Cost8_rnds0_171}
    \caption{At fine cost 8 the FFR 0.0 rate is no longer the best option, 0.1
    generally outperforms 0.0 slightly.}
\end{figure}
\FloatBarrier


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost8_rnds0_500}
    \label{fig:FFR_PR_Cost8_rnds0_500}
    \caption{The extended picture of the FFR cost 8. The round size for FFR 1.0
    is small, only 13 instances purchase per iteration.}
\end{figure}
\FloatBarrier


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost4_rnds0_171}
    \label{fig:FFR_PR_Cost4_rnds0_171}
    \caption{At fine cost 4, FFR 0.3 appears to be the highest performing rate.}
\end{figure}
\FloatBarrier


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost4_rnds20_60}
    \label{fig:FFR_PR_Cost4_rnds20_60}
    \caption{The fine cost 4 curves shown expanding the rounds 20-60.}
\end{figure}
\FloatBarrier


\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost2_rnds0_171}
    \label{fig:FFR_PR_Cost2_rnds0_171}
    \caption{At fine cost 2, the preferred rate jumps up to 0.8,
    similar to the cost 1 results.}
\end{figure}
\FloatBarrier

\FloatBarrier
\begin{figure}[!htb]
	\centering
    \includegraphics[width=1.0\columnwidth]{fig/FFR_PR_Cost2_rnds20_60}
    \label{fig:FFR_PR_Cost2_rnds20_60}
    \caption{The fine cost 2 curves shown expanding rounds 20-60.}
\end{figure}
\FloatBarrier



\chapter{Conclusions and Future Work}\label{chap:math}
\par I should probably do the Bandit experiments.










%% backmatter is needed at the end of the main body of your thesis to
%% set up page numbering correctly for the remainder of the thesis
\backmatter

%% Start the correct formatting for the appendices
\appendix

\chapter{Tuning the fine grained classes}
\par All the data and results for tuning the fine grained classes.

%% Bibliography goes here (You better have one)
%% BibTeX is your friend
%\bibliographystyle{plain}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,nuthesis}
%% Pull in all the entries in the bibtex file. Is is a useful trick to
%% check all your references.
\nocite{*}


%% Index go here (if you have one)

\end{document}
\endinput
%%
%% End of file `thesis-test.tex'.
