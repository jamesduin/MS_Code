@misc{Breiman1996,
abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes add in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992)},
author = {Breiman, Leo},
booktitle = {Machine Learning},
doi = {10.1007/BF00117832},
isbn = {0885-6125},
issn = {0885-6125},
number = {1},
pages = {49--64},
title = {{Stacked regressions}},
volume = {24},
year = {1996}
}
@article{Cox1958,
author = {Cox, D.R.},
journal = {Journal of the Royal Statistical Society},
number = {2},
pages = {215--242},
title = {{The Regression Analysis of Binary Sequences}},
volume = {20},
year = {1958}
}
@article{Friedman2001,
abstract = {.},
author = {Friedman, Jerome H.},
doi = {10.1214/aos/1013203451},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Decision trees,Function estimation,Robust nonparametric regression},
number = {5},
pages = {1189--1232},
pmid = {21740230},
title = {{Greedy function approximation: A gradient boosting machine}},
volume = {29},
year = {2001}
}
