%\documentclass[twoside,leqno,twocolumn]{article}
\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage{ltexpprt} 
\usepackage{amsmath}
% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
% \usepackage{ulem} 
\usepackage{amssymb}
% For citations
\usepackage[numbers,sort&compress]{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}

\usepackage{color}

\definecolor{Red}{rgb}{1.0,0.0,0}
\definecolor{Blue}{rgb}{0.0,0.0,1}
\definecolor{Black}{rgb}{0.0,0.0,0.0}
\definecolor{Green}{rgb}{0.0,1.0,0.0}
\definecolor{Magenta}{rgb}{1.0,0.0,1}
\definecolor{Cyan}{rgb}{0.0,1.0,1}

\newcommand{\sys}{\Call{Hal}{}} % !!! WHEN YOU CHANGE THIS, YOU MUST MAKE A CORRESPONDING CHANGE IN 
				 %  TWO PLACES: where the term is defined in the text, and in the algorithm pseudocode.

\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}[thm]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{obs}[thm]{Observation}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Learning Hierarchically Decomposable Concepts with \\ Active Over-Labeling}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Yuji Mo and Stephen D. Scott}
\IEEEauthorblockA{Department of Computer Science\\
University of Nebraska\\
Lincoln, NE \ 68588-0115 USA\\
Email: \{ymo,sscott\}@cse.unl.edu}
%\and
% \IEEEauthorblockN{Stephen D. Scott}
% \IEEEauthorblockA{Department of Computer Science\\
% University of Nebraska\\
% Lincoln, NE \ 68588-0115 USA\\
% Email: sscott@cse.unl.edu}
%\and
%\IEEEauthorblockN{Doug Downey}
%\IEEEauthorblockA{EECS Department\\
%Northwestern University\\
%Evanston, IL \ 60208 USA\\
%Email: ddowney@eecs.northwestern.edu}
}

%\author{Yuji Mo\thanks{Dept. of Computer Science and Engineering, University of
%Nebraska, email: {\tt ymo@cse.unl.edu}}   \\
%\and
%Stephen Scott\thanks{Dept. of Computer Science and Engineering, University of Nebraska, email: {\tt sscott@cse.unl.edu}} 
%\and 
%Doug Downey\thanks{EECS Department, Northwestern University, email: {\tt ddowney@eecs.northwestern.edu}}
%}
%\date{}

%\maketitle



% make the title area
\maketitle



\begin{abstract} 
\begin{small}
%\baselineskip=9pt
Many classification tasks target high-level concepts
that can be decomposed into a hierarchy of finer-grained
subconcepts.  For example, some string entities that are Locations
are also Attractions, some Attractions are Museums, etc.  Such hierarchies are common in
named entity recognition (NER), document classification, and biological
sequence analysis.
We present a new approach for learning hierarchically decomposable
concepts.  The approach learns a high-level classifier
(e.g., location vs.\ non-location) by seperately learning multiple finer-grained
classifiers (e.g., museum vs.\ non-museum), and then combining the results.
Soliciting labels at a finer level of granularity
than that of the target concept is a new approach to active learning,
which we term ``active over-labeling''.
In experiments in NER and document classification tasks, we show that active over-labeling substantially
improves area under the precision-recall curve, when compared with standard passive or active learning.
Finally, because finer-grained labels may be more expensive to obtain, we also present a cost-sensitive
active learner that uses a multi-armed bandit approach to dynamically choose the label granularity to target, and show that the 
bandit-based learner is
robust to differences in label cost and labeling budget.
\end{small}
\end{abstract} 

% \begin{keywords}
% Semi Supervised Learning, Active Learning, Hierarchical Labeling, Text Classification, Cost Analysis
% \end{keywords}

\maketitle


\section{Introduction}
\label{sec:intro}
Several applications of machine learning have instances
that can be classified with hierarchical labeling
schemes.  For example, in named entity recognition (NER), the phrase
``The Metropolitan Museum of Art'' can be labeled as a Location, as a Building,
or as a Museum, where each label forms a subcategory of the
previous one.  Likewise, in the Gene Ontology (GO)~\cite{GeneOntology},
a biological sequence can be
labeled with multiple terms from a hierarchical scheme.

While some recent work has considered classification into fine-grained (low-level) 
categories \cite{fleischman2002fine,ling2012fine}
or hierarchies \cite{yosef2012hyena}, the majority of classification tasks in practice
target a small number of labels.  NER, for example, typically
targets a small set of coarse-grained labels such as Location, Organization,
and Person \cite{finkel2005incorporating}. 

We show how classifiers 
aimed at coarse-grained tasks can be improved by training on fine-grained labels.  For
example, we show an NER system can be made more precise by not
treating the broad ``Organization'' label as a {\bf single} concept, but instead explicitly 
learning a combination of fine-graned labels comprising the category 
(e.g., ``University,'' ``Railroad Company,'' and so on).  As we argue theoretically 
in Section \ref{sec:learningtheory} and establish in our experiments,
 % (e.g., see Figure \ref{fig:RPCurve-Whole}),
fine-grained labeling information can significantly improve the accuracy of a classifier aimed at a 
coarse-grained task.  Further, actively soliciting informative fine-grained labels, rather
than passively sampling them, provides an additional performance boost.  
We refer to this new approach, which extends the standard active
learning model to one in which the learner can solicit labels at finer
levels of the hierarchy than that targeted for classification,
as {\bf active over-labeling}.  

%In our work (Section~\ref{sec:results}), we discovered
%benefits to leveraging finer-grained label data (near the leaves
%of a label hierarchy) of instances to aid in building classifiers
%for coarse-grained labeling problems (near the root of a hierarchy).
%In our experiments, we discovered that one could leverage such fine-grained
%label data to improve precision for the same level of recall when 
%compared to learning algorithms that exclusively used coarse-grained
%labels.  

We present a general schema for performing active over-labeling
for any given hierarchical classification task, using
any given probabilistic base learner.  In our experiments, we demostrate
the effectiveness of the schema across multiple classification tasks (NER and document
classification) and multiple base learners (Conditional Random Fields and Logistic Regression).
We show that over-labeling improves performance over standard
passive or active learning, and that active over-labeling outperforms passive
over-labeling, in terms of area under the precision-recall curve.

Finally, we expect that obtaining a finer-grained
label will often be more costly than obtaining
a coarse-grained label.  For example, in GO, one could
label a sequence as being involved in biological processes, but a finer-grained
label for the same sequence could specify it as involved in growth,
% immune system process,
localization, or one of 22 other labels, which 
%.  The specificity of the fine-grained label 
requires more expertise and effort
to obtain.  Thus, we also present a method that uses a multi-armed bandit
approach to dynamically balance
cost and estimated benefit when choosing the level of granularity to
query.  We then show that the bandit approach is robust to changes in
label cost.

%We work in the {\it active learning} setting, where the labels of the training instances are initially unknown, 
%and a learning algorithm pays a certain cost to an oracle to reveal
%the labels of certain instances.  We study
%the cost/benefit trade-offs of making more expensive, fine-grained
%label purchases versus the less expensive coarse-grained purchases.

%In addition to our contribution of extending active learning to hierarchical labeling
%schemes, this paper contributes a new family of algorithms for over-labeling, 
%as well as an empirical cost/benefit analysis, determining 
%at what relative price points one would prefer one algorithm over another.

The rest of this paper is organized as follows.  In Section~\ref{sec:defn},
we define our learning setting and give intuition as to why
our approach offers advantages in this context.  
%Section \ref{sec:learningtheory} provides theoretical results showing
%that over-labeling improves computational complexity and label complexity
%when compared to conventional passive or active learning.
In Section~\ref{sec:approach} we describe
our algorithms in our new model, and we then present 
experimental results in Section~\ref{sec:results}.
%, where we discuss the benefit of using
%fine grained labels Section~\ref{sec:benefit}, our cost sensitivity analysis Section~\ref{sec:cost-sens}
%and empirical results on a self-adaptive learner Section~\ref{sec:adaptive}
We then cover related work in Section~\ref{sec:relwork} and conclude in Section~\ref{sec:concl}.

\section{Problem Definition}
\label{sec:defn}

Our setting resembles conventional pool-based active learning.
Formally, our task is to learn a target concept over an input space ${\cal X}$,
i.e., a 
function $f : {\cal X} \rightarrow {\cal Y}=\{0, 1\}$.  We are given a pool of 
unlabeled examples $U \subset {\cal X}$, a base probabilistic machine learner $L$ that can
be trained on labeled examples $({\bf x}, y)$, and
access to an oracle that can be queried at some cost for the label of any example $u \in U$.
Our goal is to choose a relatively small number of examples from $U$ to be labeled, 
and train $L$ on the labeled examples to output a relatively accurate classifier.

Our setting resembles conventional active learning models in that the learner
purchases labels and builds its classifier in order to make predictions of the labels of new 
instances.  The key distinction is that in our model, the oracle can also return
more refined label information for each query example.  Specifically, we consider target
concepts that can be decomposed hierarchically into constituent subconcepts,
at varying levels of granularity.  We assume the decomposition is given
to the learner, and that the oracle can then return labels corresponding to any
node in the hierarchy, which we refer to as a {\bf labeling tree}.  An
example labeling tree based on Reuters Corpus Volume~I~(RCV1)~\cite{Lewis2004}
is shown in Figure~\ref{fig:ontologyex}.

Formally, nodes of the tree each represent concepts (i.e., subsets of the instance space ${\cal X}$).
The root of the labeling tree corresponds to the target concept to be learned, and lower 
levels of the tree are sub-concepts of their ancestor concepts.  The oracle in our
setting returns a {\it vector} of labels, corresponding to a path starting at the root of the tree.
For example, based on the label tree in
Figure~\ref{fig:ontologyex}, an instance could be labeled as
$\langle$Location,Building,Museum$\rangle$,  $\langle$Location,Attraction,Museum$\rangle$,
$\langle$Location,Lake,$X\rangle$, or $\langle X,X,X \rangle$, where an `$X$' indicates
that no value at that level applies.
Thus, the latter labeling example indicates an instance that is not a location.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.1 \columnwidth]{fig/exp-ontology.png}}
\caption{An example labeling tree based on Reuters Corpus Volume~I~(RCV1)~\cite{Lewis2004}. 
}
\label{fig:ontologyex}
\end{center}
\vskip -0.2in
\end{figure} 

A vector of labels is denoted $\langle \ell_1,\ldots,\ell_k \rangle$, where a
label $\ell_i$ is the instance's label at the $i$th level of the tree, or `$X$' if that is
undefined in the tree.  If $\ell_i=X$, then $\ell_j=X$ for all $j > i$ (i.e., if a level-$i$
label does not apply, then no other label farther from the root may either).
Further, if $i$ is the largest value such that $\ell_i \neq X$, then
the values $\ell_i,\ldots,\ell_1$ must form a path from a leaf to the tree's root. 

Each instance in $U$ is initially labeled with the vector
$\langle$?,?,$\ldots$,?$\rangle$,
where `?' denotes a label value that is yet unspecified but can
be purchased.  For a specific instance, a value for $\ell_i$ may be purchased 
at cost $c_i \ge c_j > 0$ for all $i > j$.  We assume that a purchase of $\ell_i$ automatically
yields the values of $\ell_1$ through $\ell_{i-1}$.  E.g., a purchase of $\ell_3$ of an
instance could yield
$\langle$Location,Building,Museum$\rangle$,  $\langle$Location,Attraction,Museum$\rangle$,
$\langle$Location,Lake,$X\rangle$, or $\langle X,X,X \rangle$.\footnote{We assume that all labels in the
same level are distinct, e.g., `Museum' under `Attraction' is distinguishable from the one under
`Building'.}  %A minor change of notation would facilitate this, say, referring to the former as
%`Attraction-Museum' or `Location-Attraction-Museum'.}
Further, an $\ell_2$ purchase could yield
$\langle$Location,Building,?$\rangle$,  $\langle$Location,Attraction,?$\rangle$,
$\langle$Location,Lake,$X\rangle$, or $\langle X,X,X \rangle$ (once an `$X$' or a leaf
is encountered, one can fill in the rest of the vector with `$X$').

% commented by doug on 6/12, I think if we make the inequality non-strict we can 
% remove this sentence:
%We assume that $c_i \ge c_j > 0$ for all $i > j$ since otherwise there may be no incentive
%to purchase low-level labels.

%I'm not sure this adds enough to be included:
%Figure~\ref{fig:AL} illustrates the active learning model in a hierarchical labeling scheme.
%In this model, the active learning algorithm maintains one or more classifiers to predict labels
%for each level of the hierarchy.  Each iteration,
%the active learner considers the set of unlabeled data and chooses one or more unlabeled
%instances and a label level $i$ to purchase for those instances.  It then pays cost $c_i$ for 
%oracles to provide values of labels $\ell_1,\ldots,\ell_i$ for each such instance. 
%These labels and instances are then presented to learning algorithms to update classifiers 1
%through $i$.
%When learning is complete and a new instance $x$'s label is to be predicted, label
%predictions are made for $x$ by all classifiers at all levels, and these predictions are
%combined to make a final prediction for the root  level.  Final performance is based
%on how well predictions are made at the root level, and no others.

%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\columnwidth]{fig/AL2.png}}
%\caption{The hierarchical active learning model.}
%\label{fig:AL}
%\end{center}
%\vskip -0.2in
%\end{figure} 


% Our work focuses on, based on the relative costs at each level,
% choosing at which levels to purchase labels.  


\subsection{Intuition}
\label{sec:intuition}

Over-labeling relies on learning classifiers for the fine-grained (non-root)
concepts and combining the results, rather than simply directly learning the
coarse-grained (root) concept.  To see the potential advantage of over-labeling,
consider the simple example of
Figure~\ref{fig:unionex}.  In the figure, the coarse-grained concept
to be learned is circles (positives) versus diamonds (negatives).
If one limits the set of possible classifiers $\cal C$ to the set
of single axis-parallel boxes, then any hypothesis that has high recall
will have low precision (any rectangle that contains most of the circles will also
contain many diamonds).  However, if it is the case that the set
of positive instances can be decomposed into fine-grained classes
such as the four separate types of circles in Figure~\ref{fig:unionex},
then we can decompose the problem of classifying circles versus diamonds
into four problems: classifying green solid circles versus
everything else, classifying blue open circles versus everything
else, and so on.  We could thus train four fine-grained classifiers,
one per circle type.  With these inferred fine-grained classifiers,
one can predict on a new instance by predicting
its membership in each of the four fine-grained classes and then
returning a  root-level prediction of circle if any
fine-grained classifier predicts `yes'.  

In general, over-labeling takes advantage of a natural decomposition of the
target class into finer, possibly simpler sub-classes.  If the sub-classes
are in fact simpler to learn, then we can more easily learn the general class
by first learning the sub-classes and combining the sub-class predictions via, e.g., the 
union operator.  Since the union of hypotheses is a larger, more general
hypothesis space that includes the space of original hypotheses, this lends us
a potentially strong advantage in terms of representational ability.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=2.in]{fig/union.png}}
\caption{ An example of the potential usefulness of learning multiple
fine-grained concepts to support
the learning of a single coarse-grained one.  Negative instances are diamonds and 
positive ones are circles. 
% Each size-8 cluster of circles represents a separate fine-grained concept.
}
\label{fig:unionex}
\end{center}
\vskip -0.2in
\end{figure} 

% Of course, real learning problems may not be as simple to decompose
% as that in Figure~\ref{fig:unionex}, but our experiments reveal
% clear empirical advantages.  

%Let's not mention cost here, we talked about this above.
%In general, we would anticipate that
%the fine-grained labels are more 
%expensive to obtain (e.g., when asked to label a phrase
%of text, a human labeler can probably distinguish the name of an
%attraction from that of a lake more easily than the
%name of a park from that of a zoo).  Thus, we how one can
%combine the use of labels at different labels to effectively learn
%the level-1 (root) concept.

\input{theory}

% \section{Theoretical Results}
% \label{sec:learningtheory}
% 
% We begin by proving formally that active over-labeling provides advantages
% in terms of both computational complexity and label complexity.  We present
% results in both the {\bf probably approximately correct
% (PAC)}~\cite{v-tl-84} and {\bf exact}~\cite{a-lrsqc-87} models of learning. 
% The results below focus on the simple, but important case of learning concepts
% that are unions of axes-parallel boxes 
% \textcolor{red}{(as in, e.g., decision trees \cite{})} over real
% or ordinal feature spaces.  Extending the results to other settings is an item
% of future work.
% 
% \subsection{PAC Learning with Active Over-labeling}
% 
% In the PAC model, a learner is given positive parameters $\epsilon,\delta < 1/2$ 
% and access to a set of labeled training instances
% drawn iid according to an arbitrary distribution $\cal D$.  The
% learner is then expected to output a hypothesis in polynomial time that,
% with probability at least $1-\delta$, has error at most
% $\epsilon$ on new instances drawn according to $\cal D$.  
% 
% \subsubsection{Computational Complexity}
% 
% We begin by showing that in the passive learning setting, over-labeling
% offers advantages over standard labeling.
% In the context  of computational complexity, we consider the case
% % (in both the PAC and exact learning models)
% of {\bf proper learning}, in which the training instances are labeled by a
% concept from $\cal C$ and the hypothesis inferred by the learner
% is required to also be from $\cal C$.
% %We consider the real space ${\cal X} = \mathbb{R}^d$.
% %
% %\footnote{Note that the negative results described below for both exact and PAC learning are only for proper learning.  One can get positive results for these cases by allowing a logarithmic increase in the number of boxes used, by applying the set cover approximation algorithm.
% %
% We consider the task of learning concepts that are unions of $k$ axes-parallel boxes in $\mathbb{R}^d$.  This
% task is not properly PAC-learnable if RP $\ne$ NP, a fact that can be obtained
% from results found in previous work.
% \begin{prop}
% Learning $k$-unions of axes-parallel boxes in $\mathbb{R}^d$ is not PAC-learnable unless RP $=$ NP.
% \end{prop}
% {\bf Proof}: From Blumer et al.~\cite{behw-lvd-89}, 
% a concept class $\cal C$ is properly PAC learnable  if and only if there exists a 
% polynomial-time algorithm to find a hypothesis from $\cal C$ consistent with a size-$m$ labeled training sample 
% $\cal X$ (this is called the {\bf consistent hypothesis problem}).
% It is known to be NP-hard~\cite{behw-lvd-89,m-sncscp-xx} to find a smallest
% set of rectangles to cover a set of points in $\mathbb{R}^d$ even
% for $d=2$.  Thus, the consistent hypothesis problem for
% $k$-unions of boxes is NP-hard, implying that one 
% cannot properly PAC learn $k$-unions of boxes.  \hfill $\square$
% 
% In contrast, consider an over-labeling version of this learning problem,
% % the $k$-unions of boxes problem,
% in which each of the $k$ boxes
% is a separate subconcept, as in Figure~\ref{fig:ontologyex}. 
% Thus, examples from the $i$th box ($i=1,\ldots , k$) have a
% fine-grained label (call it $I_j$) and all other 
% examples are labeled `$-$'.
% \begin{prop}
% In the over-labeling setting, $k$-unions of boxes is properly PAC-learnable.
% \end{prop}
% {\bf Proof}: For each subconcept, a consistent concept (a single bounding box) can be learned from the fine-grained labels in time $O(dm)$.
% Thus, the learner can learn each of the $k$ subconcepts separately and output their union in time $O(kdm)$, and this concept
% will be consistent with the labeled examples whenever such a concept exists.  Thus, here the consistent hypothesis
% problem can be solved in polynomial time, which Blumer et al.~\cite{behw-lvd-89} above 
% implies that $k$-unions with over-labeling is properly PAC learnable.
%  \hfill $\square$
% 
% % \subsubsection{Sample Complexity}
% % In the context of sample complexity, we look at passive PAC learning of unions of $k$
% % axis-parallel boxes in $\mathbb{R}^d$.  The
% % classic approach~\cite{behw-lvd-89} to PAC learning a function class $\cal C$ is to
% % draw a labeled sample $\cal X$ of size  at least
% % \begin{equation}
% % m(\epsilon,\delta) = \max \left( {2 \over \epsilon} \log {2 \over \delta}, {8D \over \epsilon} \log {13
% % \over \epsilon}  \right)
% % \enspace ,
% % \label{eqn:behw}
% % \end{equation}
% % where $\epsilon$ and $\delta$ are the PAC parameters and $D$ is
% % the VC-dimension (VCD) of $\cal C$,
% % and to then find some function $c \in {\cal C}$ consistent with
% % $\cal X$.  
% % 
% % The VC-dimension of the class of single $d$-dimensional axis-aligned boxes is $2d$.  Based on
% % Blumer et al.~\cite{behw-lvd-89}, then, the VCD of the $k$-union of
% % $d$-dimensional axis-aligned boxes is
% % $\Theta(d  k \log k)$.  Thus, to directly PAC-learn $k$-unions of $d$-dimensional boxes,
% % it suffices to find a hypothesis consistent with 
% % \[
% % m_{coarse}(\epsilon,\delta) = \Theta \left( {1 \over \epsilon} \log {1 \over \delta}
% % + {d k \log k \over \epsilon} \log {1 \over \epsilon}  \right)
% % \enspace 
% % \]
% % labeled examples\footnote{Note that we are not considering the time complexity of finding such
% % a consistent hypothesis, only the number of training instances.}.
% % In contrast, learning the class of individual $d$-dimensional boxes reduces
% % the VCD from $\Theta(d  k \log k)$ to $\Theta(d)$.  However, since the learning
% % process is repeated $k$ times, one needs to learn each individual box with a smaller value of
% % $\epsilon$ and a smaller value of $\delta$,
% % to allow for the union bound to be applied across all $k$
% % fine-grained hypotheses.  This yields a sample complexity of 
% % \[
% % m_{fine}(\epsilon/k,\delta/k) = \Theta \left( {k \over \epsilon} \log {k \over \delta}
% % + {d k  \over \epsilon} \log {k \over \epsilon}  \right)
% % \enspace 
% % ,
% % \]
% % which grows more slowly than $m_{coarse}$.
% % %  since  $m_{coarse}-m_{fine}$ is positive.
% 
% \subsubsection{Label Complexity}
% 
% We now consider label complexity, in which one wants to minimize the 
% number of labels purchased by a pool-based active learning algorithm.
% We will work in a model where we are given a size-$m$ set of training data $U$,
% but initially the labels are missing. When seeking a PAC algorithm for learning, one
% can apply a standard result from Blumer et al.~\cite{behw-lvd-89} that says if
% the algorithm efficiently finds a hypothesis consistent with $U$ with size at least
% \begin{equation}
% m(\epsilon,\delta) = \max \left( {2 \over \epsilon} \log {2 \over \delta}, {8D \over \epsilon} \log {13
% \over \epsilon}  \right)
% \label{eqn:behw}
% \end{equation}
% (where $D$ is the {\bf VC dimension} of ${\cal C}$),
% then with probability at least $1-\delta$, the hypothesis will have error
% at most $\epsilon$.  If the instances of $U$ are initially 
% unlabeled, the goal in active learning is to purchase as few labels of
% instances of $U$ as possible and still guarantee a consistent
% hypothesis to yield the PAC result. 
% 
% For this example, we focus on what we term the {\bf disjoint intervals
% problem}.
% % of learning axis-parallel boxes on the real line.
% That is, $\cal C$ is the set of unions of at
% most $k$ disjoint intervals on $\mathbb{R}$. When a coarse-grained label
% of instance $x \in U$ is purchased, it returns
% `$+$' if $x$ lies in one of the $k$ target intervals and `$-$' otherwise.
% When a fine-grained label of $x$ is purchased, the label is an indicator
% of which of the $k$ target intervals it lies in ($I_1,\ldots,I_k$) or `$-$' if
% it does not lie in any target interval.  We assume that there is
% at least one point from $U$ in each interval $I_j$ and that there is
% at least  one point from $U$ between each adjacent pair of intervals.
% % (otherwise, those empty intervals/gaps between intervals are irrelevant in the PAC sense).
% 
% We now analyze both the coarse-grained and fine-grained active learning
% models in this context. We assume that, for each target interval, there is
% one instance of  $\cal X$ that is pre-labeled (for free).  I.e., in the 
% coarse-grained case, there are $k$ instances labeled `$+$' (one in each target
% interval) and in the fine-grained case there is one instance labeled $I_1$,
% one labeled $I_2$, etc.
% 
% \begin{prop}
% The disjoint intervals problem with coarse-grained labels requires $\Omega(m)$ queries in the worst case.
% \end{prop}
% {\bf Proof}:  
% The goal of the algorithm is to find the
% left and right boundaries of each of the $k$ target intervals, which
% is tantamount to identifying the leftmost and rightmost negatively
% labeled points between each consecutive pair of intervals.  Consider
% two consecutive intervals $I_j$ and $I_\ell$. 
% % that, when taken together and their intervening gap, contain the largest number of
% % points from $U$ of all consecutive pairs of intervals.  Since
% % it is the largest number of points, this count has to be $\Omega(m/k)$
% % but could be $O(m)$ in the worst case.  
% In searching for
% the set of negative points from $U$ lying between $I_j$ and
% $I_\ell$, the learner must purchase the label of some point between $x_j$ and
% $x_\ell$, where $x_j$ and $x_\ell$ are the pre-labeled points from
% $U$ from $I_j$ and $I_\ell$, respectively (any other queries
% are superfluous).  In the worst case, every query will result in a 
% response of `$+$', until only one remains to be labeled
% `$-$'.  Summed over all adjacent pairs of intervals, this will
% require $\Omega(m)$ queries in the worst case. 
%  \hfill $\square$
% 
% \begin{prop}
% The disjoint intervals problem with fine-grained labels requires $O(k \log m)$
% queries in the worst case.
% \end{prop}
% {\bf Proof}:  
% An algorithm in the active over-labeling setting 
% can perform a binary search between $x_j$ and $x_\ell$  (labeled
% $I_j$ and $I_\ell$ rather than simply `$+$') until a negatively labeled
% instance $x_-$ is found.  When
% that is done, the learner can simply perform two binary searches: one between $x_-$ and
% the right-most point in $I_j$ and one between $x_-$ and
% the left-most point in $I_\ell$. This requires at most $O(\log m)$ queries
% per pair of adjacent intervals, for a total of $O(k \log m)$ queries. 
%  \hfill $\square$
% 
% Thus, we see that active over-labeling requires exponentially fewer queries
% (in $m$) in the worst case when compared with standard active learning.
% 
% % classic approach~\cite{behw-lvd-89} to PAC learning a function class $\cal C$ is to
% % draw a labeled sample $\cal X$ of size  at least
% % \begin{equation}
% % m(\epsilon,\delta) = \max \left( {2 \over \epsilon} \log {2 \over \delta}, {8D \over \epsilon} \log {13
% % \over \epsilon}  \right)
% % \enspace ,
% % \label{eqn:behw}
% % \end{equation}
% % where $\epsilon$ and $\delta$ are the PAC parameters and $D$ is
% % the VC-dimension (VCD) of $\cal C$,
% % and to then find some function $c \in {\cal C}$ consistent with
% % $\cal X$.  
% 
% Finally, we can show that the worst-case number of label purchases required by a standard active learner
% is bounded above that of an active over-labeling learner.
% Applying Equation~\ref{eqn:behw},
% Since the VC dimension $D$ of the union of $k$ intervals is $2k$, we will need
% $U$ of size 
% \[
% m \ge \max \left( {2 \over \epsilon} \log {2 \over \delta}, {16 k \over \epsilon} \log {13
% \over \epsilon}  \right)
% \enspace .
% \]
% The second term of the max is the dominant one (unless $\delta$ is exponentially small
% in $k$), so we focus on it.  This gives a worst-case lower bound 
% of purchases for a coarse-grained learner to be $\Omega ((k / \epsilon ) (\log 1/\epsilon))$,
% while our worst-case upper bound of purchases for a fine-grained learner is
% $O(k \log (k/\epsilon) + k \log \log (1/\epsilon))$. Both are linear in $k$ but the
% coarse-grained learner's bound is worse by a factor  exponential in $1/\epsilon$.
% 
% \subsection{Exact Learning with Active Over-labeling}
% 
% We illustrate the computational complexity advantages of
% active over-labeling in the exact learning setting.  In 
% exact learning, the learner gets access to two oracles: a
% {\bf membership query} (MQ) oracle and an {\bf equivalence query}
% (EQ) oracle.  An efficient learner will learn the exact identity
% of the target concept in time and number of queries that are
% polynomial in the problem size.  When the learner poses an EQ, it
% passes to the oracle a hypothesis $h$ that it thinks is exactly
% equivalent to the target concept, i.e., that will label all instances
% correctly.  The oracle either responds that the hypothesis
% is exactly correct or gives to the learner a counterexample, which
% is an instance on which $h$ is wrong.  An MQ oracle receives from
% the learner an instance $x$ and provides $x$'s label.  It is similar
% to an active learning model, except that in the MQ model, the
% instances can be arbitrary while in active learning, the instances must come
% from a pre-specified set.
% 
% Here we consider the passive (where all instances are labeled)
% proper learning of $k$-unions of axis-parallel boxes,
% in a bounded, discretized, $d$-dimensional instance space $\{0,\ldots,t-1\}^d$.
% 
% \begin{prop}
% With over-labeling, discrete $k$-unions of boxes can be exactly learned with
% $O(k)$ EQs and $O(kd \log t)$ MQs and time polynomial in the number of queries.
% \end{prop}
% {\bf Proof}:
% Using fine-grained labels for $k$ distinct fine-grained hypotheses (each using one 
% box), one can exactly learn each box individually with one EQ (to get a positive
% instance) and $O(d \log t)$ MQs (for binary search to find the box's $2d$ boundaries), for a
% total of $O(k)$ EQs and $O(kd \log t)$ MQs and time polynomial in the number of queries.
%  \hfill $\square$
% 
% This contrasts with a result from Bshouty and Burroughs~\cite{bb-plapc-03}
% that one cannot exactly properly learn unions of $\le k$ axis-parallel boxes
% ({\bf $k$-unions}) when (constant) $d>2$ unless P $=$ NP.  I.e., while one 
% can learn $k$-unions with $O(d \log k)$-unions, one cannot 
% efficiently learn $k$-unions with $k$-unions  if P $\ne$ NP.  
% %
% Note that the positive result for over-labeling works for non-constant $d$, while the
% hardness result for direct proper learning holds for constant $d$. 
% %Thus, there is clearly an advantage to having more refined label information in the form of which of the $k$ boxes an instance lies in.
% 
% The above results show that in both the exact and PAC learning setting, and in active and passive learning,
% the over-labeling approach offers theoretical advantages over standard learning.  We now present
% our general method for active over-labeling, and evaluate it experimentally.

\section{Approach}
\label{sec:approach}

We now present our method for performing the learning task outlined in 
Section~\ref{sec:defn}. 
We refer to our method as \sys , for Hierarchical Active Learner.
The high-level steps
of our algorithm are given in Algorithm~\ref{alg:treetrain}.  
%In
%our experiments, we explore variants of the basic algorithm,
%defined by 
%different strategies for purchasing labels and for assembling
%a coarse-grained level-1 classifier from the different classifiers
%learned for each level $i \le k$.  We describe the basic strategy
%and its variants below.

\sys\ iteratively purchases labels at each level of the labeling tree in batches,
where the proportion of labels purchased at each level is specified by
vector ${\bf p}$.  
% In our experiments, we explore how different settings for ${\bf p}$ impact performance. 
In the step \Call{Purchase}{$b \, p_i, i, C^*(x)$},
the system purchases $b \, p_i$ dollars worth of label vectors
defined up to level $i$ of the labeling tree, where the instances to be labeled are
chosen actively via uncertainty sampling relative to classifier $C^*(x)$\footnote{The {
\bf number} of examples purchased at each level will vary inversely with the cost of labels
at that level, and later we explore how varying label cost ratios impacts performance of the algorithm
for a given budget level.
% in our experiments
% (see Section~\ref{sec:cost-sens}).
} (discussed below).
Because label vectors are defined up to level $i$, they include
labels for all levels $m \le i$ (Section~\ref{sec:defn}).
\Call{LabelMap}{$E', m, j$} then creates individual labeled examples for
class $m$ at level $j$
corresponding to the given label vectors $E'$, for training the classifiers $C_{i, j}$.

%After the level-$i$ purchase, the function \Call{LabelMap}{$E', m, j$} is called 
%for every level $m \le i$ and every class $j$ in level $m$.
%Calling \Call{LabelMap}{$E', m, j$} for each $j$ converts the purchased label vectors in 
%$E'$ into corresponding labels for level $m$
%Specifically, \Call{LabelMap}{}
%goes through every instance $x \in E'$ and $x$'s newly-purchased label vector $\langle \ell_1,
%\ldots, \ell_i,?,\ldots,? \rangle$, 
%and looks at the value $\ell_m$ of the level-$m$ label in
%the label vector.  
% Let the value of $\ell_m$ be $v$.  
%In the set $E_{m,j}$ returned by \Call{LabelMap}{$E', m, j$},
%instance $x$ is given a binary label of $1$ if $\ell_m=j$ and 0 otherwise.
%E.g., if $x$'s level-2 label is ``Lake'', then the label of $x$ in $E_{2,\mbox{Lake}}$ is
%1 and the label of $x$ in $E_{2,\mbox{Mountain}}$ is 0. 

%All examples purchased for level $j$ also serve
%as labeled examples for levels $i<j$, as discussed in the Section~\ref{sec:defn}.


%We experiment with two approaches for selecting which examples
%to label for level $i$.  The first ignores the classifier $C_{i,*}$ and simply
%selects examples at random---i.e.,
%it performs passive learning.  Our second approach uses active learning,
%selecting examples for level $i$ using {\it uncertainty sampling}~\cite{tk-svmalatc-01}
%relative to the current level-$i$ classifier \textcolor{magenta}{$C_{i,*}$} (discussed below).

\sys\ then trains a probabilistic binary classifier $C_{i,j}$ for each class $j$ 
at level $i$, using the machine learning algorithm $L$.  $C_{i,j}(x)$ denotes
$C_{i,j}$'s estimate of the probability that an arbitrary example $x$ is positive for class $j$ at level $i$ (e.g.,
$C_{2,\mbox{``Lake''}}(x)$ is the probability that instance $x$ is a Lake).  
The choice of $L$ depends on the particular learning task
(we use Gradient Boosted Regression Trees, Logistic Regression and Conditional Random Fields  in our experiments).

A key problem for \sys\ is how to combine the 
classifiers $C_{i,j}$ into an ensemble classifier for the coarse-grained level-1
concept.  In principle,
the level-1 concept is a disjunction over the concepts $j$ at any given level $i$.
However, modeling the $C_{i,j}$ for a given $i$ explicitly as a disjunction can be
challenging, due to dependencies across the different level-$i$ classifiers (which
are trained on related sets of data $E_{i,j}$).  
In preliminary experiments, we explored
combining the classifiers with a noisy-or model (i.e., assuming independence), a linear model~\cite{Breiman1996},
or taking a p-norm across all $j$.  None of these approaches outperformed a simple approach of
simply taking a maximum.  Thus, we define:
\begin{equation}
\label{eq:maxcombine}
%  C^*(x) = \left(\sum_{i,j}(C_{i,j}(x)^p) \right)^{1/p}
  C^*(x) = \max_{i,j} C_{i,j}(x)
\end{equation}
as the output ensemble classifier. 
%In our experiment, we choose $p=\infty$ so that Equation~\ref{eq:maxcombine} is equivalent to a maximum.

Finally, when purchasing examples with active learning, \sys\ uses uncertainty sampling.
The uncertainty at level $i$
is measured with respect to the ensemble classifier $C^*(x)$.  Specifically,
we define the uncertainty $u_i(x)$ of the label for example $x$ for level $i$ as:
\begin{equation}
 u(x) = 0.5 - | C^*(x) -0.5|
\enspace .
\end{equation}


\begin{algorithm}[ht]
\small{
\caption{Method for learning the concept at the root of labeling tree $T$.  
See text for 
Purchase and LabelMap.
% are separate subroutines (see text).
}
\label{alg:treetrain}
\begin{algorithmic}
\Function{Hal}{Unlabeled examples $U$, labeling tree $T$, machine learner $L$, budget
$B$, per-iteration budget $b$, Purchase proportions ${\bf p} = (p_1, \ldots , p_k)$ with
$||{\bf p}||_1=1$ and $p_i \ge 0$}
\State  $E_{i,j} \gets \emptyset$  \Comment binary-labeled train set for
level $i$, label $j$
\State Initialize $C_{i,j}$ for all $i, j$
\While{$B > b$}
  \State $B \gets B-b$
  \ForAll{Level $i \in T$}
    \State $E' \gets$ \Call{Purchase}{$b \, p_i, i, C_{i,*}$}
    \ForAll{Level $m \le i$}
       \ForAll{Class $j$ in Level $m$}
           \State $E_{m,j} \cup=$ \Call{LabelMap}{$E', m, j$} 
       \EndFor
    \EndFor
%    \Comment Purchase $b p_i$ labels for level $i$ (see text)
  \EndFor
  \ForAll{Level $i \in T$}
  \ForAll{Class $j$ in Level $i$}
    \State $C_{i,j} \gets$ Train $L$ on $E_{i,j}$
  \EndFor
  \EndFor
\EndWhile
\State \Return Ensemble classifier $C^*(\{C_{i,j}\}, {\bf p})$
%  \Comment Return an ensemble of the individual classifiers $C_i$ (see text)
\EndFunction
\end{algorithmic}
}
\end{algorithm}

\subsection{Dynamically Adapting Purchase Proportions}
\label{sec:adaptive}

\sys\ takes as input a vector of purchase proportions, specifying how much of the budget should be
spent acquiring labels at each given level of granularity in the hierarchy.  The cost of labeling an example can
vary across levels of the hierarchy, and as we show in Section~\ref{sec:cost-sens}, the relative benefit of labels from a given
level in the hierarchy often changes as learning progresses.  Thus, we desire a cost-sensitive approach that dynamically
adapts the purchase proportions.
% during learning.

We formulate the task of choosing which level of granularity to purchase next as a multi-armed bandit problem, and solve it using
the $\epsilon$-greedy bandit algorithm~\cite{Auer2002} (BANDIT).  For clarity, we focus on dynamically choosing
between two strategies corresponding to purchase proportions, ${\bf p}$ and ${\bf p'}$, but the generalization to more strategies is straightforward.

BANDIT iteratively chooses strategies, and uses a running average of the reward observed for each strategy to guide
its choices.  For active learning, a natural way to define reward in round $j$ is in terms of observed model change:
\begin{equation}
g_{j} = \frac{1}{\|X\|}\sum_{x_i \in X} \log{(|f_{j-1}(x_i) - f_{j}(x_i)|)}
\enspace ,
\end{equation}
where $X$ is the set of withheld {\em unlabeled} examples, and $f_{j}(x_i)$ is \sys 's output for the 
input $x_i$ after the $j$th iteration.
% of \sys .

However, our problem setting has special characteristics that make the gain equation above unsuitable.
In particular, this gain score is not stationary as is assumed in the generic BANDIT
algorithm. Instead,
as the number of purchased examples increases and \sys\ gradually fits the data, the model becomes less likely to change. 
This means the running average of the gain will slowly decrease.  As a result,
BANDIT will disproportionately favor arms it has not played recently (whose average gains have not recently been updated).
In our preliminary experiments, we found that these characteristics led the generic BANDIT approach to thrash 
between arms in cases where sticking with one arm for longer was a stronger strategy.

To adapt BANDIT to our setting, instead of modeling each purchase strategy as
an arm, we instead use
two arms: (1) maintain the same strategy as before, and (2) switch strategies.
The reward from the first arm is always zero.  The reward from the second arm
depends on the difference in gain before and after switching, defined as:
\vspace{-0.1in}
\begin{equation}
\label{eq:reward}
    r_{j}=
\begin{cases}
   -\frac{g_{j}}{|g_{j}|}, & \text{if } {\bf p}\rightarrow{\bf p'}\\
    \frac{g_{j}}{|g_{j}|}, & \text{if } {\bf p'}\rightarrow{\bf p}\\
    0, & \text{if } {\bf p}\rightarrow{\bf p} \text{ or } {\bf p'}\rightarrow{\bf p'}\\
\end{cases}
\enspace .
\end{equation}
%We then utilize the average reward $r_j$ for a given arm within the existing 
%$\epsilon$-greedy bandit algorithm~\cite{Auer2002}.

\input{experiment}

%\input{discussion}


\section{Related Work}
\label{sec:relwork}

To our knowledge, our experiments are the first to demonstrate
how leveraging fine-grained label information can improve the
accuracy of a coarse-grained (root-level) classifier, and the first
investigation into active learning in a hierarchical setting where
label acquisition cost can vary.

Previous work in text classification~\cite{mccallum1998improving} and rich media indexing~\cite{jiang2013} has considered using
hierarchies of labels to improve a fine-grained classifier, through
techniques that back off to coarse
levels of the hierarchy when fine-grained data are sparse. 
By contrast, we present novel techniques
that work in the opposite direction, utilizing selectively
acquired fine-grained labels to improve
classification over coarse categories.
In named entity recognition (NER), some recent work has targeted
fine-grained entity categories~\cite{fleischman2002fine,ling2012fine}
or hierarchies~\cite{yosef2012hyena}.  Our work differs from this
previous work in that we focus on active learning under variable
label acquisition costs.
Our experiments illustrate that our active approach outperforms
passive learning on the NER task, and we demonstrate how the
relative cost of obtaining finer-grained labels impacts
which NER approach is most appropriate to use.

Our approach builds on a variety of previous work in active learning.
We focus on ``pool-based'' active learning, in which
a learner selects instances from a pool of unlabeled data to be 
labeled by an oracle. When acquiring
labels is costly, active learning can reduce
the expense by requesting only a relatively small subset
of the most informative labels~\cite{Rubens2011}.
One criterion used for selection of instances to label is to
choose those that reduce uncertainty. In our case,
uncertainty is measured in terms of the confidence of output values
(e.g., Merialdo~\cite{Merialdo2001}); other measures include
uncertainty in the parameters of probabilistic models~\cite{Hofmann2003}
or the size of a model's decision boundary~\cite{Schohn2000}.  
% Experiments with other active learning approaches in our hierarchical setting is an item of future work.

In previous work, active learning has also been shown 
to reduce sampling bias by utilizing the hierarchical structure of input
features~\cite{Dasgupta2008, Symons2006}. By contrast, 
our work focuses on active learning over hierarchically
structured output labels.

Luo et al.~\cite{lsu-lsal-13} looked at active learning in to perform structure
prediction, e.g., to predict a segmentation of an image or a parse tree of a
sentence.  While the predictions their algorithms made are structured in
nature, it is not similar to our work, which predicts labels according to a
fixed hierarchy known {\em a priori} and varying costs.

\section{Conclusions}
\label{sec:concl}

Hierarchical labeling schemes are increasingly common in a variety
of applications.  Our results demonstrate that fine-grained label
data (labels specified at nodes removed from the root of a labeling
tree) can be used to improve precision of a classifier for the
coarse-grained (root) concept.  However, it is likely that such
fine-grained labels will be more expensive to obtain.  We defined
a new active learning approach, {\bf active over-labeling}, to address this scenario, created a family of
hybrid algorithms to actively make label purchase decisions, 
empirically evaluated this family of algorithms, and analyzed the 
relative cost points at which one algorithm is preferred over another
at various budget levels. Finally we proposed a more sophisticated algorithm which dynamically adjust the
mix of different levels of labels, which benefit from more detailed cost analyses.

In the roadmp, it would be
interesting to consider other hierarchically labeled data sets with multiple layers, e.g.,
that labeled by the Gene Ontology~\cite{GeneOntology}.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


% \bibliography{doug,bib/colt,bib/lrgbrt,bib/textanalysis,bib/me,bib/roc,bib/netw,bib/tutor,bib/extras,bib/active,bib/multi,bib/fuzzy,bib/bio,bib/mcmc,bib/physics,bib/budgeted,bib/bayesnets}


% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{small}
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{GeneOntology}
\BIBentryALTinterwordspacing
``The {Gene} {Ontology},'' [Online]. Available:
  \url{geneontology.org}
\BIBentrySTDinterwordspacing

\bibitem{fleischman2002fine}
M.~Fleischman and E.~Hovy, ``Fine-grained classification of named entities,''
  in \emph{Proc. 19th Int. Conf. on Comp.
  Linguistics-Vol 1}, pp. 1--7,  2002. 

\bibitem{ling2012fine}
X.~Ling and D.~S. Weld, ``Fine-grained entity recognition.'' in \emph{AAAI},
  2012.

\bibitem{yosef2012hyena}
M.~A. Yosef, S.~Bauer, J.~Hoffart, M.~Spaniol, and G.~Weikum, ``Hyena:
  Hierarchical type classification for entity names.'' in \emph{COLING
  (Posters)}, 2012, pp. 1361--1370.

\bibitem{finkel2005incorporating}
J.~R. Finkel, T.~Grenager, and C.~Manning, ``Incorporating non-local
  information into information extraction systems by Gibbs sampling,'' 
  \emph{Proc.~of 43rd ACL}, pp. 363--370, 2005.

\bibitem{Lewis2004}
D.~D. Lewis, Y.~Yang, T.~G. Rose, and F.~Li, ``{RCV1: A New Benchmark
  Collection for Text Categorization Research},'' \emph{JMLR},
  vol.~5, pp. 361--397, 2004.

\bibitem{v-tl-84}
L.~G. Valiant, ``A theory of the learnable,'' \emph{Commun. ACM}, vol.~27,
  no.~11, pp. 1134--1142, Nov. 1984.

\bibitem{a-lrsqc-87}
D.~Angluin, ``Learning regular sets from queries and counterexamples,''
  \emph{Inform. Comput.}, vol.~75, pp.~87--106, 1987.

\bibitem{behw-lvd-89}
A.~Blumer, A.~Ehrenfeucht, D.~Haussler, and M.~K. Warmuth, ``Learnability and
  the {V}apnik-{C}hervonenkis dimension,'' \emph{J. ACM}, vol.~36, no.~4, pp.
  929--965, 1989.

\bibitem{m-sncscp-xx}
W.~J. Masek, ``Some {NP}-complete set cover problems,'' unpublished manuscript,
  {MIT} Laboratory for Computer Science.

\bibitem{bb-plapc-03}
N.~Bshouty and L.~Burroughs, ``On the proper learning of axis-parallel
  concepts,'' \emph{JMLR}, vol.~4, pp.~157--176, 2003.

\bibitem{Breiman1996}
L.~Breiman, ``{Stacked regressions},'' \emph{Machine Learning}, vol.~24, pp. 49--64, 1996.

\bibitem{Friedman2001}
J.~H. Friedman, ``{Greedy function approximation: A gradient boosting
  machine},'' \emph{Annals of Statistics}, vol.~29, no.~5, pp. 1189--1232,
  2001.

\bibitem{Auer2002}
P. Auer, N. Cesa-Bianchi, P. Fischer, ``{Finite-time analysis of the multi-armed bandit problem},'' \emph{Machine Learning} 47, 235-256 (2002).

\bibitem{Salton1988}
%\BIBentryALTinterwordspacing
G.~Salton and C.~Buckley, ``{Term-weighting approaches in automatic text
  retrieval},'' \emph{Inf.~Proc.~\&~Man.}, 1988. %[Online].
%  Available:
%  \url{http://www.sciencedirect.com/science/article/pii/0306457388900210}
%\BIBentrySTDinterwordspacing

\bibitem{Cox1958}
D.~Cox, ``{The regression analysis of binary sequences},'' \emph{Jour.~of the
  Royal Stat.~Soc.}, vol.~20, pp.~215--242, 1958.

\bibitem{RichmondDispatch}
%\BIBentryALTinterwordspacing
{University of Richmond Libraries}, ``{\em {Richmond} {Daily} {Dispatch},
  1860--1865},'' 2014. %[Online]. Available:
%  \url{dlxs.richmond.edu/d/ddr/techinfo.html}
%\BIBentrySTDinterwordspacing

\bibitem{Sutton2006}
%\BIBentryALTinterwordspacing
C.~Sutton, ``{An introduction to conditional random fields for relational
  learning},'' \emph{Graphical Models}, vol.~7, p.~93, 2006. %[Online].
%  Available:
%  \url{http://books.google.com/books?hl=en\&amp;lr=\&amp;id=lSkIewOw2WoC\&amp;oi=fnd\&amp;pg=PA93\&amp;dq=An+Introduction+to+Conditional+Random+Fields+for+Relational+Learning\&amp;ots=T-AKP-ger3\&amp;sig=Ew0XSB-xE6hBfQQFdspsv\_qZdcE}
%\BIBentrySTDinterwordspacing

\bibitem{Culotta2004}
%\BIBentryALTinterwordspacing
A.~Culotta, ``{Confidence estimation for information extraction},''
  \emph{HLT-NAACL}, 2004. %[Online]. 
% Available:
%  \url{http://dl.acm.org/citation.cfm?id=1614012}
%\BIBentrySTDinterwordspacing

\bibitem{mccallum1998improving}
A.~McCallum, R.~Rosenfeld, T.~M. Mitchell, and A.~Y. Ng, ``Improving text
  classification by shrinkage in a hierarchy of classes.'' \emph{ICML},
  pp. 359--367, 1998.

\bibitem{Rubens2011}
%\BIBentryALTinterwordspacing
N.~Rubens, D.~Kaplan, and M.~Sugiyama, ``{Active learning in recommender
  systems},'' \emph{Recommender Systems Handbook}, pp. 1--31, 2011. %[Online].
%  Available:
%  \url{http://link.springer.com/chapter/10.1007/978-0-387-85820-3\_23}
%\BIBentrySTDinterwordspacing

\bibitem{Merialdo2001}
%\BIBentryALTinterwordspacing
A.~Merialdo, ``{Improving Collaborative Filtering For New-Users By Smart Object
  Selection},'' \emph{ICMF}, 2001. %[Online]. %Available:
%  \url{http://www.eurecom.fr/publication/670
%  https://www.eurecom.fr/fr/publication/670/download/mm-kohrar-010508.pdf}
%\BIBentrySTDinterwordspacing

\bibitem{Hofmann2003}
%\BIBentryALTinterwordspacing
T.~Hofmann, ``{Collaborative filtering via gaussian probabilistic latent
  semantic analysis},'' in \emph{ SIGIR '03}, p. 259, 2003.
%  \url{http://dl.acm.org/citation.cfm?id=860435.860483}
%\BIBentrySTDinterwordspacing

\bibitem{Schohn2000}
%\BIBentryALTinterwordspacing
G.~Schohn and D.~Cohn, ``{Less is more: Active learning with support vector
  machines},'' \emph{ICML}, pp. 839--846. %, Jun. 2000. %[Online]. %Available:
%  \url{http://dl.acm.org/citation.cfm?id=645529.657802
%  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.6090\&rep=rep1\&type=pdf}
%\BIBentrySTDinterwordspacing

\bibitem{Dasgupta2008}
%\BIBentryALTinterwordspacing
S.~Dasgupta and D.~Hsu, ``{Hierarchical sampling for active learning},''
  \emph{ICML }, pp. 208--215, 2008. %[Online]. %Available:
%  \url{http://portal.acm.org/citation.cfm?doid=1390156.1390183}
%\BIBentrySTDinterwordspacing

\bibitem{Symons2006}
%\BIBentryALTinterwordspacing
C.~Symons et al.,
  ``{Multi-Criterion Active Learning in Conditional
  Random Fields},'' \emph{ ICTAI}, pp. 323--331, 2006. %[Online].
%  Available:
%  \url{http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4031915
%  http://dl.acm.org/citation.cfm?id=1190614.1191040}
%\BIBentrySTDinterwordspacing

\bibitem{lsu-lsal-13}
W.~Luo, A.~Schwing, and R.~Urtasun, ``Latent structured active learning,'' in
  \emph{NIPS}, 2013.
  
\bibitem{jiang2013}
W.~Jiang and Z.~Ras, ``Multi-label Automatic Indexing of Music by Cascade Classifiers,'' in \emph{Web Intelli. and Agent Sys.}, 2013.

\end{thebibliography}
\end{small}

% that's all folks
\end{document}
