
%
%\begin{figure}[!htb]
%	\centering
%    \includegraphics[width=1.0\columnwidth]{fig/resultsrunActPassLogReg4plots_pr}
%    \label{fig:resultsrunActPassLogReg4plots_pr}
%    \caption{this}
%\end{figure}
%\FloatBarrier
%
%\begin{figure}[!htb]
%	\centering
%    \includegraphics[width=1.0\columnwidth]{fig/resultsrunActPassLogReg4plots_roc}
%    \label{fig:resultsrunActPassLogReg4plots_pr}
%    \caption{this}
%\end{figure}
%\FloatBarrier




%\FloatBarrier
%\begin{table}[H]
%  \centering
%  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
%    coarse-pr & fine-pr & coarse-roc & fine-roc & coarse-acc & fine-acc & coarse-f1 & fine-f1 \\ \midrule
%    0.898 & 0.905 & 0.905 & 0.899 & 0.767 & 0.912 & 0.259 & 0.404 \\
%    0.870 & 0.871 & 0.846 & 0.852 & 0.803 & 0.918 & 0.272 & 0.406 \\
%    0.897 & 0.908 & 0.895 & 0.902 & 0.793 & 0.919 & 0.287 & 0.449 \\
%    0.864 & 0.860 & 0.852 & 0.852 & 0.778 & 0.908 & 0.256 & 0.391 \\
%    0.855 & 0.864 & 0.859 & 0.857 & 0.795 & 0.920 & 0.269 & 0.423 \\
%    0.867 & 0.863 & 0.874 & 0.864 & 0.785 & 0.913 & 0.263 & 0.411 \\
%    0.871 & 0.882 & 0.873 & 0.879 & 0.784 & 0.910 & 0.269 & 0.404 \\
%    0.835 & 0.842 & 0.843 & 0.841 & 0.794 & 0.910 & 0.258 & 0.357 \\
%    0.869 & 0.878 & 0.868 & 0.875 & 0.785 & 0.914 & 0.265 & 0.418 \\
%    0.873 & 0.873 & 0.891 & 0.890 & 0.786 & 0.906 & 0.279 & 0.365 \\
%    avg 0.870 & avg 0.875 & avg 0.871 & avg 0.871 & avg 0.787 & avg 0.913 & avg 0.268 & avg 0.403 \\ \bottomrule
%  \end{tabular}
%  \caption{Here are the results for the logistic regression passive 10 folds.}
%  \label{tab:logReg}
%\end{table}
%\FloatBarrier
%
%\FloatBarrier
%\begin{table}[H]
%  \centering
%  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
%    coarse-tn & fine-tn & coarse-fp & fine-fp & coarse-fn & fine-fn & coarse-tp & fine-tp \\ \midrule
%    1460 & 1773 & 454 & 141 & 14 & 36 & 82 & 60 \\
%    1540 & 1790 & 374 & 124 & 22 & 40 & 74 & 56 \\
%    1509 & 1782 & 405 & 132 & 12 & 30 & 84 & 66 \\
%    1486 & 1767 & 428 & 147 & 19 & 37 & 77 & 59 \\
%    1521 & 1790 & 393 & 124 & 20 & 37 & 76 & 59 \\
%    1501 & 1774 & 413 & 140 & 19 & 35 & 77 & 61 \\
%    1496 & 1769 & 417 & 144 & 17 & 36 & 80 & 61 \\
%    1524 & 1780 & 389 & 133 & 25 & 47 & 72 & 50 \\
%    1498 & 1773 & 415 & 140 & 17 & 33 & 78 & 62 \\
%    1497 & 1767 & 416 & 146 & 13 & 42 & 83 & 54 \\
%    avg 1503.2 & avg 1776.5 & avg 410.4 & avg 137.1 & avg 17.8 & avg 37.3 & avg 78.3 & avg 58.8 \\ \bottomrule
%  \end{tabular}
%  \caption{Here are the results for the logistic regression confusion matrices. The main source of the advantage
%  for fine is from the decreased amount of false negatives.}
%  \label{tab:logReg}
%\end{table}
%\FloatBarrier

\FloatBarrier
\begin{table}[H]
\centering
\begin{tabular}{|l||l||l||l||l||l||l|}\toprule
title & pr & roc & acc & f1 & conf (tn/fn) & conf (fp/tp) \\ \midrule
coarse & 0.870 & 0.871 & 0.787 & 0.268 & ( 1503.2 / 17.8 ) & ( 410.4 / 78.3 ) \\
fine & 0.875 & 0.871 & 0.913 & 0.403 & ( 1776.5 / 37.3 ) & ( 137.1 / 58.8 ) \\ \bottomrule
\end{tabular}
\caption{LogReg entire dataset results after parameter tuning}
\label{tab:LogRegAll-Wt23}
\end{table}
\FloatBarrier



%
%
%\FloatBarrier
%\begin{table}[H]
%  \centering
%  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
%    coarse-pr & fine-pr & coarse-roc & fine-roc & coarse-acc & fine-acc & coarse-f1 & fine-f1 \\ \midrule
%    0.894 & 0.914 & 0.890 & 0.900 & 0.862 & 0.944 & 0.351 & 0.525 \\
%    0.896 & 0.902 & 0.886 & 0.888 & 0.870 & 0.948 & 0.359 & 0.514 \\
%    0.885 & 0.889 & 0.856 & 0.859 & 0.865 & 0.941 & 0.333 & 0.482 \\
%    0.893 & 0.893 & 0.881 & 0.874 & 0.859 & 0.942 & 0.331 & 0.496 \\
%    0.886 & 0.884 & 0.872 & 0.872 & 0.865 & 0.932 & 0.340 & 0.445 \\
%    0.879 & 0.887 & 0.863 & 0.870 & 0.868 & 0.943 & 0.342 & 0.472 \\
%    0.902 & 0.899 & 0.892 & 0.886 & 0.868 & 0.945 & 0.342 & 0.488 \\
%    0.879 & 0.896 & 0.859 & 0.875 & 0.860 & 0.943 & 0.332 & 0.491 \\
%    0.911 & 0.913 & 0.904 & 0.903 & 0.871 & 0.943 & 0.360 & 0.482 \\
%    0.891 & 0.901 & 0.898 & 0.890 & 0.874 & 0.940 & 0.375 & 0.452 \\
%    avg 0.892 & avg 0.898 & avg 0.880 & avg 0.882 & avg 0.866 & avg 0.942 & avg 0.347 & avg 0.485 \\ \bottomrule
%  \end{tabular}
%  \caption{Here are the results for the SVM passive 10 folds.}
%  \label{tab:SVM}
%\end{table}
%\FloatBarrier
%
%\FloatBarrier
%\begin{table}[H]
%  \centering
%  \begin{tabular}{|l||l||l||l||l||l||l||l|}\toprule
%    coarse-tn & fine-tn & coarse-fp & fine-fp & coarse-fn & fine-fn & coarse-tp & fine-tp \\ \midrule
%    1658 & 1836 & 256 & 78 & 21 & 34 & 75 & 62 \\
%    1676 & 1851 & 238 & 63 & 23 & 41 & 73 & 55 \\
%    1670 & 1837 & 244 & 77 & 28 & 41 & 68 & 55 \\
%    1657 & 1837 & 257 & 77 & 26 & 39 & 70 & 57 \\
%    1668 & 1818 & 246 & 96 & 26 & 41 & 70 & 55 \\
%    1676 & 1845 & 238 & 69 & 27 & 45 & 69 & 51 \\
%    1676 & 1846 & 237 & 67 & 28 & 44 & 69 & 53 \\
%    1658 & 1841 & 255 & 72 & 27 & 42 & 70 & 55 \\
%    1676 & 1841 & 237 & 72 & 22 & 42 & 73 & 53 \\
%    1680 & 1838 & 233 & 75 & 20 & 46 & 76 & 50 \\
%    avg 1669.5 & avg 1839.0 & avg 244.1 & avg 74.6 & avg 24.8 & avg 41.5 & avg 71.3 & avg 54.6 \\ \bottomrule
%  \end{tabular}
%  \caption{Here are the results for the SVM confusion matrices. Here the fine returns less false negatives
%  than the coarse, but not as many true positives compared to coarse.}
%  \label{tab:SVM}
%\end{table}
%\FloatBarrier

\FloatBarrier
\begin{table}[H]
\centering
\begin{tabular}{|l||l||l||l||l||l||l|}\toprule
title & pr & roc & acc & f1 & conf (tn/fn) & conf (fp/tp) \\ \midrule
coarse & 0.892 & 0.880 & 0.866 & 0.347 & ( 1669.5 / 24.8 ) & ( 244.1 / 71.3 ) \\
fine & 0.898 & 0.882 & 0.942 & 0.485 & ( 1839.0 / 41.5 ) & ( 74.6 / 54.6 ) \\ \bottomrule
\end{tabular}
\caption{SVM entire dataset results after parameter tuning}
\label{tab:SVM-All}
\end{table}
\FloatBarrier



\FloatBarrier
\begin{table}[h]
  \small
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l||l||l||l||l|}\toprule
    Titles & fld1 & fld2 & fld3 & fld4 & fld5 & fld6 & fld7 & fld8 & fld9 & fld10 & Avg \\ \midrule
    fineErr & 111 & 112 & 106 & 114 & 107 & 123 & 121 & 120 & 123 & 134 & 117.1 \\
    coarseErr & 468 & 396 & 418 & 447 & 413 & 432 & 434 & 414 & 433 & 429 & 428.4 \\
    total & 579 & 508 & 524 & 561 & 520 & 555 & 555 & 534 & 556 & 563 & 545.5 \\
    jaccardInd & 0.15 & 0.2 & 0.16 & 0.16 & 0.17 & 0.19 & 0.18 & 0.19 & 0.19 & 0.2 & 0.18 \\
    InterSect &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 63 & 62 & 62 & 61 & 55 & 69 & 69 & 60 & 69 & 79 & 64.9 \\
    1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 2 & 1 & 0.80 \\
    2 & 5 & 10 & 5 & 10 & 6 & 3 & 5 & 10 & 6 & 5 & 6.5 \\
    3 & 5 & 10 & 4 & 3 & 8 & 10 & 8 & 8 & 6 & 4 & 6.6 \\
    4 & 3 & 0 & 1 & 1 & 2 & 4 & 1 & 3 & 1 & 2 & 1.8 \\
    5 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.10 \\
    6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.00 \\
    7 & 1 & 2 & 2 & 3 & 2 & 1 & 1 & 2 & 3 & 1 & 1.8 \\
    8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.10 \\
    Total & 77 & 84 & 74 & 79 & 75 & 88 & 85 & 85 & 87 & 92 & 82.6 \\
    Coarse &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 454 & 374 & 406 & 428 & 393 & 413 & 417 & 389 & 415 & 416 & 410.5 \\
    1 & 0 & 0 & 0 & 1 & 1 & 1 & 2 & 1 & 2 & 1 & 0.90 \\
    2 & 5 & 10 & 5 & 10 & 6 & 3 & 5 & 10 & 6 & 5 & 6.5 \\
    3 & 5 & 10 & 4 & 3 & 8 & 10 & 8 & 8 & 6 & 4 & 6.6 \\
    4 & 3 & 0 & 1 & 1 & 2 & 4 & 1 & 3 & 1 & 2 & 1.8 \\
    5 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0.10 \\
    6 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.00 \\
    7 & 1 & 2 & 2 & 4 & 2 & 1 & 1 & 2 & 3 & 1 & 1.9 \\
    8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0.10 \\
    Total & 468 & 396 & 418 & 447 & 413 & 432 & 434 & 414 & 433 & 429 & 428.4 \\
    Fine &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 65 & 63 & 63 & 61 & 55 & 71 & 72 & 61 & 72 & 77 & 66.0 \\
    1 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 2 & 1 & 0.90 \\
    2 & 11 & 16 & 13 & 16 & 16 & 11 & 10 & 15 & 15 & 13 & 13.6 \\
    3 & 18 & 20 & 11 & 21 & 17 & 17 & 17 & 23 & 15 & 21 & 18.0 \\
    4 & 6 & 3 & 7 & 5 & 6 & 10 & 11 & 10 & 8 & 7 & 7.3 \\
    5 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0.50 \\
    6 & 3 & 3 & 8 & 3 & 2 & 5 & 5 & 4 & 4 & 4 & 4.1 \\
    7 & 6 & 5 & 4 & 5 & 5 & 5 & 4 & 3 & 4 & 5 & 4.6 \\
    8 & 0 & 2 & 0 & 2 & 4 & 2 & 0 & 3 & 3 & 5 & 2.1 \\
    Total & 111 & 112 & 106 & 114 & 107 & 123 & 121 & 120 & 123 & 134 & 117.1 \\ \bottomrule
  \end{tabular}
  \caption{The jaccard results for Logistic Regression. Of the 117 instances in the
  Fine error set, 82 match with instances in the Coarse error set. The Jaccard index is 0.18,
  this compares to a higher Jaccard index of 0.43 in the SVM results. The difference is mainly
  due to the higher number of instances in the Coarse error set in the Log Reg results, 428 compared
  to 123. The error sets and the intersection sets contain representative samples of all the classes.}
  \label{tab:SVM}
\end{table}
\FloatBarrier



\FloatBarrier
\begin{table}[h]
  \small
  \centering
  \begin{tabular}{|l||l||l||l||l||l||l||l||l||l||l||l|}\toprule
    Titles & fld1 & fld2 & fld3 & fld4 & fld5 & fld6 & fld7 & fld8 & fld9 & fld10 & Avg \\ \midrule
    fineErr & 87 & 86 & 82 & 89 & 94 & 89 & 83 & 85 & 81 & 83 & 85.9 \\
    coarseErr & 120 & 119 & 112 & 130 & 138 & 132 & 116 & 124 & 116 & 124 & 123.1 \\
    total & 207 & 205 & 194 & 219 & 232 & 221 & 199 & 209 & 197 & 207 & 209.0 \\
    jaccardInd & 0.41 & 0.4 & 0.47 & 0.42 & 0.41 & 0.41 & 0.47 & 0.45 & 0.44 & 0.45 & 0.43 \\
    InterSect &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 27 & 23 & 19 & 27 & 22 & 22 & 20 & 23 & 22 & 19 & 22.4 \\
    1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0.90 \\
    2 & 11 & 12 & 13 & 12 & 14 & 12 & 15 & 12 & 12 & 12 & 12.5 \\
    3 & 11 & 9 & 14 & 10 & 10 & 15 & 12 & 16 & 12 & 15 & 12.4 \\
    4 & 3 & 8 & 7 & 7 & 10 & 10 & 8 & 8 & 4 & 8 & 7.3 \\
    5 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0.50 \\
    6 & 2 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 0.50 \\
    7 & 2 & 4 & 3 & 5 & 4 & 3 & 4 & 2 & 4 & 4 & 3.5 \\
    8 & 3 & 1 & 4 & 3 & 5 & 1 & 4 & 2 & 3 & 4 & 3.0 \\
    Total & 60 & 59 & 62 & 65 & 67 & 64 & 64 & 65 & 60 & 64 & 63.0 \\
    Coarse &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 86 & 81 & 67 & 92 & 92 & 87 & 71 & 80 & 77 & 78 & 81.1 \\
    1 & 1 & 1 & 1 & 1 & 1 & 0 & 2 & 1 & 2 & 1 & 1.1 \\
    2 & 11 & 12 & 14 & 12 & 14 & 12 & 15 & 12 & 12 & 12 & 12.6 \\
    3 & 11 & 10 & 14 & 10 & 10 & 16 & 12 & 17 & 12 & 15 & 12.7 \\
    4 & 4 & 9 & 8 & 7 & 10 & 10 & 8 & 8 & 4 & 9 & 7.7 \\
    5 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0.50 \\
    6 & 2 & 0 & 0 & 0 & 0 & 2 & 0 & 2 & 1 & 0 & 0.70 \\
    7 & 2 & 4 & 3 & 5 & 5 & 3 & 4 & 2 & 4 & 4 & 3.6 \\
    8 & 3 & 1 & 4 & 3 & 5 & 2 & 4 & 2 & 3 & 4 & 3.1 \\
    Total & 120 & 119 & 112 & 130 & 138 & 132 & 116 & 124 & 116 & 124 & 123.1 \\
    Fine &  &  &  &  &  &  &  &  &  &  &  \\
    0 & 33 & 26 & 21 & 32 & 28 & 25 & 21 & 26 & 25 & 23 & 26.0 \\
    1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 0.90 \\
    2 & 15 & 18 & 16 & 16 & 15 & 18 & 17 & 16 & 16 & 17 & 16.4 \\
    3 & 18 & 15 & 19 & 19 & 22 & 19 & 17 & 23 & 19 & 19 & 19.0 \\
    4 & 7 & 11 & 11 & 9 & 12 & 12 & 12 & 11 & 8 & 10 & 10.3 \\
    5 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 & 0.60 \\
    6 & 4 & 6 & 3 & 1 & 3 & 3 & 4 & 2 & 1 & 3 & 3.0 \\
    7 & 2 & 6 & 3 & 6 & 5 & 5 & 6 & 3 & 4 & 4 & 4.4 \\
    8 & 7 & 2 & 7 & 4 & 7 & 7 & 5 & 3 & 6 & 5 & 5.3 \\
    Total & 87 & 86 & 82 & 89 & 94 & 89 & 83 & 85 & 81 & 83 & 85.9 \\ \bottomrule
  \end{tabular}
  \caption{The jaccard results for SVM. The fine error set has a total of 86 instances, with
  63 of those matching with the coarse set. Similar to the Log Reg jaccard results,
  the error sets and the intersection sets contain representative samples of all the classes.}
  \label{tab:SVM}
\end{table}
\FloatBarrier








\par Comparing the Log Reg coarse error set to the SVM coarse error set had
an intersection of 0.

\par Comparing the Log Reg fine error set to the SVM fine error set had
an intersection of 0.

\par Comparing the Log Reg fine error set to the SVM coarse error set had
an intersection of 0.

\par Comparing the Log Reg coarse error set to the SVM fine error set had
an intersection of 0.
