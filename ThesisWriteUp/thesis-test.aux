\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Contents}{v}{section*.1}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\newlabel{chap:aenied}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Machine Learning}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Hierarchical Bioinformatics Data Set}{1}{section.1.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Mito_tree}{{\M@TitleReference {\caption@xref {fig:Mito_tree}{ on input line 172}}{Hierarchical Bioinformatics Data Set}}{2}{Hierarchical Bioinformatics Data Set}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The protein dataset is labeled according to where it originates in the cell. At the root is “mitochondrion”, then there is the sub level labels for if its native to the mitochondria or if it has a separate target compartment specification. The complete tree along with the number of instances belonging to the each label is included in \ref  {fig:Mito_tree}.\relax }}{2}{figure.caption.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Coarse Grained vs Fine Grained Trade Off}{2}{section.1.3}}
\newlabel{fig:union}{{\M@TitleReference {\caption@xref {fig:union}{ on input line 200}}{Coarse Grained vs Fine Grained Trade Off}}{3}{Coarse Grained vs Fine Grained Trade Off}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Demonstration of a dataset that would benefit from multiple fine grained learners for each circle type. In order for the coarse grain learner to have high recall, precision must be scarified and a large amount of false positives returned. By combining fine grained classifiers the same level of recall can be achieved with a higher level of precision because none of the false positive diamonds will be returned\relax }}{3}{figure.caption.3}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background and Related Work}{4}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Active Learning}{4}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Other Papers cited by Yugi}{4}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Hierarchical Active Learning}{5}{section.2.3}}
\newlabel{fig:AL2}{{\M@TitleReference {\caption@xref {fig:AL2}{ on input line 248}}{Hierarchical Active Learning}}{5}{Hierarchical Active Learning}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Diagram of HAL approach\relax }}{5}{figure.caption.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Application to Dispatch Dataset}{5}{section.2.4}}
\newlabel{fig:draft-richmond}{{\M@TitleReference {\caption@xref {fig:draft-richmond}{ on input line 274}}{Application to Dispatch Dataset}}{6}{Application to Dispatch Dataset}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Application of HAL demonstrating the benefit of Actively selecting the type of labels to purchase for instances rather than randomly selecting labels to purchase, as in the Passive curves.\relax }}{6}{figure.caption.5}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Bio HAL Application}{7}{chapter.3}}
\newlabel{chap:math}{{\M@TitleReference {3}{Bio HAL Application}}{7}{Bio HAL Application}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Training and Testing Coarse Grain and Fine Grain Classifiers}{7}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces This is what the dataset looks like there are 20098 instances total with 450 features each.\relax }}{7}{table.caption.6}}
\newlabel{tab:ClassesAll}{{\M@TitleReference {3.1}{This is what the dataset looks like there are 20098 instances total with 450 features each.\relax }}{7}{This is what the dataset looks like there are 20098 instances total with 450 features each.\relax }{table.caption.6}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces This is what the folds of the dataset look like.\relax }}{8}{table.caption.7}}
\newlabel{tab:partitions}{{\M@TitleReference {3.2}{This is what the folds of the dataset look like.\relax }}{8}{This is what the folds of the dataset look like.\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces This is what the train and test set look like.\relax }}{8}{table.caption.8}}
\newlabel{tab:TrainTest}{{\M@TitleReference {3.3}{This is what the train and test set look like.\relax }}{8}{This is what the train and test set look like.\relax }{table.caption.8}{}}
\@writefile{brf}{\backcite{scikit-learn}{{8}{3.1}{table.caption.8}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Passive SVM Rbf kernel vs Logistic Reg}{10}{section.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Here are the results for the logistic regression passive 10 folds.\relax }}{10}{table.caption.9}}
\newlabel{tab:logReg}{{\M@TitleReference {3.4}{Here are the results for the logistic regression passive 10 folds.\relax }}{10}{Here are the results for the logistic regression passive 10 folds.\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Here are the results for the logistic regression confusion matrices. The main source of the advantage for fine is from the decreased amount of false negatives.\relax }}{10}{table.caption.10}}
\newlabel{tab:logReg}{{\M@TitleReference {3.5}{Here are the results for the logistic regression confusion matrices. The main source of the advantage for fine is from the decreased amount of false negatives.\relax }}{10}{Here are the results for the logistic regression confusion matrices. The main source of the advantage for fine is from the decreased amount of false negatives.\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Here are the results for the SVM passive 10 folds.\relax }}{11}{table.caption.11}}
\newlabel{tab:SVM}{{\M@TitleReference {3.6}{Here are the results for the SVM passive 10 folds.\relax }}{11}{Here are the results for the SVM passive 10 folds.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Here are the results for the SVM confusion matrices. Here the fine returns less false negatives than the coarse, but not as many true positives compared to coarse.\relax }}{11}{table.caption.12}}
\newlabel{tab:SVM}{{\M@TitleReference {3.7}{Here are the results for the SVM confusion matrices. Here the fine returns less false negatives than the coarse, but not as many true positives compared to coarse.\relax }}{11}{Here are the results for the SVM confusion matrices. Here the fine returns less false negatives than the coarse, but not as many true positives compared to coarse.\relax }{table.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Fine has a higher accuracy than coarse at the default threshold.\relax }}{12}{figure.caption.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The fine threshold occurs at a point on the pr-curve associated with a higher f-measure than the coarse curves.\relax }}{12}{figure.caption.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces SVM results are similar between coarse and fine.\relax }}{13}{figure.caption.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces SVM results for PR-curves and F-measure have coarse and fine picking different parts of the curves for their respective thresholds, coarse f1 avg is slightly higher at 0.468 compared to 0.457 for fine.\relax }}{13}{figure.caption.16}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces The jaccard results for Logistic Regression. Of the 117 instances in the Fine error set, 82 match with instances in the Coarse error set. The Jaccard index is 0.18, this compares to a higher Jaccard index of 0.43 in the SVM results. The difference is mainly due to the higher number of instances in the Coarse error set in the Log Reg results, 428 compared to 123. The error sets and the intersection sets contain representative samples of all the classes.\relax }}{14}{table.caption.17}}
\newlabel{tab:SVM}{{\M@TitleReference {3.8}{The jaccard results for Logistic Regression. Of the 117 instances in the Fine error set, 82 match with instances in the Coarse error set. The Jaccard index is 0.18, this compares to a higher Jaccard index of 0.43 in the SVM results. The difference is mainly due to the higher number of instances in the Coarse error set in the Log Reg results, 428 compared to 123. The error sets and the intersection sets contain representative samples of all the classes.\relax }}{14}{The jaccard results for Logistic Regression. Of the 117 instances in the Fine error set, 82 match with instances in the Coarse error set. The Jaccard index is 0.18, this compares to a higher Jaccard index of 0.43 in the SVM results. The difference is mainly due to the higher number of instances in the Coarse error set in the Log Reg results, 428 compared to 123. The error sets and the intersection sets contain representative samples of all the classes.\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces The jaccard results for SVM. The fine error set has a total of 86 instances, with 63 of those matching with the coarse set. Similar to the Log Reg jaccard results, the error sets and the intersection sets contain representative samples of all the classes.\relax }}{15}{table.caption.18}}
\newlabel{tab:SVM}{{\M@TitleReference {3.9}{The jaccard results for SVM. The fine error set has a total of 86 instances, with 63 of those matching with the coarse set. Similar to the Log Reg jaccard results, the error sets and the intersection sets contain representative samples of all the classes.\relax }}{15}{The jaccard results for SVM. The fine error set has a total of 86 instances, with 63 of those matching with the coarse set. Similar to the Log Reg jaccard results, the error sets and the intersection sets contain representative samples of all the classes.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Active vs Passive curves}{16}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Plots for Logistic Regression Active vs Passive curves}{16}{subsection.3.3.1}}
\newlabel{fig:ActiveVsPassivePRLR}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassivePRLR}{ on input line 735}}{Plots for Logistic Regression Active vs Passive curves}}{17}{Plots for Logistic Regression Active vs Passive curves}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The PR AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with active-fine having the highest performance. Active-coarse outperforms passive-coarse. Passive-fine doesn't outperform the coarse classifiers until rnd 100. \relax }}{17}{figure.caption.19}}
\newlabel{fig:ActiveVsPassiveROCLR}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveROCLR}{ on input line 747}}{Plots for Logistic Regression Active vs Passive curves}}{18}{Plots for Logistic Regression Active vs Passive curves}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces The ROC AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both coarse and fine. Coarse roc starts with an advantage over fine as in the PR curves. Both converge to the same rate after roc auc level after 80.\relax }}{18}{figure.caption.20}}
\newlabel{fig:ActiveVsPassiveAccLR}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveAccLR}{ on input line 759}}{Plots for Logistic Regression Active vs Passive curves}}{19}{Plots for Logistic Regression Active vs Passive curves}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The accuracy of the fine classifiers stays at roughly the same rate throughout the rounds, this is due to an effective weighting scheme for the fine grained classifiers. The active coarse accuracy drops towards the end due to an increase in false positives as more negative instances are added in the later rounds.\relax }}{19}{figure.caption.21}}
\newlabel{fig:ActiveVsPassiveF1LR}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveF1LR}{ on input line 772}}{Plots for Logistic Regression Active vs Passive curves}}{20}{Plots for Logistic Regression Active vs Passive curves}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The F-measure of the the fine classifiers increases throughout the rounds as more true positives are predicted. The active coarse again decreases at later rounds due to increased false positives.\relax }}{20}{figure.caption.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Plots for SVM Active vs Passive curves}{20}{subsection.3.3.2}}
\newlabel{fig:ActiveVsPassivePRSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassivePRSVM}{ on input line 792}}{Plots for SVM Active vs Passive curves}}{21}{Plots for SVM Active vs Passive curves}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The PR AUC curves for rounds with SVM show little advantage for fine. The results are slightly different than the ones shown on 2/14 due to fixing a bug with the code that wasn't performing the preprocessing scaling for the SVM case at the same stage as it was being done for the logistic regression classifier.\relax }}{21}{figure.caption.23}}
\newlabel{fig:ActiveVsPassiveROCSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveROCSVM}{ on input line 805}}{Plots for SVM Active vs Passive curves}}{22}{Plots for SVM Active vs Passive curves}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces The ROC curves show more of an advantage for coarse classifiers.\relax }}{22}{figure.caption.24}}
\newlabel{fig:ActiveVsPassiveAccSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveAccSVM}{ on input line 815}}{Plots for SVM Active vs Passive curves}}{23}{Plots for SVM Active vs Passive curves}{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The accuracy for the coarse decreases sharply due to coarse predicting steadily more false positives, behaving similar to the Log Reg case. Fine accuracy is higher due to predicting less false positives than coarse. Fine also predicts less true positives, compare apx. 37 to apx. 60 t.p. for coarse at round 60.\relax }}{23}{figure.caption.25}}
\newlabel{fig:ActiveVsPassiveF1SVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveF1SVM}{ on input line 828}}{Plots for SVM Active vs Passive curves}}{24}{Plots for SVM Active vs Passive curves}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces The F-measure favors coarse, and trends to the same level for both coarse and fine.\relax }}{24}{figure.caption.26}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Plots for FFR experiments}{24}{section.3.4}}
\newlabel{fig:FFR_PR_Cost1}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost1}{ on input line 842}}{Plots for FFR experiments}}{25}{Plots for FFR experiments}{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces The strategy is changed from purchasing a set number of instances per round to having a set budget per round and spending a portion of that budget on fine and coarse grained labels. For this curve the fine and coarse grain labels both have a cost of 1. The purple 1.0 curve shows that if only fine grained labels are purchased, the highest performing PR-AUC can be obtained. The results are an average of 10 folds.\relax }}{25}{figure.caption.27}}
\newlabel{fig:FFR_PR_Cost16_rnds0_171}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost16_rnds0_171}{ on input line 856}}{Plots for FFR experiments}}{26}{Plots for FFR experiments}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces The fine cost is increased to 16. The budget for each iteration is 100, and for the case of the 0.5 curve, 50 instances are bought for coarse and 3.125 instances are bought for fine. The remainder 0.125 is then turned into a 0.125 chance for any round to purchase an extra fine label. The round size for the FFR 1.0 curve is very small, with only 7 labels purchased per iteration. The cost is to high for the fine label advantage to offset the decreased number of instances purchased.\relax }}{26}{figure.caption.28}}
\newlabel{fig:FFR_PR_Cost16_rnds0_500}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost16_rnds0_500}{ on input line 871}}{Plots for FFR experiments}}{27}{Plots for FFR experiments}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces This shows the iterations continuing through round 500, the curves with the higher fine rates eventually settle to the same end point that the curves with the high rates of coarse labels purchased achieved at previous iterations.\relax }}{27}{figure.caption.29}}
\newlabel{fig:FFR_PR_Cost8_rnds0_171}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost8_rnds0_171}{ on input line 885}}{Plots for FFR experiments}}{28}{Plots for FFR experiments}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces At fine cost 8 the FFR 0.0 rate is no longer the best option, 0.1 generally outperforms 0.0 slightly.\relax }}{28}{figure.caption.30}}
\newlabel{fig:FFR_PR_Cost8_rnds0_500}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost8_rnds0_500}{ on input line 896}}{Plots for FFR experiments}}{29}{Plots for FFR experiments}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces The extended picture of the FFR cost 8. The round size for FFR 1.0 is small, only 13 instances purchase per iteration.\relax }}{29}{figure.caption.31}}
\newlabel{fig:FFR_PR_Cost4_rnds0_171}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost4_rnds0_171}{ on input line 907}}{Plots for FFR experiments}}{30}{Plots for FFR experiments}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces At fine cost 4, FFR 0.3 appears to be the highest performing rate.\relax }}{30}{figure.caption.32}}
\newlabel{fig:FFR_PR_Cost4_rnds20_60}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost4_rnds20_60}{ on input line 917}}{Plots for FFR experiments}}{31}{Plots for FFR experiments}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces The fine cost 4 curves shown expanding the rounds 20-60.\relax }}{31}{figure.caption.33}}
\newlabel{fig:FFR_PR_Cost2_rnds0_171}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost2_rnds0_171}{ on input line 927}}{Plots for FFR experiments}}{32}{Plots for FFR experiments}{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces At fine cost 2, the preferred rate jumps up to 0.8, similar to the cost 1 results.\relax }}{32}{figure.caption.34}}
\newlabel{fig:FFR_PR_Cost2_rnds20_60}{{\M@TitleReference {\caption@xref {fig:FFR_PR_Cost2_rnds20_60}{ on input line 937}}{Plots for FFR experiments}}{33}{Plots for FFR experiments}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces The fine cost 2 curves shown expanding rounds 20-60.\relax }}{33}{figure.caption.35}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Conclusions and Future Work}{34}{chapter.4}}
\newlabel{chap:math}{{\M@TitleReference {4}{Conclusions and Future Work}}{34}{Conclusions and Future Work}{chapter.4}{}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,nuthesis}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {A}Tuning the fine grained classes}{35}{appendix.A}}
\bibcite{scikit-learn}{1}
\bibcite{Merialdo2001}{2}
\bibcite{lsu-lsal-13}{3}
\bibcite{Dasgupta2008}{4}
\bibcite{Ling2012fine}{5}
\bibcite{sklearn_api}{6}
\citation{*}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{36}{section*.37}}
\memsetcounter{lastsheet}{41}
\memsetcounter{lastpage}{36}
