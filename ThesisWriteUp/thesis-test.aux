\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Contents}{v}{section*.1}}
\citation{bioPoster}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\@writefile{brf}{\backcite{bioPoster}{{1}{1}{chapter.1}}}
\citation{mitchell}
\citation{sklearn-api}
\citation{Coursera}
\citation{scikit-learn}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Machine Learning}{2}{section.1.1}}
\@writefile{brf}{\backcite{mitchell}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{sklearn-api}{{2}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{Coursera}{{2}{1.1}{section.1.1}}}
\newlabel{eq:logit}{{1.1}{2}{Machine Learning}{equation.1.1.1}{}}
\citation{scikit-learn}
\citation{ethem}
\citation{scikit-learn}
\citation{scikit-learn}
\citation{scikit-learn}
\@writefile{brf}{\backcite{scikit-learn}{{3}{1.1}{equation.1.1.1}}}
\@writefile{brf}{\backcite{scikit-learn}{{3}{1.1}{equation.1.1.1}}}
\newlabel{eq:costlogit}{{1.2}{3}{Machine Learning}{equation.1.1.2}{}}
\@writefile{brf}{\backcite{scikit-learn}{{3}{1.1}{equation.1.1.2}}}
\@writefile{brf}{\backcite{ethem}{{3}{1.1}{equation.1.1.2}}}
\newlabel{eq:svmfcn}{{1.3}{3}{Machine Learning}{equation.1.1.3}{}}
\newlabel{eq:kernelfcn}{{1.4}{3}{Machine Learning}{equation.1.1.4}{}}
\@writefile{brf}{\backcite{scikit-learn}{{3}{1.1}{equation.1.1.4}}}
\citation{scikit-learn}
\@writefile{brf}{\backcite{scikit-learn}{{4}{1.1}{equation.1.1.4}}}
\@writefile{brf}{\backcite{scikit-learn}{{4}{1.1}{equation.1.1.4}}}
\@writefile{brf}{\backcite{scikit-learn}{{4}{1.1}{equation.1.1.4}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Evaluating Classifier Performance}{4}{section.1.2}}
\citation{bioPoster}
\citation{bioPoster}
\citation{mitoproteome}
\citation{UniProt}
\citation{bioPoster}
\newlabel{eq:precision}{{1.5}{5}{Evaluating Classifier Performance}{equation.1.2.5}{}}
\newlabel{eq:recall}{{1.6}{5}{Evaluating Classifier Performance}{equation.1.2.6}{}}
\newlabel{eq:fmes}{{1.7}{5}{Evaluating Classifier Performance}{equation.1.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Hierarchical Bioinformatics Data Set}{5}{section.1.3}}
\citation{bioPoster}
\citation{PROFEAT1}
\citation{Cui2}
\citation{PROSO3}
\citation{Phobius4}
\citation{PSSCP5}
\citation{SignalP6}
\citation{TMBHunt7}
\citation{NetOgly8}
\citation{TatP9}
\@writefile{brf}{\backcite{bioPoster}{{6}{1.3}{section.1.3}}}
\@writefile{brf}{\backcite{bioPoster}{{6}{1.3}{section.1.3}}}
\@writefile{brf}{\backcite{mitoproteome}{{6}{1.3}{section.1.3}}}
\@writefile{brf}{\backcite{UniProt}{{6}{1.3}{section.1.3}}}
\@writefile{brf}{\backcite{bioPoster}{{6}{1.3}{section.1.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Features of the protein dataset along with their respective sources.\relax }}{7}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:featureList}{{\M@TitleReference {1.1}{Features of the protein dataset along with their respective sources.\relax }}{7}{Features of the protein dataset along with their respective sources.\relax }{table.caption.2}{}}
\@writefile{brf}{\backcite{bioPoster}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{PROFEAT1}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{Cui2}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{PROSO3}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{Phobius4}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{PSSCP5}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{SignalP6}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{TMBHunt7}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{NetOgly8}{{7}{1.1}{table.caption.2}}}
\@writefile{brf}{\backcite{TatP9}{{7}{1.1}{table.caption.2}}}
\citation{yugi}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The protein dataset hierarchy of labels along with the instance count for each label.\relax }}{8}{figure.caption.3}}
\newlabel{fig:Mitotree}{{\M@TitleReference {1.1}{The protein dataset hierarchy of labels along with the instance count for each label.\relax }}{8}{The protein dataset hierarchy of labels along with the instance count for each label.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Coarse-grained vs Fine-grained Trade Off}{8}{section.1.4}}
\@writefile{brf}{\backcite{yugi}{{8}{1.4}{section.1.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Demonstration of a dataset that would benefit from multiple fine-grained learners for each circle type.\relax }}{9}{figure.caption.4}}
\newlabel{fig:union}{{\M@TitleReference {1.2}{Demonstration of a dataset that would benefit from multiple fine-grained learners for each circle type.\relax }}{9}{Demonstration of a dataset that would benefit from multiple fine-grained learners for each circle type.\relax }{figure.caption.4}{}}
\citation{mitchell}
\citation{Lewis2004}
\citation{yugi}
\citation{Lewis2004}
\citation{Lewis2004}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background and Theory}{10}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Active Over-Labeling}{10}{section.2.1}}
\newlabel{sect:activeOverLabel}{{\M@TitleReference {2.1}{Active Over-Labeling}}{10}{Active Over-Labeling}{section.2.1}{}}
\@writefile{brf}{\backcite{mitchell}{{10}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{Lewis2004}{{10}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{yugi}{{11}{2.1}{section.2.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A labeling tree based on the text categorization dataset RCV1 \cite  {Lewis2004}.\relax }}{11}{figure.caption.5}}
\@writefile{brf}{\backcite{Lewis2004}{{11}{2.1}{figure.caption.5}}}
\newlabel{fig:exp-ontology}{{\M@TitleReference {2.1}{A labeling tree based on the text categorization dataset RCV1 \cite  {Lewis2004}.\relax }}{11}{A labeling tree based on the text categorization dataset RCV1 \cite {Lewis2004}.\relax }{figure.caption.5}{}}
\citation{bioPoster}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Hierarchical Active Learning}{12}{section.2.2}}
\@writefile{brf}{\backcite{bioPoster}{{12}{2.2}{section.2.2}}}
\citation{yugi}
\citation{yugi}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Diagram of HAL approach\relax }}{13}{figure.caption.6}}
\newlabel{fig:HALapproach}{{\M@TitleReference {2.2}{Diagram of HAL approach\relax }}{13}{Diagram of HAL approach\relax }{figure.caption.6}{}}
\newlabel{eq:maxcombine}{{2.1}{13}{Hierarchical Active Learning}{equation.2.2.1}{}}
\newlabel{eq:uncert}{{2.2}{13}{Hierarchical Active Learning}{equation.2.2.2}{}}
\citation{yugi}
\citation{Auer2002}
\@writefile{brf}{\backcite{yugi}{{14}{1}{algorithm.1}}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Method hierarchical active learning for a fixed fine-grained ratio (FFR) \cite  {yugi}. See text for Purchase and LabelMap. \relax }}{14}{algorithm.1}}
\newlabel{alg:halalgo}{{\M@TitleReference {1}{Method hierarchical active learning for a fixed fine-grained ratio (FFR) \cite  {yugi}. See text for Purchase and LabelMap. \relax }}{14}{Method hierarchical active learning for a fixed fine-grained ratio (FFR) \cite {yugi}. See text for Purchase and LabelMap. \relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Dynamically Adapting Purchase Proportions}{14}{section.2.3}}
\newlabel{sect:BANDIT}{{\M@TitleReference {2.3}{Dynamically Adapting Purchase Proportions}}{14}{Dynamically Adapting Purchase Proportions}{section.2.3}{}}
\@writefile{brf}{\backcite{yugi}{{14}{2.3}{section.2.3}}}
\@writefile{brf}{\backcite{Auer2002}{{14}{2.3}{section.2.3}}}
\newlabel{eq:reward}{{2.3}{15}{Dynamically Adapting Purchase Proportions}{equation.2.3.3}{}}
\newlabel{eq:modReward}{{2.4}{15}{Dynamically Adapting Purchase Proportions}{equation.2.3.4}{}}
\citation{yugi}
\citation{mccallum1998improving}
\citation{jiang2013}
\citation{Rubens2011}
\citation{Merialdo2001}
\citation{Hofmann2003}
\citation{Schohn2000}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Related Work}{16}{chapter.3}}
\@writefile{brf}{\backcite{yugi}{{16}{3}{chapter.3}}}
\@writefile{brf}{\backcite{mccallum1998improving}{{16}{3}{chapter.3}}}
\@writefile{brf}{\backcite{jiang2013}{{16}{3}{chapter.3}}}
\@writefile{brf}{\backcite{Rubens2011}{{16}{3}{chapter.3}}}
\@writefile{brf}{\backcite{Merialdo2001}{{16}{3}{chapter.3}}}
\@writefile{brf}{\backcite{Hofmann2003}{{16}{3}{chapter.3}}}
\@writefile{brf}{\backcite{Schohn2000}{{16}{3}{chapter.3}}}
\citation{Dasgupta2008}
\citation{Symons2006}
\citation{yugi}
\citation{yugi}
\citation{yugi}
\citation{Lewis2004}
\@writefile{brf}{\backcite{Dasgupta2008, Symons2006}{{17}{3}{chapter.3}}}
\@writefile{brf}{\backcite{yugi}{{17}{3}{chapter.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Application to Dispatch Dataset}{17}{section.3.1}}
\@writefile{brf}{\backcite{yugi}{{17}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{yugi}{{17}{3.1}{section.3.1}}}
\@writefile{brf}{\backcite{Lewis2004}{{17}{3.1}{section.3.1}}}
\citation{yugi}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Application of HAL demonstrating the benefit of fine-grained active learning on the RCV1 dataset.\relax }}{18}{figure.caption.7}}
\newlabel{fig:draft-RCV1}{{\M@TitleReference {3.1}{Application of HAL demonstrating the benefit of fine-grained active learning on the RCV1 dataset.\relax }}{18}{Application of HAL demonstrating the benefit of fine-grained active learning on the RCV1 dataset.\relax }{figure.caption.7}{}}
\@writefile{brf}{\backcite{yugi}{{18}{3.1}{figure.caption.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces HAL and BANDIT experiments for RCV1 dataset.\relax }}{19}{figure.caption.8}}
\newlabel{fig:RCV1-16-nobandit-curve-RCV1}{{\M@TitleReference {3.2}{HAL and BANDIT experiments for RCV1 dataset.\relax }}{19}{HAL and BANDIT experiments for RCV1 dataset.\relax }{figure.caption.8}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Experimental Setup}{20}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Training and Testing Coarse-Grain and Fine-Grain Classifiers}{20}{section.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces This dataset contains 20098 instances total with 449 features each. An example partitioning is shown, some classes like 1 and 5 contain only 1-2 instances in a given test set. Note there is a heavy class imbalance with approx. 20 negative instances for each positive instance.\relax }}{21}{table.caption.9}}
\newlabel{tab:dataset}{{\M@TitleReference {4.1}{This dataset contains 20098 instances total with 449 features each. An example partitioning is shown, some classes like 1 and 5 contain only 1-2 instances in a given test set. Note there is a heavy class imbalance with approx. 20 negative instances for each positive instance.\relax }}{21}{This dataset contains 20098 instances total with 449 features each. An example partitioning is shown, some classes like 1 and 5 contain only 1-2 instances in a given test set. Note there is a heavy class imbalance with approx. 20 negative instances for each positive instance.\relax }{table.caption.9}{}}
\newlabel{tab:ClassesAll}{{\M@TitleReference {4.1a}{Classes\relax }}{21}{Classes\relax }{table.caption.9}{}}
\newlabel{sub@tab:ClassesAll}{{\M@TitleReference {a}{Classes\relax }}{21}{Classes\relax }{table.caption.9}{}}
\newlabel{tab:partitions}{{\M@TitleReference {4.1b}{Folds\relax }}{21}{Folds\relax }{table.caption.9}{}}
\newlabel{sub@tab:partitions}{{\M@TitleReference {b}{Folds\relax }}{21}{Folds\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Example of totals for the Train and Test corresponding to when the first fold is held out to be the test set.\relax }}{21}{table.caption.10}}
\newlabel{tab:TrainTest}{{\M@TitleReference {4.2}{Example of totals for the Train and Test corresponding to when the first fold is held out to be the test set.\relax }}{21}{Example of totals for the Train and Test corresponding to when the first fold is held out to be the test set.\relax }{table.caption.10}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The subset of instances used for tuning classifier parameters contains approximately $1/5$ and retains all positive instances.\relax }}{22}{table.caption.11}}
\newlabel{tab:subset}{{\M@TitleReference {4.3}{The subset of instances used for tuning classifier parameters contains approximately $1/5$ and retains all positive instances.\relax }}{22}{The subset of instances used for tuning classifier parameters contains approximately $1/5$ and retains all positive instances.\relax }{table.caption.11}{}}
\newlabel{tab:ClassesSub}{{\M@TitleReference {4.3a}{Classes Subset\relax }}{22}{Classes Subset\relax }{table.caption.11}{}}
\newlabel{sub@tab:ClassesSub}{{\M@TitleReference {a}{Classes Subset\relax }}{22}{Classes Subset\relax }{table.caption.11}{}}
\newlabel{tab:PartitionsSubset}{{\M@TitleReference {4.3b}{Folds Subset\relax }}{22}{Folds Subset\relax }{table.caption.11}{}}
\newlabel{sub@tab:PartitionsSubset}{{\M@TitleReference {b}{Folds Subset\relax }}{22}{Folds Subset\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Example totals for the train and test set for the subset of data. The subset of data is used for the majority of the parameter search.\relax }}{22}{table.caption.12}}
\newlabel{tab:subTrainTest}{{\M@TitleReference {4.4}{Example totals for the train and test set for the subset of data. The subset of data is used for the majority of the parameter search.\relax }}{22}{Example totals for the train and test set for the subset of data. The subset of data is used for the majority of the parameter search.\relax }{table.caption.12}{}}
\@writefile{brf}{\backcite{scikit-learn}{{23}{4.1}{table.caption.12}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces SVM default results without parameter selection or preprocessing. Where PR curve AUC is (pr), ROC curve AUC is (roc), accuracy is (acc), F1-measure is (f1).\relax }}{23}{table.caption.13}}
\newlabel{tab:SVMDefResStats}{{\M@TitleReference {4.5}{SVM default results without parameter selection or preprocessing. Where PR curve AUC is (pr), ROC curve AUC is (roc), accuracy is (acc), F1-measure is (f1).\relax }}{23}{SVM default results without parameter selection or preprocessing. Where PR curve AUC is (pr), ROC curve AUC is (roc), accuracy is (acc), F1-measure is (f1).\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces SVM default results confusion matrix. Where True Negatives is (tn), False Positives is (fp), False Negatives (fn), True Positives is (tp).\relax }}{24}{table.caption.14}}
\newlabel{tab:SVMDefConfMat}{{\M@TitleReference {4.6}{SVM default results confusion matrix. Where True Negatives is (tn), False Positives is (fp), False Negatives (fn), True Positives is (tp).\relax }}{24}{SVM default results confusion matrix. Where True Negatives is (tn), False Positives is (fp), False Negatives (fn), True Positives is (tp).\relax }{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces SVM default condensed view of summary performance metrics, each value is the average of 10 folds.\relax }}{24}{table.caption.15}}
\newlabel{tab:SVMDef}{{\M@TitleReference {4.7}{SVM default condensed view of summary performance metrics, each value is the average of 10 folds.\relax }}{24}{SVM default condensed view of summary performance metrics, each value is the average of 10 folds.\relax }{table.caption.15}{}}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Varying SVM Scaling Methods}{25}{subsection.4.1.1}}
\newlabel{sect:paramStart}{{\M@TitleReference {4.1.1}{Varying SVM Scaling Methods}}{25}{Varying SVM Scaling Methods}{subsection.4.1.1}{}}
\@writefile{brf}{\backcite{scikit-learn}{{25}{4.1.1}{subsection.4.1.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces SVM minimax-scaler results.\relax }}{25}{table.caption.16}}
\newlabel{tab:SVMMinMax}{{\M@TitleReference {4.8}{SVM minimax-scaler results.\relax }}{25}{SVM minimax-scaler results.\relax }{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces SVM norm-scaler results.\relax }}{25}{table.caption.17}}
\newlabel{tab:SVMNorm}{{\M@TitleReference {4.9}{SVM norm-scaler results.\relax }}{25}{SVM norm-scaler results.\relax }{table.caption.17}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces SVM std-scaler results. This option is chosen.\relax }}{26}{table.caption.18}}
\newlabel{tab:SVMStandard}{{\M@TitleReference {4.10}{SVM std-scaler results. This option is chosen.\relax }}{26}{SVM std-scaler results. This option is chosen.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Varying SVM Kernels}{26}{subsection.4.1.2}}
\@writefile{brf}{\backcite{scikit-learn}{{26}{4.1.2}{subsection.4.1.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Linear kernel results.\relax }}{26}{table.caption.19}}
\newlabel{tab:Linear}{{\M@TitleReference {4.11}{Linear kernel results.\relax }}{26}{Linear kernel results.\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces Poly degree 3 kernel results.\relax }}{26}{table.caption.20}}
\newlabel{tab:polyDeg3}{{\M@TitleReference {4.12}{Poly degree 3 kernel results.\relax }}{26}{Poly degree 3 kernel results.\relax }{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.13}{\ignorespaces Poly degree 6 kernel results.\relax }}{26}{table.caption.21}}
\newlabel{tab:polyDeg6}{{\M@TitleReference {4.13}{Poly degree 6 kernel results.\relax }}{26}{Poly degree 6 kernel results.\relax }{table.caption.21}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {4.14}{\ignorespaces Sigmoid kernel results.\relax }}{27}{table.caption.22}}
\newlabel{tab:sigmoid}{{\M@TitleReference {4.14}{Sigmoid kernel results.\relax }}{27}{Sigmoid kernel results.\relax }{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.15}{\ignorespaces RBF kernel results. This option is chosen.\relax }}{27}{table.caption.23}}
\newlabel{tab:RbfOrig}{{\M@TitleReference {4.15}{RBF kernel results. This option is chosen.\relax }}{27}{RBF kernel results. This option is chosen.\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Varying SVM Feature Selection}{27}{subsection.4.1.3}}
\@writefile{brf}{\backcite{scikit-learn}{{27}{4.1.3}{subsection.4.1.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.16}{\ignorespaces SVM select percentile, keep 25\% of features.\relax }}{27}{table.caption.24}}
\newlabel{tab:SVMSel25}{{\M@TitleReference {4.16}{SVM select percentile, keep 25\% of features.\relax }}{27}{SVM select percentile, keep 25\% of features.\relax }{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.17}{\ignorespaces SVM select percentile, keep 50\% of features.\relax }}{28}{table.caption.25}}
\newlabel{tab:SVMSel50}{{\M@TitleReference {4.17}{SVM select percentile, keep 50\% of features.\relax }}{28}{SVM select percentile, keep 50\% of features.\relax }{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.18}{\ignorespaces SVM select percentile, keep 75\% of features. This option is chosen.\relax }}{28}{table.caption.26}}
\newlabel{tab:SVMSel75}{{\M@TitleReference {4.18}{SVM select percentile, keep 75\% of features. This option is chosen.\relax }}{28}{SVM select percentile, keep 75\% of features. This option is chosen.\relax }{table.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Varying Logistic Regression Scaling}{28}{subsection.4.1.4}}
\newlabel{sect:logitParam}{{\M@TitleReference {4.1.4}{Varying Logistic Regression Scaling}}{28}{Varying Logistic Regression Scaling}{subsection.4.1.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.19}{\ignorespaces Logistic Regression - No scaling.\relax }}{28}{table.caption.27}}
\newlabel{tab:LogRegDef}{{\M@TitleReference {4.19}{Logistic Regression - No scaling.\relax }}{28}{Logistic Regression - No scaling.\relax }{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.20}{\ignorespaces Logistic Regression standard scaling.\relax }}{28}{table.caption.28}}
\newlabel{tab:LogRegStandard}{{\M@TitleReference {4.20}{Logistic Regression standard scaling.\relax }}{28}{Logistic Regression standard scaling.\relax }{table.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.21}{\ignorespaces Logistic Regression normalization scaling.\relax }}{28}{table.caption.29}}
\newlabel{tab:LogRegNorm}{{\M@TitleReference {4.21}{Logistic Regression normalization scaling.\relax }}{28}{Logistic Regression normalization scaling.\relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.22}{\ignorespaces Logistic Regression MinMax scaling. This option is chosen.\relax }}{29}{table.caption.30}}
\newlabel{tab:LogRegMinMax}{{\M@TitleReference {4.22}{Logistic Regression MinMax scaling. This option is chosen.\relax }}{29}{Logistic Regression MinMax scaling. This option is chosen.\relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Varying Logistic Regression Feature Selection}{29}{subsection.4.1.5}}
\@writefile{lot}{\contentsline {table}{\numberline {4.23}{\ignorespaces Logistic Regression select percentile 25\%.\relax }}{29}{table.caption.31}}
\newlabel{tab:LogRegSel25}{{\M@TitleReference {4.23}{Logistic Regression select percentile 25\%.\relax }}{29}{Logistic Regression select percentile 25\%.\relax }{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.24}{\ignorespaces Logistic Regression select percentile 50\%.\relax }}{29}{table.caption.32}}
\newlabel{tab:LogRegSel50}{{\M@TitleReference {4.24}{Logistic Regression select percentile 50\%.\relax }}{29}{Logistic Regression select percentile 50\%.\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.25}{\ignorespaces Logistic Regression select percentile 75\%.\relax }}{29}{table.caption.33}}
\newlabel{tab:LogRegSel75}{{\M@TitleReference {4.25}{Logistic Regression select percentile 75\%.\relax }}{29}{Logistic Regression select percentile 75\%.\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.26}{\ignorespaces Logistic Regression select percentile 100\%. This option is chosen.\relax }}{29}{table.caption.34}}
\newlabel{tab:LogRegMinMax}{{\M@TitleReference {4.26}{Logistic Regression select percentile 100\%. This option is chosen.\relax }}{29}{Logistic Regression select percentile 100\%. This option is chosen.\relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Varying Logistic Regression Positive Class Weight and Cost}{30}{subsection.4.1.6}}
\@writefile{lot}{\contentsline {table}{\numberline {4.27}{\ignorespaces Logit weight 4.977, cost 1.0\relax }}{30}{table.caption.35}}
\newlabel{tab:LogRegWtOrig-C1}{{\M@TitleReference {4.27}{Logit weight 4.977, cost 1.0\relax }}{30}{Logit weight 4.977, cost 1.0\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.28}{\ignorespaces Logit weight 4.977, cost 0.1\relax }}{30}{table.caption.36}}
\newlabel{tab:LogRegWtOrig-Cp1}{{\M@TitleReference {4.28}{Logit weight 4.977, cost 0.1\relax }}{30}{Logit weight 4.977, cost 0.1\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.29}{\ignorespaces Logit weight 4.977, cost 10.0\relax }}{30}{table.caption.37}}
\newlabel{tab:LogRegWtOrig-C10}{{\M@TitleReference {4.29}{Logit weight 4.977, cost 10.0\relax }}{30}{Logit weight 4.977, cost 10.0\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.30}{\ignorespaces Logit weight 10.0, cost 1.0\relax }}{30}{table.caption.38}}
\newlabel{tab:LogRegWt10-C1}{{\M@TitleReference {4.30}{Logit weight 10.0, cost 1.0\relax }}{30}{Logit weight 10.0, cost 1.0\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.31}{\ignorespaces Logit weight 10.0, cost 0.1\relax }}{31}{table.caption.39}}
\newlabel{tab:LogRegWt10-Cp1}{{\M@TitleReference {4.31}{Logit weight 10.0, cost 0.1\relax }}{31}{Logit weight 10.0, cost 0.1\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.32}{\ignorespaces Logit weight 10.0, cost 10.0\relax }}{31}{table.caption.40}}
\newlabel{tab:LogRegWt10-C10}{{\M@TitleReference {4.32}{Logit weight 10.0, cost 10.0\relax }}{31}{Logit weight 10.0, cost 10.0\relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.33}{\ignorespaces Logit weight 7.5, cost 1.0\relax }}{31}{table.caption.41}}
\newlabel{tab:LogRegWt7p5-C1}{{\M@TitleReference {4.33}{Logit weight 7.5, cost 1.0\relax }}{31}{Logit weight 7.5, cost 1.0\relax }{table.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.34}{\ignorespaces Logit weight 7.5, cost 0.1. This option is chosen due to showing advantage for the fine classifier compared to the coarse classifier.\relax }}{31}{table.caption.42}}
\newlabel{tab:LogRegWt7p5-Cp1}{{\M@TitleReference {4.34}{Logit weight 7.5, cost 0.1. This option is chosen due to showing advantage for the fine classifier compared to the coarse classifier.\relax }}{31}{Logit weight 7.5, cost 0.1. This option is chosen due to showing advantage for the fine classifier compared to the coarse classifier.\relax }{table.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.35}{\ignorespaces Logit weight 7.5, cost 10.0\relax }}{31}{table.caption.43}}
\newlabel{tab:LogRegWt7p5-C10}{{\M@TitleReference {4.35}{Logit weight 7.5, cost 10.0\relax }}{31}{Logit weight 7.5, cost 10.0\relax }{table.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.7}Varying Logistic Regression Fine Class Weights}{31}{subsection.4.1.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.1}Tune Fine Class 1 Weights}{32}{subsubsection.4.1.7.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.36}{\ignorespaces Logit Class 1 weight ratio 1.0\relax }}{32}{table.caption.44}}
\newlabel{tab:LogRegCls1-Wt1}{{\M@TitleReference {4.36}{Logit Class 1 weight ratio 1.0\relax }}{32}{Logit Class 1 weight ratio 1.0\relax }{table.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.37}{\ignorespaces Logit Class 1 weight ratio 0.5\relax }}{32}{table.caption.45}}
\newlabel{tab:LogRegCls1-Wtp5}{{\M@TitleReference {4.37}{Logit Class 1 weight ratio 0.5\relax }}{32}{Logit Class 1 weight ratio 0.5\relax }{table.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.38}{\ignorespaces Logit Class 1 weight ratio 3.0. This option is chosen.\relax }}{32}{table.caption.46}}
\newlabel{tab:LogRegCls1-Wt3}{{\M@TitleReference {4.38}{Logit Class 1 weight ratio 3.0. This option is chosen.\relax }}{32}{Logit Class 1 weight ratio 3.0. This option is chosen.\relax }{table.caption.46}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.39}{\ignorespaces Logit Class 1 weight ratio 5.0\relax }}{33}{table.caption.47}}
\newlabel{tab:LogRegCls1-Wt5}{{\M@TitleReference {4.39}{Logit Class 1 weight ratio 5.0\relax }}{33}{Logit Class 1 weight ratio 5.0\relax }{table.caption.47}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.2}Tune Fine Class 2 Weights}{33}{subsubsection.4.1.7.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4.40}{\ignorespaces Logit Class 2 weight ratio 1.0. This option is chosen.\relax }}{33}{table.caption.48}}
\newlabel{tab:LogRegCls2-Wt1}{{\M@TitleReference {4.40}{Logit Class 2 weight ratio 1.0. This option is chosen.\relax }}{33}{Logit Class 2 weight ratio 1.0. This option is chosen.\relax }{table.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.41}{\ignorespaces Logit Class 2 weight ratio 0.5\relax }}{33}{table.caption.49}}
\newlabel{tab:LogitCls2-Wtp5}{{\M@TitleReference {4.41}{Logit Class 2 weight ratio 0.5\relax }}{33}{Logit Class 2 weight ratio 0.5\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.42}{\ignorespaces Logit Class 2 weight ratio 1.5\relax }}{33}{table.caption.50}}
\newlabel{tab:LogRegCls2-Wt1p5}{{\M@TitleReference {4.42}{Logit Class 2 weight ratio 1.5\relax }}{33}{Logit Class 2 weight ratio 1.5\relax }{table.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.3}Tune Fine Class 3 Weights}{33}{subsubsection.4.1.7.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.43}{\ignorespaces Logit Class 3 weight ratio 1.0. This option is chosen.\relax }}{34}{table.caption.51}}
\newlabel{tab:LogRegCls3-Wt1}{{\M@TitleReference {4.43}{Logit Class 3 weight ratio 1.0. This option is chosen.\relax }}{34}{Logit Class 3 weight ratio 1.0. This option is chosen.\relax }{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.44}{\ignorespaces Logit Class 3 weight ratio 0.5\relax }}{34}{table.caption.52}}
\newlabel{tab:LogRegCls3-Wtp5}{{\M@TitleReference {4.44}{Logit Class 3 weight ratio 0.5\relax }}{34}{Logit Class 3 weight ratio 0.5\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.45}{\ignorespaces Logit Class 3 weight ratio 1.5\relax }}{34}{table.caption.53}}
\newlabel{tab:LogRegCls3-Wt1p5}{{\M@TitleReference {4.45}{Logit Class 3 weight ratio 1.5\relax }}{34}{Logit Class 3 weight ratio 1.5\relax }{table.caption.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.4}Tune Fine Class 4 Weights}{34}{subsubsection.4.1.7.4}}
\@writefile{lot}{\contentsline {table}{\numberline {4.46}{\ignorespaces Logit Class 4 weight ratio 1.0\relax }}{34}{table.caption.54}}
\newlabel{tab:LogRegCls4-Wt1}{{\M@TitleReference {4.46}{Logit Class 4 weight ratio 1.0\relax }}{34}{Logit Class 4 weight ratio 1.0\relax }{table.caption.54}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.47}{\ignorespaces Logit Class 4 weight ratio 0.5\relax }}{35}{table.caption.55}}
\newlabel{tab:LogRegCls4-Wtp5}{{\M@TitleReference {4.47}{Logit Class 4 weight ratio 0.5\relax }}{35}{Logit Class 4 weight ratio 0.5\relax }{table.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.48}{\ignorespaces Logit Class 4 weight ratio 1.5. This option is chosen.\relax }}{35}{table.caption.56}}
\newlabel{tab:LogRegCls4-Wt1p5}{{\M@TitleReference {4.48}{Logit Class 4 weight ratio 1.5. This option is chosen.\relax }}{35}{Logit Class 4 weight ratio 1.5. This option is chosen.\relax }{table.caption.56}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.49}{\ignorespaces Logit Class 4 weight ratio 2.0\relax }}{35}{table.caption.57}}
\newlabel{tab:LogRegCls4-Wt2}{{\M@TitleReference {4.49}{Logit Class 4 weight ratio 2.0\relax }}{35}{Logit Class 4 weight ratio 2.0\relax }{table.caption.57}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.5}Tune Fine Class 5 Weights}{35}{subsubsection.4.1.7.5}}
\@writefile{lot}{\contentsline {table}{\numberline {4.50}{\ignorespaces Logit Class 5 weight ratio 1.0\relax }}{35}{table.caption.58}}
\newlabel{tab:LogRegCls5-Wt1}{{\M@TitleReference {4.50}{Logit Class 5 weight ratio 1.0\relax }}{35}{Logit Class 5 weight ratio 1.0\relax }{table.caption.58}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.51}{\ignorespaces Logit Class 5 weight ratio 0.5\relax }}{36}{table.caption.59}}
\newlabel{tab:LogRegCls5-Wtp5}{{\M@TitleReference {4.51}{Logit Class 5 weight ratio 0.5\relax }}{36}{Logit Class 5 weight ratio 0.5\relax }{table.caption.59}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.52}{\ignorespaces Logit Class 5 weight ratio 1.5\relax }}{36}{table.caption.60}}
\newlabel{tab:LogRegCls5-Wt1p5}{{\M@TitleReference {4.52}{Logit Class 5 weight ratio 1.5\relax }}{36}{Logit Class 5 weight ratio 1.5\relax }{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.53}{\ignorespaces Logit Class 5 weight ratio 5.0\relax }}{36}{table.caption.61}}
\newlabel{tab:LogRegCls5-Wt5}{{\M@TitleReference {4.53}{Logit Class 5 weight ratio 5.0\relax }}{36}{Logit Class 5 weight ratio 5.0\relax }{table.caption.61}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.54}{\ignorespaces Logit Class 5 weight ratio 10.0. This option is chosen.\relax }}{36}{table.caption.62}}
\newlabel{tab:LogRegCls5-Wt10}{{\M@TitleReference {4.54}{Logit Class 5 weight ratio 10.0. This option is chosen.\relax }}{36}{Logit Class 5 weight ratio 10.0. This option is chosen.\relax }{table.caption.62}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.55}{\ignorespaces LogitCls5-Wt20\relax }}{36}{table.caption.63}}
\newlabel{tab:LogRegCls5-Wt20}{{\M@TitleReference {4.55}{LogitCls5-Wt20\relax }}{36}{LogitCls5-Wt20\relax }{table.caption.63}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.6}Tune Fine Class 6 Weights}{37}{subsubsection.4.1.7.6}}
\@writefile{lot}{\contentsline {table}{\numberline {4.56}{\ignorespaces Logit Class 6 weight ratio 1.0\relax }}{37}{table.caption.64}}
\newlabel{tab:LogRegCls6-Wt1}{{\M@TitleReference {4.56}{Logit Class 6 weight ratio 1.0\relax }}{37}{Logit Class 6 weight ratio 1.0\relax }{table.caption.64}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.57}{\ignorespaces Logit Class 6 weight ratio 0.5\relax }}{37}{table.caption.65}}
\newlabel{tab:LogRegCls6-Wtp5}{{\M@TitleReference {4.57}{Logit Class 6 weight ratio 0.5\relax }}{37}{Logit Class 6 weight ratio 0.5\relax }{table.caption.65}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.58}{\ignorespaces Logit Class 6 weight ratio 2.0. This option is chosen.\relax }}{37}{table.caption.66}}
\newlabel{tab:LogRegCls6-Wt2}{{\M@TitleReference {4.58}{Logit Class 6 weight ratio 2.0. This option is chosen.\relax }}{37}{Logit Class 6 weight ratio 2.0. This option is chosen.\relax }{table.caption.66}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.59}{\ignorespaces Logit Class 6 weight ratio 3.0\relax }}{37}{table.caption.67}}
\newlabel{tab:LogRegCls6-Wt3}{{\M@TitleReference {4.59}{Logit Class 6 weight ratio 3.0\relax }}{37}{Logit Class 6 weight ratio 3.0\relax }{table.caption.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.7}Tune Fine Class 7 Weights}{37}{subsubsection.4.1.7.7}}
\@writefile{lot}{\contentsline {table}{\numberline {4.60}{\ignorespaces Logit Class 7 weight ratio 1.0\relax }}{38}{table.caption.68}}
\newlabel{tab:LogRegCls7-Wt1}{{\M@TitleReference {4.60}{Logit Class 7 weight ratio 1.0\relax }}{38}{Logit Class 7 weight ratio 1.0\relax }{table.caption.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.61}{\ignorespaces Logit Class 7 weight ratio 0.5\relax }}{38}{table.caption.69}}
\newlabel{tab:LogRegCls7-Wtp5}{{\M@TitleReference {4.61}{Logit Class 7 weight ratio 0.5\relax }}{38}{Logit Class 7 weight ratio 0.5\relax }{table.caption.69}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.62}{\ignorespaces Logit Class 7 weight ratio 3.0. This option is chosen.\relax }}{38}{table.caption.70}}
\newlabel{tab:LogRegCls7-Wt3}{{\M@TitleReference {4.62}{Logit Class 7 weight ratio 3.0. This option is chosen.\relax }}{38}{Logit Class 7 weight ratio 3.0. This option is chosen.\relax }{table.caption.70}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.63}{\ignorespaces Logit Class 7 weight ratio 5.0\relax }}{38}{table.caption.71}}
\newlabel{tab:LogRegCls7-Wt5}{{\M@TitleReference {4.63}{Logit Class 7 weight ratio 5.0\relax }}{38}{Logit Class 7 weight ratio 5.0\relax }{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.7.8}Tune Fine Class 8 Weights}{38}{subsubsection.4.1.7.8}}
\@writefile{lot}{\contentsline {table}{\numberline {4.64}{\ignorespaces Logit Class 8 weight ratio 1.0. This option is chosen.\relax }}{39}{table.caption.72}}
\newlabel{tab:LogRegCls8-Wt1}{{\M@TitleReference {4.64}{Logit Class 8 weight ratio 1.0. This option is chosen.\relax }}{39}{Logit Class 8 weight ratio 1.0. This option is chosen.\relax }{table.caption.72}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.65}{\ignorespaces Logit Class 8 weight ratio 0.5\relax }}{39}{table.caption.73}}
\newlabel{tab:LogRegCls8-Wtp5}{{\M@TitleReference {4.65}{Logit Class 8 weight ratio 0.5\relax }}{39}{Logit Class 8 weight ratio 0.5\relax }{table.caption.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.66}{\ignorespaces Logit Class 8 weight ratio 1.5\relax }}{39}{table.caption.74}}
\newlabel{tab:LogRegCls8-Wt1p5}{{\M@TitleReference {4.66}{Logit Class 8 weight ratio 1.5\relax }}{39}{Logit Class 8 weight ratio 1.5\relax }{table.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.8}Varying Logistic Regression Tolerance}{39}{subsection.4.1.8}}
\@writefile{lot}{\contentsline {table}{\numberline {4.67}{\ignorespaces Logit results after fine tuning, effectively had a tolerance of 0.0001\relax }}{39}{table.caption.75}}
\newlabel{tab:LogRegAftFineTune}{{\M@TitleReference {4.67}{Logit results after fine tuning, effectively had a tolerance of 0.0001\relax }}{39}{Logit results after fine tuning, effectively had a tolerance of 0.0001\relax }{table.caption.75}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {4.68}{\ignorespaces Logit Tolerance 0.0001, notice that the fine pr and roc decreased by 0.001, and that the coarse roc decreased by 0.001 upon rerunning, there is some statistical variation in these metrics.\relax }}{40}{table.caption.76}}
\newlabel{tab:LogRegOrig-0001}{{\M@TitleReference {4.68}{Logit Tolerance 0.0001, notice that the fine pr and roc decreased by 0.001, and that the coarse roc decreased by 0.001 upon rerunning, there is some statistical variation in these metrics.\relax }}{40}{Logit Tolerance 0.0001, notice that the fine pr and roc decreased by 0.001, and that the coarse roc decreased by 0.001 upon rerunning, there is some statistical variation in these metrics.\relax }{table.caption.76}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.69}{\ignorespaces Logit Tolerance 0.00001. This option is chosen.\relax }}{40}{table.caption.77}}
\newlabel{tab:LogReg-00001Redo}{{\M@TitleReference {4.69}{Logit Tolerance 0.00001. This option is chosen.\relax }}{40}{Logit Tolerance 0.00001. This option is chosen.\relax }{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.70}{\ignorespaces Logit Tolerance 0.000001\relax }}{40}{table.caption.78}}
\newlabel{tab:LogReg-000001}{{\M@TitleReference {4.70}{Logit Tolerance 0.000001\relax }}{40}{Logit Tolerance 0.000001\relax }{table.caption.78}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.9}Varying Sample Weight On Test Set and Dropping Intermediate ROC Curve Values}{40}{subsection.4.1.9}}
\@writefile{brf}{\backcite{scikit-learn}{{40}{4.1.9}{subsection.4.1.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {4.71}{\ignorespaces Logit sample weights, drop intermediate values True. The default option is chosen.\relax }}{41}{table.caption.79}}
\newlabel{tab:LogRegDefSWDropFalse}{{\M@TitleReference {4.71}{Logit sample weights, drop intermediate values True. The default option is chosen.\relax }}{41}{Logit sample weights, drop intermediate values True. The default option is chosen.\relax }{table.caption.79}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.72}{\ignorespaces Logit no sample weights, drop intermediate values True\relax }}{41}{table.caption.80}}
\newlabel{tab:LogReg-NoSW}{{\M@TitleReference {4.72}{Logit no sample weights, drop intermediate values True\relax }}{41}{Logit no sample weights, drop intermediate values True\relax }{table.caption.80}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.73}{\ignorespaces Logit sample weights, drop intermediate values False\relax }}{41}{table.caption.81}}
\newlabel{tab:LogReg-DropFalse}{{\M@TitleReference {4.73}{Logit sample weights, drop intermediate values False\relax }}{41}{Logit sample weights, drop intermediate values False\relax }{table.caption.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.74}{\ignorespaces Logit no sample weights, drop intermediate values False\relax }}{41}{table.caption.82}}
\newlabel{tab:LogReg-NoSW-DropFalse}{{\M@TitleReference {4.74}{Logit no sample weights, drop intermediate values False\relax }}{41}{Logit no sample weights, drop intermediate values False\relax }{table.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.10}Varying Logistic Regression Positive Class Weight For Full Dataset}{41}{subsection.4.1.10}}
\@writefile{lot}{\contentsline {table}{\numberline {4.75}{\ignorespaces Logit entire dataset, weight 20.887\relax }}{42}{table.caption.83}}
\newlabel{tab:LogRegAllOrig-Wt20p887}{{\M@TitleReference {4.75}{Logit entire dataset, weight 20.887\relax }}{42}{Logit entire dataset, weight 20.887\relax }{table.caption.83}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.76}{\ignorespaces Logit entire dataset, weight 23.0. This option is chosen.\relax }}{42}{table.caption.84}}
\newlabel{tab:LogRegAll-Wt23}{{\M@TitleReference {4.76}{Logit entire dataset, weight 23.0. This option is chosen.\relax }}{42}{Logit entire dataset, weight 23.0. This option is chosen.\relax }{table.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.77}{\ignorespaces Logit entire dataset, weight 25.0.\relax }}{42}{table.caption.85}}
\newlabel{tab:LogRegAll-Wt25}{{\M@TitleReference {4.77}{Logit entire dataset, weight 25.0.\relax }}{42}{Logit entire dataset, weight 25.0.\relax }{table.caption.85}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.11}Varying SVM Cost and Gamma}{42}{subsection.4.1.11}}
\newlabel{sect:paramEnd}{{\M@TitleReference {4.1.11}{Varying SVM Cost and Gamma}}{42}{Varying SVM Cost and Gamma}{subsection.4.1.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.78}{\ignorespaces SVM Cost 1.0 Gamma 0.0029674\relax }}{43}{table.caption.86}}
\newlabel{tab:SVM-C1-Gp0029674}{{\M@TitleReference {4.78}{SVM Cost 1.0 Gamma 0.0029674\relax }}{43}{SVM Cost 1.0 Gamma 0.0029674\relax }{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.79}{\ignorespaces SVM Cost 2.0 Gamma 0.0029674\relax }}{43}{table.caption.87}}
\newlabel{tab:SVM-C2-Gp0029674}{{\M@TitleReference {4.79}{SVM Cost 2.0 Gamma 0.0029674\relax }}{43}{SVM Cost 2.0 Gamma 0.0029674\relax }{table.caption.87}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.80}{\ignorespaces SVM Cost 0.1 Gamma 0.0029674\relax }}{43}{table.caption.88}}
\newlabel{tab:SVM-Cp1-Gp0029674}{{\M@TitleReference {4.80}{SVM Cost 0.1 Gamma 0.0029674\relax }}{43}{SVM Cost 0.1 Gamma 0.0029674\relax }{table.caption.88}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.81}{\ignorespaces SVM Cost 0.05 Gamma 0.0029674\relax }}{43}{table.caption.89}}
\newlabel{tab:SVM-Cp05-Gp0029674}{{\M@TitleReference {4.81}{SVM Cost 0.05 Gamma 0.0029674\relax }}{43}{SVM Cost 0.05 Gamma 0.0029674\relax }{table.caption.89}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.82}{\ignorespaces SVM Cost 0.15 Gamma 0.0029674. This cost option is chosen.\relax }}{44}{table.caption.90}}
\newlabel{tab:SVM-Cp15-Gp0029674}{{\M@TitleReference {4.82}{SVM Cost 0.15 Gamma 0.0029674. This cost option is chosen.\relax }}{44}{SVM Cost 0.15 Gamma 0.0029674. This cost option is chosen.\relax }{table.caption.90}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.83}{\ignorespaces SVM Cost 0.2 Gamma 0.0029674.\relax }}{44}{table.caption.91}}
\newlabel{tab:SVM-Cp2-Gp0029674}{{\M@TitleReference {4.83}{SVM Cost 0.2 Gamma 0.0029674.\relax }}{44}{SVM Cost 0.2 Gamma 0.0029674.\relax }{table.caption.91}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.84}{\ignorespaces SVM Cost 0.15 Gamma 0.002. This option for Cost and Gamma is chosen.\relax }}{44}{table.caption.92}}
\newlabel{tab:SVM-Cp15-Gp002}{{\M@TitleReference {4.84}{SVM Cost 0.15 Gamma 0.002. This option for Cost and Gamma is chosen.\relax }}{44}{SVM Cost 0.15 Gamma 0.002. This option for Cost and Gamma is chosen.\relax }{table.caption.92}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.85}{\ignorespaces SVM Cost 0.15 Gamma 0.001\relax }}{44}{table.caption.93}}
\newlabel{tab:SVM-Cp15-Gp001}{{\M@TitleReference {4.85}{SVM Cost 0.15 Gamma 0.001\relax }}{44}{SVM Cost 0.15 Gamma 0.001\relax }{table.caption.93}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Results and Analysis}{45}{chapter.5}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}SVM and Logit Classifier Performance}{45}{section.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Logit entire dataset results after parameter tuning\relax }}{45}{table.caption.94}}
\newlabel{tab:LogRegAll-Wt23}{{\M@TitleReference {5.1}{Logit entire dataset results after parameter tuning\relax }}{45}{Logit entire dataset results after parameter tuning\relax }{table.caption.94}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces SVM entire dataset results after parameter tuning\relax }}{45}{table.caption.95}}
\newlabel{tab:SVM-All}{{\M@TitleReference {5.2}{SVM entire dataset results after parameter tuning\relax }}{45}{SVM entire dataset results after parameter tuning\relax }{table.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The fine default threshold occurs at a point on the PR curve associated with a higher F-measure score compared to the coarse curves.\relax }}{46}{figure.caption.96}}
\newlabel{fig:LogRegThreshPr}{{\M@TitleReference {5.1}{The fine default threshold occurs at a point on the PR curve associated with a higher F-measure score compared to the coarse curves.\relax }}{46}{The fine default threshold occurs at a point on the PR curve associated with a higher F-measure score compared to the coarse curves.\relax }{figure.caption.96}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Fine has a higher accuracy than coarse at the default threshold for the Logit classifier.\relax }}{47}{figure.caption.97}}
\newlabel{fig:LogRegThreshAcc}{{\M@TitleReference {5.2}{Fine has a higher accuracy than coarse at the default threshold for the Logit classifier.\relax }}{47}{Fine has a higher accuracy than coarse at the default threshold for the Logit classifier.\relax }{figure.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces SVM results for PR curves and F-measure have coarse and fine picking different parts of the curves for their respective thresholds. This results in a slight advantage for fine at the default threshold, similar to the results for the Logit classifier.\relax }}{47}{figure.caption.98}}
\newlabel{fig:SVMThreshPr}{{\M@TitleReference {5.3}{SVM results for PR curves and F-measure have coarse and fine picking different parts of the curves for their respective thresholds. This results in a slight advantage for fine at the default threshold, similar to the results for the Logit classifier.\relax }}{47}{SVM results for PR curves and F-measure have coarse and fine picking different parts of the curves for their respective thresholds. This results in a slight advantage for fine at the default threshold, similar to the results for the Logit classifier.\relax }{figure.caption.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces SVM accuracy results are similar between coarse and fine.\relax }}{48}{figure.caption.99}}
\newlabel{fig:SVMThreshRoc}{{\M@TitleReference {5.4}{SVM accuracy results are similar between coarse and fine.\relax }}{48}{SVM accuracy results are similar between coarse and fine.\relax }{figure.caption.99}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Active vs Passive curves}{48}{section.5.2}}
\newlabel{sect:actpass}{{\M@TitleReference {5.2}{Active vs Passive curves}}{48}{Active vs Passive curves}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Plots for Logistic Regression Active vs Passive curves}{49}{subsection.5.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces The PR-AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with active fine having the highest performance, and Active outperforming Passive for both coarse and fine classifier types.\relax }}{49}{figure.caption.100}}
\newlabel{fig:runActPassLogReg_pr}{{\M@TitleReference {5.5}{The PR-AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with active fine having the highest performance, and Active outperforming Passive for both coarse and fine classifier types.\relax }}{49}{The PR-AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with active fine having the highest performance, and Active outperforming Passive for both coarse and fine classifier types.\relax }{figure.caption.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces The ROC-AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both coarse and fine. Note that active fine ROC curve doesn't converge to the active coarse ROC curve until round 40. This is contrasted to a dominance of the active fine PR curve after round 10.\relax }}{50}{figure.caption.101}}
\newlabel{fig:runActPassLogReg_roc}{{\M@TitleReference {5.6}{The ROC-AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both coarse and fine. Note that active fine ROC curve doesn't converge to the active coarse ROC curve until round 40. This is contrasted to a dominance of the active fine PR curve after round 10.\relax }}{50}{The ROC-AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both coarse and fine. Note that active fine ROC curve doesn't converge to the active coarse ROC curve until round 40. This is contrasted to a dominance of the active fine PR curve after round 10.\relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The accuracy of the classifiers stays at roughly the same rate throughout the rounds; this is due to an effective weighting scheme. Both curves show a dominance of fine over coarse and Active over Passive.\relax }}{51}{figure.caption.102}}
\newlabel{fig:ActiveVsPassiveAccFmesLR}{{\M@TitleReference {5.7}{The accuracy of the classifiers stays at roughly the same rate throughout the rounds; this is due to an effective weighting scheme. Both curves show a dominance of fine over coarse and Active over Passive.\relax }}{51}{The accuracy of the classifiers stays at roughly the same rate throughout the rounds; this is due to an effective weighting scheme. Both curves show a dominance of fine over coarse and Active over Passive.\relax }{figure.caption.102}{}}
\citation{DavisRocPr}
\citation{DavisRocPr}
\citation{DavisRocPr}
\@writefile{brf}{\backcite{DavisRocPr}{{52}{5.2.1}{figure.caption.102}}}
\@writefile{brf}{\backcite{DavisRocPr}{{52}{5.2.1}{figure.caption.102}}}
\@writefile{brf}{\backcite{DavisRocPr}{{52}{5.2.1}{figure.caption.102}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces PR curves for each fold at Round 20\relax }}{53}{figure.caption.103}}
\newlabel{fig:rnd20LogRegPr}{{\M@TitleReference {5.8}{PR curves for each fold at Round 20\relax }}{53}{PR curves for each fold at Round 20\relax }{figure.caption.103}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces ROC curves for each fold at Round 20\relax }}{53}{figure.caption.104}}
\newlabel{fig:rnd20LogRegRoc}{{\M@TitleReference {5.9}{ROC curves for each fold at Round 20\relax }}{53}{ROC curves for each fold at Round 20\relax }{figure.caption.104}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Plots for SVM Active vs Passive curves}{54}{subsection.5.2.2}}
\newlabel{fig:ActiveVsPassivePRSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassivePRSVM}{ on input line 2517}}{Plots for SVM Active vs Passive curves}}{54}{Plots for SVM Active vs Passive curves}{figure.caption.105}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The PR AUC curves for SVM show a slight advantage for active fine, similar to the Logit results.\relax }}{54}{figure.caption.105}}
\newlabel{fig:ActiveVsPassiveROCSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveROCSVM}{ on input line 2527}}{Plots for SVM Active vs Passive curves}}{55}{Plots for SVM Active vs Passive curves}{figure.caption.106}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces The ROC AUC curves for SVM match the Logit results, the convergence of active fine to active coarse takes slightly longer, round 60 compared to round 40.\relax }}{55}{figure.caption.106}}
\citation{bioPoster}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Plots for Fine Fixed Ratio experiments}{56}{section.5.3}}
\newlabel{ffrSection}{{\M@TitleReference {5.3}{Plots for Fine Fixed Ratio experiments}}{56}{Plots for Fine Fixed Ratio experiments}{section.5.3}{}}
\@writefile{brf}{\backcite{bioPoster}{{56}{5.3}{section.5.3}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces For this curve the fine and coarse grain labels both have a cost of 1. The purple 1.0 curve shows that if only fine-grained labels are purchased, the highest performing PR-AUC can be obtained. All FFR ratios end at the same round since the cost of the fine and coarse instances is the same the budget\relax }}{57}{figure.caption.107}}
\newlabel{fig:ParamsFFR_PR_Cost1_rnds0_180}{{\M@TitleReference {5.12}{For this curve the fine and coarse grain labels both have a cost of 1. The purple 1.0 curve shows that if only fine-grained labels are purchased, the highest performing PR-AUC can be obtained. All FFR ratios end at the same round since the cost of the fine and coarse instances is the same the budget\relax }}{57}{For this curve the fine and coarse grain labels both have a cost of 1. The purple 1.0 curve shows that if only fine-grained labels are purchased, the highest performing PR-AUC can be obtained. All FFR ratios end at the same round since the cost of the fine and coarse instances is the same the budget\relax }{figure.caption.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces At fine cost 2, advantage of the higher FFR values decreases but the ordering of the curves remains unchanged.\relax }}{58}{figure.caption.108}}
\newlabel{fig:ParamsFFR_PR_Cost2_rnds0_180}{{\M@TitleReference {5.13}{At fine cost 2, advantage of the higher FFR values decreases but the ordering of the curves remains unchanged.\relax }}{58}{At fine cost 2, advantage of the higher FFR values decreases but the ordering of the curves remains unchanged.\relax }{figure.caption.108}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces At fine cost 4, the highest FFR 1.0 is no longer preferred, the cost is to high for fine instances PR-AUC utility to overcome the PR-AUC increase gained by purchasing more coarse instances.\relax }}{59}{figure.caption.109}}
\newlabel{fig:ParamsFFR_PR_Cost4_rnds0_180}{{\M@TitleReference {5.14}{At fine cost 4, the highest FFR 1.0 is no longer preferred, the cost is to high for fine instances PR-AUC utility to overcome the PR-AUC increase gained by purchasing more coarse instances.\relax }}{59}{At fine cost 4, the highest FFR 1.0 is no longer preferred, the cost is to high for fine instances PR-AUC utility to overcome the PR-AUC increase gained by purchasing more coarse instances.\relax }{figure.caption.109}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces At fine cost 8 the middle FFR values outperform the extreme values for rounds 0 to 180.\relax }}{60}{figure.caption.110}}
\newlabel{fig:ParamsFFR_PR_Cost8_rnds0_180}{{\M@TitleReference {5.15}{At fine cost 8 the middle FFR values outperform the extreme values for rounds 0 to 180.\relax }}{60}{At fine cost 8 the middle FFR values outperform the extreme values for rounds 0 to 180.\relax }{figure.caption.110}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces This shows the iterations continuing through round 500, the curves with the higher fine rates eventually settle to the same end point that the curves with the high rates of coarse labels purchased achieved at previous iterations.\relax }}{61}{figure.caption.111}}
\newlabel{fig:ParamsFFR_PR_Cost8_rnds0_500}{{\M@TitleReference {5.16}{This shows the iterations continuing through round 500, the curves with the higher fine rates eventually settle to the same end point that the curves with the high rates of coarse labels purchased achieved at previous iterations.\relax }}{61}{This shows the iterations continuing through round 500, the curves with the higher fine rates eventually settle to the same end point that the curves with the high rates of coarse labels purchased achieved at previous iterations.\relax }{figure.caption.111}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces The fine cost 8 curves shown expanding the rounds 20-60. If a round budget of 40 occurs than the recommended FFR would be 0.2\relax }}{62}{figure.caption.112}}
\newlabel{fig:ParamsFFR_PR_Cost8_rnds20_60}{{\M@TitleReference {5.17}{The fine cost 8 curves shown expanding the rounds 20-60. If a round budget of 40 occurs than the recommended FFR would be 0.2\relax }}{62}{The fine cost 8 curves shown expanding the rounds 20-60. If a round budget of 40 occurs than the recommended FFR would be 0.2\relax }{figure.caption.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces The fine cost is increased to 16. The cost is to high for the fine label advantage to offset the decreased number of instances purchased.\relax }}{63}{figure.caption.113}}
\newlabel{fig:ParamsFFR_PR_Cost16_rnds0_180}{{\M@TitleReference {5.18}{The fine cost is increased to 16. The cost is to high for the fine label advantage to offset the decreased number of instances purchased.\relax }}{63}{The fine cost is increased to 16. The cost is to high for the fine label advantage to offset the decreased number of instances purchased.\relax }{figure.caption.113}{}}
\citation{bioPoster}
\citation{GeneOntology}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,nuthesis}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Conclusions and Future Work}{64}{chapter.6}}
\@writefile{brf}{\backcite{bioPoster}{{64}{6}{chapter.6}}}
\@writefile{brf}{\backcite{GeneOntology}{{64}{6}{chapter.6}}}
\bibcite{bioPoster}{1}
\bibcite{mitchell}{2}
\bibcite{sklearn-api}{3}
\bibcite{Coursera}{4}
\bibcite{scikit-learn}{5}
\bibcite{ethem}{6}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{65}{section*.115}}
\bibcite{mitoproteome}{7}
\bibcite{UniProt}{8}
\bibcite{PROFEAT1}{9}
\bibcite{Cui2}{10}
\bibcite{PROSO3}{11}
\bibcite{Phobius4}{12}
\bibcite{PSSCP5}{13}
\bibcite{SignalP6}{14}
\bibcite{TMBHunt7}{15}
\bibcite{NetOgly8}{16}
\bibcite{TatP9}{17}
\bibcite{yugi}{18}
\bibcite{Lewis2004}{19}
\bibcite{Auer2002}{20}
\bibcite{mccallum1998improving}{21}
\bibcite{jiang2013}{22}
\bibcite{Rubens2011}{23}
\bibcite{Merialdo2001}{24}
\bibcite{Hofmann2003}{25}
\bibcite{Schohn2000}{26}
\bibcite{Dasgupta2008}{27}
\bibcite{Symons2006}{28}
\bibcite{DavisRocPr}{29}
\bibcite{GeneOntology}{30}
\citation{*}
\memsetcounter{lastsheet}{75}
\memsetcounter{lastpage}{69}
