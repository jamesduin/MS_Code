\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*{\memsetcounter}[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Contents}{v}{section*.1}}
\citation{mitchell}
\citation{scikit-learn}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}{chapter.1}}
\newlabel{chap:aenied}{{\M@TitleReference {1}{Introduction}}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Machine Learning}{1}{section.1.1}}
\@writefile{brf}{\backcite{mitchell}{{1}{1.1}{section.1.1}}}
\@writefile{brf}{\backcite{scikit-learn}{{1}{1.1}{section.1.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Hierarchical Bioinformatics Data Set}{2}{section.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The protein dataset is labeled according to where it originates in the cell. At the root is “mitochondrion”, then there is the sub level labels for if its native to the mitochondria or if it has a separate target compartment specification. The complete tree along with the number of instances belonging to the each label is included in this figure.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:Mitotree}{{\M@TitleReference {1.1}{The protein dataset is labeled according to where it originates in the cell. At the root is “mitochondrion”, then there is the sub level labels for if its native to the mitochondria or if it has a separate target compartment specification. The complete tree along with the number of instances belonging to the each label is included in this figure.\relax }}{2}{The protein dataset is labeled according to where it originates in the cell. At the root is “mitochondrion”, then there is the sub level labels for if its native to the mitochondria or if it has a separate target compartment specification. The complete tree along with the number of instances belonging to the each label is included in this figure.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Coarse Grained vs Fine Grained Trade Off}{2}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Demonstration of a dataset that would benefit from multiple fine-grained learners for each circle type. In order for the coarse-grained learner to have high recall, precision must be scarified and a large amount of false positives returned. By combining fine-grained classifiers the same level of recall can be achieved with a higher level of precision because none of the false positive diamonds will be returned\relax }}{4}{figure.caption.3}}
\newlabel{fig:union}{{\M@TitleReference {1.2}{Demonstration of a dataset that would benefit from multiple fine-grained learners for each circle type. In order for the coarse-grained learner to have high recall, precision must be scarified and a large amount of false positives returned. By combining fine-grained classifiers the same level of recall can be achieved with a higher level of precision because none of the false positive diamonds will be returned\relax }}{4}{Demonstration of a dataset that would benefit from multiple fine-grained learners for each circle type. In order for the coarse-grained learner to have high recall, precision must be scarified and a large amount of false positives returned. By combining fine-grained classifiers the same level of recall can be achieved with a higher level of precision because none of the false positive diamonds will be returned\relax }{figure.caption.3}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Background and Related Work}{5}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Active Learning}{5}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Hierarchical Active Learning}{6}{section.2.2}}
\newlabel{fig:AL2}{{\M@TitleReference {\caption@xref {fig:AL2}{ on input line 270}}{Hierarchical Active Learning}}{6}{Hierarchical Active Learning}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Diagram of HAL approach\relax }}{6}{figure.caption.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Application to Dispatch Dataset}{7}{section.2.3}}
\newlabel{fig:draft-richmond}{{\M@TitleReference {\caption@xref {fig:draft-richmond}{ on input line 296}}{Application to Dispatch Dataset}}{8}{Application to Dispatch Dataset}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Application of HAL demonstrating the benefit of Actively selecting the type of labels to purchase for instances rather than randomly selecting labels to purchase, as in the Passive curves.\relax }}{8}{figure.caption.5}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Experimental Setup}{9}{chapter.3}}
\newlabel{chap:math}{{\M@TitleReference {3}{Experimental Setup}}{9}{Experimental Setup}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Training and Testing Coarse Grain and Fine Grain Classifiers}{9}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces This dataset contains 20098 instances total with 449 features each. An example partitioning is shown, some classes like 1 and 5 contain only 1-2 instances in a given test set. Note there is a heavy class imbalance with approx. 20 negative instances for each positve instance.\relax }}{10}{table.caption.6}}
\newlabel{tab:dataset}{{\M@TitleReference {3.1}{This dataset contains 20098 instances total with 449 features each. An example partitioning is shown, some classes like 1 and 5 contain only 1-2 instances in a given test set. Note there is a heavy class imbalance with approx. 20 negative instances for each positve instance.\relax }}{10}{This dataset contains 20098 instances total with 449 features each. An example partitioning is shown, some classes like 1 and 5 contain only 1-2 instances in a given test set. Note there is a heavy class imbalance with approx. 20 negative instances for each positve instance.\relax }{table.caption.6}{}}
\newlabel{tab:ClassesAll}{{\M@TitleReference {3.1a}{Classes\relax }}{10}{Classes\relax }{table.caption.6}{}}
\newlabel{sub@tab:ClassesAll}{{\M@TitleReference {a}{Classes\relax }}{10}{Classes\relax }{table.caption.6}{}}
\newlabel{tab:partitions}{{\M@TitleReference {3.1b}{Folds\relax }}{10}{Folds\relax }{table.caption.6}{}}
\newlabel{sub@tab:partitions}{{\M@TitleReference {b}{Folds\relax }}{10}{Folds\relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Example of totals for the Train and Test corresponding to when the first fold is held out to be the test set.\relax }}{10}{table.caption.7}}
\newlabel{tab:TrainTest}{{\M@TitleReference {3.2}{Example of totals for the Train and Test corresponding to when the first fold is held out to be the test set.\relax }}{10}{Example of totals for the Train and Test corresponding to when the first fold is held out to be the test set.\relax }{table.caption.7}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces The subset of instances used for tuning classifier paramters contains approximately $1/5^{th}$ and retains all postive instances.\relax }}{11}{table.caption.8}}
\newlabel{tab:subset}{{\M@TitleReference {3.3}{The subset of instances used for tuning classifier paramters contains approximately $1/5^{th}$ and retains all postive instances.\relax }}{11}{The subset of instances used for tuning classifier paramters contains approximately $1/5^{th}$ and retains all postive instances.\relax }{table.caption.8}{}}
\newlabel{tab:ClassesSub}{{\M@TitleReference {3.3a}{Classes Subset\relax }}{11}{Classes Subset\relax }{table.caption.8}{}}
\newlabel{sub@tab:ClassesSub}{{\M@TitleReference {a}{Classes Subset\relax }}{11}{Classes Subset\relax }{table.caption.8}{}}
\newlabel{tab:PartitionsSubset}{{\M@TitleReference {3.3b}{Folds Subset\relax }}{11}{Folds Subset\relax }{table.caption.8}{}}
\newlabel{sub@tab:PartitionsSubset}{{\M@TitleReference {b}{Folds Subset\relax }}{11}{Folds Subset\relax }{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Example totals for the train and test set for the subset of data. The subset of data is used for the majority of the parameter search.\relax }}{11}{table.caption.9}}
\newlabel{tab:subTrainTest}{{\M@TitleReference {3.4}{Example totals for the train and test set for the subset of data. The subset of data is used for the majority of the parameter search.\relax }}{11}{Example totals for the train and test set for the subset of data. The subset of data is used for the majority of the parameter search.\relax }{table.caption.9}{}}
\@writefile{brf}{\backcite{scikit-learn}{{12}{3.1}{table.caption.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces SVM default results without parameter selection or preprocessing. Where Precision Recall area under the curve is (pr), Reciever Operator Characteristic area under the curve is (roc), accuracy is (acc), F1-measure is (f1).\relax }}{12}{table.caption.10}}
\newlabel{tab:SVMDefResStats}{{\M@TitleReference {3.5}{SVM default results without parameter selection or preprocessing. Where Precision Recall area under the curve is (pr), Reciever Operator Characteristic area under the curve is (roc), accuracy is (acc), F1-measure is (f1).\relax }}{12}{SVM default results without parameter selection or preprocessing. Where Precision Recall area under the curve is (pr), Reciever Operator Characteristic area under the curve is (roc), accuracy is (acc), F1-measure is (f1).\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces SVM default results confusion matrix. Where True Negatives is (tn), False Positives is (fp), False Negatives (fn), True Positives is (tp).\relax }}{13}{table.caption.11}}
\newlabel{tab:SVMDefConfMat}{{\M@TitleReference {3.6}{SVM default results confusion matrix. Where True Negatives is (tn), False Positives is (fp), False Negatives (fn), True Positives is (tp).\relax }}{13}{SVM default results confusion matrix. Where True Negatives is (tn), False Positives is (fp), False Negatives (fn), True Positives is (tp).\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces SVM default condensed view of summary performance metrics, each value is the average of 10 folds.\relax }}{13}{table.caption.12}}
\newlabel{tab:SVMDef}{{\M@TitleReference {3.7}{SVM default condensed view of summary performance metrics, each value is the average of 10 folds.\relax }}{13}{SVM default condensed view of summary performance metrics, each value is the average of 10 folds.\relax }{table.caption.12}{}}
\citation{scikit-learn}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Varying SVM Scaling Methods}{14}{subsection.3.1.1}}
\@writefile{brf}{\backcite{scikit-learn}{{14}{3.1.1}{subsection.3.1.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces SVM minmax-scaler results.\relax }}{14}{table.caption.13}}
\newlabel{tab:SVMMinMax}{{\M@TitleReference {3.8}{SVM minmax-scaler results.\relax }}{14}{SVM minmax-scaler results.\relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces SVM norm-scaler results.\relax }}{14}{table.caption.14}}
\newlabel{tab:SVMNorm}{{\M@TitleReference {3.9}{SVM norm-scaler results.\relax }}{14}{SVM norm-scaler results.\relax }{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.10}{\ignorespaces SVM std-scaler results. This option is chosen.\relax }}{14}{table.caption.15}}
\newlabel{tab:SVMStandard}{{\M@TitleReference {3.10}{SVM std-scaler results. This option is chosen.\relax }}{14}{SVM std-scaler results. This option is chosen.\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Varying SVM Kernels}{15}{subsection.3.1.2}}
\@writefile{brf}{\backcite{scikit-learn}{{15}{3.1.2}{subsection.3.1.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.11}{\ignorespaces Linear kernel results.\relax }}{15}{table.caption.16}}
\newlabel{tab:Linear}{{\M@TitleReference {3.11}{Linear kernel results.\relax }}{15}{Linear kernel results.\relax }{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.12}{\ignorespaces Poly degree 3 kernel results.\relax }}{15}{table.caption.17}}
\newlabel{tab:polyDeg3}{{\M@TitleReference {3.12}{Poly degree 3 kernel results.\relax }}{15}{Poly degree 3 kernel results.\relax }{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.13}{\ignorespaces Poly degree 6 kernel results.\relax }}{15}{table.caption.18}}
\newlabel{tab:polyDeg6}{{\M@TitleReference {3.13}{Poly degree 6 kernel results.\relax }}{15}{Poly degree 6 kernel results.\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.14}{\ignorespaces Sigmoid kernel results.\relax }}{15}{table.caption.19}}
\newlabel{tab:sigmoid}{{\M@TitleReference {3.14}{Sigmoid kernel results.\relax }}{15}{Sigmoid kernel results.\relax }{table.caption.19}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {3.15}{\ignorespaces RBF kernel results. This option is chosen.\relax }}{16}{table.caption.20}}
\newlabel{tab:RbfOrig}{{\M@TitleReference {3.15}{RBF kernel results. This option is chosen.\relax }}{16}{RBF kernel results. This option is chosen.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Varying SVM Feature Selection}{16}{subsection.3.1.3}}
\@writefile{brf}{\backcite{scikit-learn}{{16}{3.1.3}{subsection.3.1.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.16}{\ignorespaces SVM select percentile, keep 25\% of features.\relax }}{16}{table.caption.21}}
\newlabel{tab:SVMSel25}{{\M@TitleReference {3.16}{SVM select percentile, keep 25\% of features.\relax }}{16}{SVM select percentile, keep 25\% of features.\relax }{table.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.17}{\ignorespaces SVM select percentile, keep 50\% of features.\relax }}{16}{table.caption.22}}
\newlabel{tab:SVMSel50}{{\M@TitleReference {3.17}{SVM select percentile, keep 50\% of features.\relax }}{16}{SVM select percentile, keep 50\% of features.\relax }{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.18}{\ignorespaces SVM select percentile, keep 75\% of features. This option is chosen.\relax }}{16}{table.caption.23}}
\newlabel{tab:SVMSel75}{{\M@TitleReference {3.18}{SVM select percentile, keep 75\% of features. This option is chosen.\relax }}{16}{SVM select percentile, keep 75\% of features. This option is chosen.\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Varying Logistic Regression Scaling}{17}{subsection.3.1.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3.19}{\ignorespaces Logistic Regression - No scaling.\relax }}{17}{table.caption.24}}
\newlabel{tab:LogRegDef}{{\M@TitleReference {3.19}{Logistic Regression - No scaling.\relax }}{17}{Logistic Regression - No scaling.\relax }{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.20}{\ignorespaces Logistic Regression standard scaling.\relax }}{17}{table.caption.25}}
\newlabel{tab:LogRegStandard}{{\M@TitleReference {3.20}{Logistic Regression standard scaling.\relax }}{17}{Logistic Regression standard scaling.\relax }{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.21}{\ignorespaces Logistic Regression normalization scaling.\relax }}{17}{table.caption.26}}
\newlabel{tab:LogRegNorm}{{\M@TitleReference {3.21}{Logistic Regression normalization scaling.\relax }}{17}{Logistic Regression normalization scaling.\relax }{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.22}{\ignorespaces Logistic Regression MinMax scaling. This option is chosen.\relax }}{17}{table.caption.27}}
\newlabel{tab:LogRegMinMax}{{\M@TitleReference {3.22}{Logistic Regression MinMax scaling. This option is chosen.\relax }}{17}{Logistic Regression MinMax scaling. This option is chosen.\relax }{table.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Varying Logistic Regression Feature Selection}{17}{subsection.3.1.5}}
\@writefile{lot}{\contentsline {table}{\numberline {3.23}{\ignorespaces Logistic Regression select percentile 25\%.\relax }}{18}{table.caption.28}}
\newlabel{tab:LogRegSel25}{{\M@TitleReference {3.23}{Logistic Regression select percentile 25\%.\relax }}{18}{Logistic Regression select percentile 25\%.\relax }{table.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.24}{\ignorespaces Logistic Regression select percentile 50\%.\relax }}{18}{table.caption.29}}
\newlabel{tab:LogRegSel50}{{\M@TitleReference {3.24}{Logistic Regression select percentile 50\%.\relax }}{18}{Logistic Regression select percentile 50\%.\relax }{table.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.25}{\ignorespaces Logistic Regression select percentile 75\%.\relax }}{18}{table.caption.30}}
\newlabel{tab:LogRegSel75}{{\M@TitleReference {3.25}{Logistic Regression select percentile 75\%.\relax }}{18}{Logistic Regression select percentile 75\%.\relax }{table.caption.30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.26}{\ignorespaces Logistic Regression select percentile 100\%. This option is chosen.\relax }}{18}{table.caption.31}}
\newlabel{tab:LogRegMinMax}{{\M@TitleReference {3.26}{Logistic Regression select percentile 100\%. This option is chosen.\relax }}{18}{Logistic Regression select percentile 100\%. This option is chosen.\relax }{table.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Varying Logistic Regression Postive Class Weight and Cost}{18}{subsection.3.1.6}}
\@writefile{lot}{\contentsline {table}{\numberline {3.27}{\ignorespaces LogReg weight 4.977, cost 1.0\relax }}{19}{table.caption.32}}
\newlabel{tab:LogRegWtOrig-C1}{{\M@TitleReference {3.27}{LogReg weight 4.977, cost 1.0\relax }}{19}{LogReg weight 4.977, cost 1.0\relax }{table.caption.32}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.28}{\ignorespaces LogReg weight 4.977, cost 0.1\relax }}{19}{table.caption.33}}
\newlabel{tab:LogRegWtOrig-Cp1}{{\M@TitleReference {3.28}{LogReg weight 4.977, cost 0.1\relax }}{19}{LogReg weight 4.977, cost 0.1\relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.29}{\ignorespaces LogReg weight 4.977, cost 10.0\relax }}{19}{table.caption.34}}
\newlabel{tab:LogRegWtOrig-C10}{{\M@TitleReference {3.29}{LogReg weight 4.977, cost 10.0\relax }}{19}{LogReg weight 4.977, cost 10.0\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.30}{\ignorespaces LogReg weight 10.0, cost 1.0\relax }}{19}{table.caption.35}}
\newlabel{tab:LogRegWt10-C1}{{\M@TitleReference {3.30}{LogReg weight 10.0, cost 1.0\relax }}{19}{LogReg weight 10.0, cost 1.0\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.31}{\ignorespaces LogReg weight 10.0, cost 0.1\relax }}{19}{table.caption.36}}
\newlabel{tab:LogRegWt10-Cp1}{{\M@TitleReference {3.31}{LogReg weight 10.0, cost 0.1\relax }}{19}{LogReg weight 10.0, cost 0.1\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.32}{\ignorespaces LogReg weight 10.0, cost 10.0\relax }}{19}{table.caption.37}}
\newlabel{tab:LogRegWt10-C10}{{\M@TitleReference {3.32}{LogReg weight 10.0, cost 10.0\relax }}{19}{LogReg weight 10.0, cost 10.0\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.33}{\ignorespaces LogReg weight 7.5, cost 1.0\relax }}{20}{table.caption.38}}
\newlabel{tab:LogRegWt7p5-C1}{{\M@TitleReference {3.33}{LogReg weight 7.5, cost 1.0\relax }}{20}{LogReg weight 7.5, cost 1.0\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.34}{\ignorespaces LogReg weight 7.5, cost 0.1. This option is chosen due to showing advantage for the fine classifier compared to the coarse classifier.\relax }}{20}{table.caption.39}}
\newlabel{tab:LogRegWt7p5-Cp1}{{\M@TitleReference {3.34}{LogReg weight 7.5, cost 0.1. This option is chosen due to showing advantage for the fine classifier compared to the coarse classifier.\relax }}{20}{LogReg weight 7.5, cost 0.1. This option is chosen due to showing advantage for the fine classifier compared to the coarse classifier.\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.35}{\ignorespaces LogReg weight 7.5, cost 10.0\relax }}{20}{table.caption.40}}
\newlabel{tab:LogRegWt7p5-C10}{{\M@TitleReference {3.35}{LogReg weight 7.5, cost 10.0\relax }}{20}{LogReg weight 7.5, cost 10.0\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.7}Varying Logistic Regression Tune Fine Class Weights}{20}{subsection.3.1.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.1}Fine Tune Class 1 Weights}{21}{subsubsection.3.1.7.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.36}{\ignorespaces LogReg Class 1 weight ratio 1.0\relax }}{21}{table.caption.41}}
\newlabel{tab:LogRegCls1-Wt1}{{\M@TitleReference {3.36}{LogReg Class 1 weight ratio 1.0\relax }}{21}{LogReg Class 1 weight ratio 1.0\relax }{table.caption.41}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.37}{\ignorespaces LogReg Class 1 weight ratio 0.5\relax }}{21}{table.caption.42}}
\newlabel{tab:LogRegCls1-Wtp5}{{\M@TitleReference {3.37}{LogReg Class 1 weight ratio 0.5\relax }}{21}{LogReg Class 1 weight ratio 0.5\relax }{table.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.38}{\ignorespaces LogReg Class 1 weight ratio 3.0. This option is chosen.\relax }}{21}{table.caption.43}}
\newlabel{tab:LogRegCls1-Wt3}{{\M@TitleReference {3.38}{LogReg Class 1 weight ratio 3.0. This option is chosen.\relax }}{21}{LogReg Class 1 weight ratio 3.0. This option is chosen.\relax }{table.caption.43}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.39}{\ignorespaces LogReg Class 1 weight ratio 5.0\relax }}{21}{table.caption.44}}
\newlabel{tab:LogRegCls1-Wt5}{{\M@TitleReference {3.39}{LogReg Class 1 weight ratio 5.0\relax }}{21}{LogReg Class 1 weight ratio 5.0\relax }{table.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.2}Fine Tune Class 2 Weights}{22}{subsubsection.3.1.7.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.40}{\ignorespaces LogReg Class 2 weight ratio 1.0. This option is chosen.\relax }}{22}{table.caption.45}}
\newlabel{tab:LogRegCls2-Wt1}{{\M@TitleReference {3.40}{LogReg Class 2 weight ratio 1.0. This option is chosen.\relax }}{22}{LogReg Class 2 weight ratio 1.0. This option is chosen.\relax }{table.caption.45}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.41}{\ignorespaces LogReg Class 2 weight ratio 0.5\relax }}{22}{table.caption.46}}
\newlabel{tab:LogRegCls2-Wtp5}{{\M@TitleReference {3.41}{LogReg Class 2 weight ratio 0.5\relax }}{22}{LogReg Class 2 weight ratio 0.5\relax }{table.caption.46}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.42}{\ignorespaces LogReg Class 2 weight ratio 1.5\relax }}{22}{table.caption.47}}
\newlabel{tab:LogRegCls2-Wt1p5}{{\M@TitleReference {3.42}{LogReg Class 2 weight ratio 1.5\relax }}{22}{LogReg Class 2 weight ratio 1.5\relax }{table.caption.47}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.3}Fine Tune Class 3 Weights}{22}{subsubsection.3.1.7.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.43}{\ignorespaces LogReg Class 3 weight ratio 1.0. This option is chosen.\relax }}{22}{table.caption.48}}
\newlabel{tab:LogRegCls3-Wt1}{{\M@TitleReference {3.43}{LogReg Class 3 weight ratio 1.0. This option is chosen.\relax }}{22}{LogReg Class 3 weight ratio 1.0. This option is chosen.\relax }{table.caption.48}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.44}{\ignorespaces LogReg Class 3 weight ratio 0.5\relax }}{23}{table.caption.49}}
\newlabel{tab:LogRegCls3-Wtp5}{{\M@TitleReference {3.44}{LogReg Class 3 weight ratio 0.5\relax }}{23}{LogReg Class 3 weight ratio 0.5\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.45}{\ignorespaces LogReg Class 3 weight ratio 1.5\relax }}{23}{table.caption.50}}
\newlabel{tab:LogRegCls3-Wt1p5}{{\M@TitleReference {3.45}{LogReg Class 3 weight ratio 1.5\relax }}{23}{LogReg Class 3 weight ratio 1.5\relax }{table.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.4}Fine Tune Class 4 Weights}{23}{subsubsection.3.1.7.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3.46}{\ignorespaces LogReg Class 4 weight ratio 1.0\relax }}{23}{table.caption.51}}
\newlabel{tab:LogRegCls4-Wt1}{{\M@TitleReference {3.46}{LogReg Class 4 weight ratio 1.0\relax }}{23}{LogReg Class 4 weight ratio 1.0\relax }{table.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.47}{\ignorespaces LogReg Class 4 weight ratio 0.5\relax }}{23}{table.caption.52}}
\newlabel{tab:LogRegCls4-Wtp5}{{\M@TitleReference {3.47}{LogReg Class 4 weight ratio 0.5\relax }}{23}{LogReg Class 4 weight ratio 0.5\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.48}{\ignorespaces LogReg Class 4 weight ratio 1.5. This option is chosen.\relax }}{23}{table.caption.53}}
\newlabel{tab:LogRegCls4-Wt1p5}{{\M@TitleReference {3.48}{LogReg Class 4 weight ratio 1.5. This option is chosen.\relax }}{23}{LogReg Class 4 weight ratio 1.5. This option is chosen.\relax }{table.caption.53}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.49}{\ignorespaces LogReg Class 4 weight ratio 2.0\relax }}{24}{table.caption.54}}
\newlabel{tab:LogRegCls4-Wt2}{{\M@TitleReference {3.49}{LogReg Class 4 weight ratio 2.0\relax }}{24}{LogReg Class 4 weight ratio 2.0\relax }{table.caption.54}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.5}Fine Tune Class 5 Weights}{24}{subsubsection.3.1.7.5}}
\@writefile{lot}{\contentsline {table}{\numberline {3.50}{\ignorespaces LogReg Class 5 weight ratio 1.0\relax }}{24}{table.caption.55}}
\newlabel{tab:LogRegCls5-Wt1}{{\M@TitleReference {3.50}{LogReg Class 5 weight ratio 1.0\relax }}{24}{LogReg Class 5 weight ratio 1.0\relax }{table.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.51}{\ignorespaces LogReg Class 5 weight ratio 0.5\relax }}{24}{table.caption.56}}
\newlabel{tab:LogRegCls5-Wtp5}{{\M@TitleReference {3.51}{LogReg Class 5 weight ratio 0.5\relax }}{24}{LogReg Class 5 weight ratio 0.5\relax }{table.caption.56}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.52}{\ignorespaces LogReg Class 5 weight ratio 1.5\relax }}{24}{table.caption.57}}
\newlabel{tab:LogRegCls5-Wt1p5}{{\M@TitleReference {3.52}{LogReg Class 5 weight ratio 1.5\relax }}{24}{LogReg Class 5 weight ratio 1.5\relax }{table.caption.57}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.53}{\ignorespaces LogReg Class 5 weight ratio 5.0\relax }}{24}{table.caption.58}}
\newlabel{tab:LogRegCls5-Wt5}{{\M@TitleReference {3.53}{LogReg Class 5 weight ratio 5.0\relax }}{24}{LogReg Class 5 weight ratio 5.0\relax }{table.caption.58}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.54}{\ignorespaces LogReg Class 5 weight ratio 10.0. This option is chosen.\relax }}{25}{table.caption.59}}
\newlabel{tab:LogRegCls5-Wt10}{{\M@TitleReference {3.54}{LogReg Class 5 weight ratio 10.0. This option is chosen.\relax }}{25}{LogReg Class 5 weight ratio 10.0. This option is chosen.\relax }{table.caption.59}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.55}{\ignorespaces LogRegCls5-Wt20\relax }}{25}{table.caption.60}}
\newlabel{tab:LogRegCls5-Wt20}{{\M@TitleReference {3.55}{LogRegCls5-Wt20\relax }}{25}{LogRegCls5-Wt20\relax }{table.caption.60}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.6}Fine Tune Class 6 Weights}{25}{subsubsection.3.1.7.6}}
\@writefile{lot}{\contentsline {table}{\numberline {3.56}{\ignorespaces LogReg Class 6 weight ratio 1.0\relax }}{25}{table.caption.61}}
\newlabel{tab:LogRegCls6-Wt1}{{\M@TitleReference {3.56}{LogReg Class 6 weight ratio 1.0\relax }}{25}{LogReg Class 6 weight ratio 1.0\relax }{table.caption.61}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.57}{\ignorespaces LogReg Class 6 weight ratio 0.5\relax }}{25}{table.caption.62}}
\newlabel{tab:LogRegCls6-Wtp5}{{\M@TitleReference {3.57}{LogReg Class 6 weight ratio 0.5\relax }}{25}{LogReg Class 6 weight ratio 0.5\relax }{table.caption.62}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.58}{\ignorespaces LogReg Class 6 weight ratio 2.0. This option is chosen.\relax }}{25}{table.caption.63}}
\newlabel{tab:LogRegCls6-Wt2}{{\M@TitleReference {3.58}{LogReg Class 6 weight ratio 2.0. This option is chosen.\relax }}{25}{LogReg Class 6 weight ratio 2.0. This option is chosen.\relax }{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.59}{\ignorespaces LogReg Class 6 weight ratio 3.0\relax }}{26}{table.caption.64}}
\newlabel{tab:LogRegCls6-Wt3}{{\M@TitleReference {3.59}{LogReg Class 6 weight ratio 3.0\relax }}{26}{LogReg Class 6 weight ratio 3.0\relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.7}Fine Tune Class 7 Weights}{26}{subsubsection.3.1.7.7}}
\@writefile{lot}{\contentsline {table}{\numberline {3.60}{\ignorespaces LogReg Class 7 weight ratio 1.0\relax }}{26}{table.caption.65}}
\newlabel{tab:LogRegCls7-Wt1}{{\M@TitleReference {3.60}{LogReg Class 7 weight ratio 1.0\relax }}{26}{LogReg Class 7 weight ratio 1.0\relax }{table.caption.65}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.61}{\ignorespaces LogReg Class 7 weight ratio 0.5\relax }}{26}{table.caption.66}}
\newlabel{tab:LogRegCls7-Wtp5}{{\M@TitleReference {3.61}{LogReg Class 7 weight ratio 0.5\relax }}{26}{LogReg Class 7 weight ratio 0.5\relax }{table.caption.66}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.62}{\ignorespaces LogReg Class 7 weight ratio 3.0. This option is chosen.\relax }}{26}{table.caption.67}}
\newlabel{tab:LogRegCls7-Wt3}{{\M@TitleReference {3.62}{LogReg Class 7 weight ratio 3.0. This option is chosen.\relax }}{26}{LogReg Class 7 weight ratio 3.0. This option is chosen.\relax }{table.caption.67}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.63}{\ignorespaces LogReg Class 7 weight ratio 5.0\relax }}{26}{table.caption.68}}
\newlabel{tab:LogRegCls7-Wt5}{{\M@TitleReference {3.63}{LogReg Class 7 weight ratio 5.0\relax }}{26}{LogReg Class 7 weight ratio 5.0\relax }{table.caption.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7.8}Fine Tune Class 8 Weights}{27}{subsubsection.3.1.7.8}}
\@writefile{lot}{\contentsline {table}{\numberline {3.64}{\ignorespaces LogReg Class 8 weight ratio 1.0. This option is chosen.\relax }}{27}{table.caption.69}}
\newlabel{tab:LogRegCls8-Wt1}{{\M@TitleReference {3.64}{LogReg Class 8 weight ratio 1.0. This option is chosen.\relax }}{27}{LogReg Class 8 weight ratio 1.0. This option is chosen.\relax }{table.caption.69}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.65}{\ignorespaces LogReg Class 8 weight ratio 0.5\relax }}{27}{table.caption.70}}
\newlabel{tab:LogRegCls8-Wtp5}{{\M@TitleReference {3.65}{LogReg Class 8 weight ratio 0.5\relax }}{27}{LogReg Class 8 weight ratio 0.5\relax }{table.caption.70}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.66}{\ignorespaces LogReg Class 8 weight ratio 1.5\relax }}{27}{table.caption.71}}
\newlabel{tab:LogRegCls8-Wt1p5}{{\M@TitleReference {3.66}{LogReg Class 8 weight ratio 1.5\relax }}{27}{LogReg Class 8 weight ratio 1.5\relax }{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.8}Varying Logistic Regression Tolerance}{27}{subsection.3.1.8}}
\@writefile{lot}{\contentsline {table}{\numberline {3.67}{\ignorespaces LogReg results after fine tuning, effectively had a tolerance of 0.0001\relax }}{27}{table.caption.72}}
\newlabel{tab:LogRegAftFineTune}{{\M@TitleReference {3.67}{LogReg results after fine tuning, effectively had a tolerance of 0.0001\relax }}{27}{LogReg results after fine tuning, effectively had a tolerance of 0.0001\relax }{table.caption.72}{}}
\citation{scikit-learn}
\@writefile{lot}{\contentsline {table}{\numberline {3.68}{\ignorespaces LogReg Tolerance 0.0001, notice that the fine pr and roc decreased by 0.001, and that the coarse roc decreased by 0.001 upon rerunning, there is some statistical variation in these metrics.\relax }}{28}{table.caption.73}}
\newlabel{tab:LogRegOrig-0001}{{\M@TitleReference {3.68}{LogReg Tolerance 0.0001, notice that the fine pr and roc decreased by 0.001, and that the coarse roc decreased by 0.001 upon rerunning, there is some statistical variation in these metrics.\relax }}{28}{LogReg Tolerance 0.0001, notice that the fine pr and roc decreased by 0.001, and that the coarse roc decreased by 0.001 upon rerunning, there is some statistical variation in these metrics.\relax }{table.caption.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.69}{\ignorespaces LogReg Tolerance 0.0001. This option is chosen.\relax }}{28}{table.caption.74}}
\newlabel{tab:LogReg-00001Redo}{{\M@TitleReference {3.69}{LogReg Tolerance 0.0001. This option is chosen.\relax }}{28}{LogReg Tolerance 0.0001. This option is chosen.\relax }{table.caption.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.70}{\ignorespaces LogReg Tolerance 0.000001\relax }}{28}{table.caption.75}}
\newlabel{tab:LogReg-000001}{{\M@TitleReference {3.70}{LogReg Tolerance 0.000001\relax }}{28}{LogReg Tolerance 0.000001\relax }{table.caption.75}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.9}Varying Sample Weight On Test Set and Dropping Intermediate ROC Curve Values}{28}{subsection.3.1.9}}
\@writefile{brf}{\backcite{scikit-learn}{{28}{3.1.9}{subsection.3.1.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {3.71}{\ignorespaces LogReg sample weights, drop intermediate values True. The default option is chosen.\relax }}{29}{table.caption.76}}
\newlabel{tab:LogRegDefSWDropFalse}{{\M@TitleReference {3.71}{LogReg sample weights, drop intermediate values True. The default option is chosen.\relax }}{29}{LogReg sample weights, drop intermediate values True. The default option is chosen.\relax }{table.caption.76}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.72}{\ignorespaces LogReg no sample weights, drop intermediate values True\relax }}{29}{table.caption.77}}
\newlabel{tab:LogReg-NoSW}{{\M@TitleReference {3.72}{LogReg no sample weights, drop intermediate values True\relax }}{29}{LogReg no sample weights, drop intermediate values True\relax }{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.73}{\ignorespaces LogReg sample weights, drop intermediate values False\relax }}{29}{table.caption.78}}
\newlabel{tab:LogReg-DropFalse}{{\M@TitleReference {3.73}{LogReg sample weights, drop intermediate values False\relax }}{29}{LogReg sample weights, drop intermediate values False\relax }{table.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.74}{\ignorespaces LogReg no sample weights, drop intermediate values False\relax }}{29}{table.caption.79}}
\newlabel{tab:LogReg-NoSW-DropFalse}{{\M@TitleReference {3.74}{LogReg no sample weights, drop intermediate values False\relax }}{29}{LogReg no sample weights, drop intermediate values False\relax }{table.caption.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.10}Varying Logistic Regression Positive Class Weight For Full Dataset}{29}{subsection.3.1.10}}
\@writefile{lot}{\contentsline {table}{\numberline {3.75}{\ignorespaces LogReg entire dataset, weight 20.887\relax }}{30}{table.caption.80}}
\newlabel{tab:LogRegAllOrig-Wt20p887}{{\M@TitleReference {3.75}{LogReg entire dataset, weight 20.887\relax }}{30}{LogReg entire dataset, weight 20.887\relax }{table.caption.80}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.76}{\ignorespaces LogReg entire dataset, weight 23.0. This option is chosen.\relax }}{30}{table.caption.81}}
\newlabel{tab:LogRegAll-Wt23}{{\M@TitleReference {3.76}{LogReg entire dataset, weight 23.0. This option is chosen.\relax }}{30}{LogReg entire dataset, weight 23.0. This option is chosen.\relax }{table.caption.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.77}{\ignorespaces LogReg entire dataset, weight 25.0.\relax }}{30}{table.caption.82}}
\newlabel{tab:LogRegAll-Wt25}{{\M@TitleReference {3.77}{LogReg entire dataset, weight 25.0.\relax }}{30}{LogReg entire dataset, weight 25.0.\relax }{table.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.11}Varying SVM Gamma}{30}{subsection.3.1.11}}
\@writefile{lot}{\contentsline {table}{\numberline {3.78}{\ignorespaces SVM Cost 1.0 Gamma 0.0029674\relax }}{31}{table.caption.83}}
\newlabel{tab:SVM-C1-Gp0029674}{{\M@TitleReference {3.78}{SVM Cost 1.0 Gamma 0.0029674\relax }}{31}{SVM Cost 1.0 Gamma 0.0029674\relax }{table.caption.83}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.79}{\ignorespaces SVM Cost 2.0 Gamma 0.0029674\relax }}{31}{table.caption.84}}
\newlabel{tab:SVM-C2-Gp0029674}{{\M@TitleReference {3.79}{SVM Cost 2.0 Gamma 0.0029674\relax }}{31}{SVM Cost 2.0 Gamma 0.0029674\relax }{table.caption.84}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.80}{\ignorespaces SVM Cost 0.1 Gamma 0.0029674\relax }}{31}{table.caption.85}}
\newlabel{tab:SVM-Cp1-Gp0029674}{{\M@TitleReference {3.80}{SVM Cost 0.1 Gamma 0.0029674\relax }}{31}{SVM Cost 0.1 Gamma 0.0029674\relax }{table.caption.85}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.81}{\ignorespaces SVM Cost 0.05 Gamma 0.0029674\relax }}{31}{table.caption.86}}
\newlabel{tab:SVM-Cp05-Gp0029674}{{\M@TitleReference {3.81}{SVM Cost 0.05 Gamma 0.0029674\relax }}{31}{SVM Cost 0.05 Gamma 0.0029674\relax }{table.caption.86}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.82}{\ignorespaces SVM Cost 0.15 Gamma 0.0029674. This cost option is chosen.\relax }}{31}{table.caption.87}}
\newlabel{tab:SVM-Cp15-Gp0029674}{{\M@TitleReference {3.82}{SVM Cost 0.15 Gamma 0.0029674. This cost option is chosen.\relax }}{31}{SVM Cost 0.15 Gamma 0.0029674. This cost option is chosen.\relax }{table.caption.87}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.83}{\ignorespaces SVM Cost 0.2 Gamma 0.0029674.\relax }}{32}{table.caption.88}}
\newlabel{tab:SVM-Cp2-Gp0029674}{{\M@TitleReference {3.83}{SVM Cost 0.2 Gamma 0.0029674.\relax }}{32}{SVM Cost 0.2 Gamma 0.0029674.\relax }{table.caption.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.12}Varying SVM Cost}{32}{subsection.3.1.12}}
\@writefile{lot}{\contentsline {table}{\numberline {3.84}{\ignorespaces SVM Cost 0.15 Gamma 0.002. This option for Cost and Gamma is chosen.\relax }}{32}{table.caption.89}}
\newlabel{tab:SVM-Cp15-Gp002}{{\M@TitleReference {3.84}{SVM Cost 0.15 Gamma 0.002. This option for Cost and Gamma is chosen.\relax }}{32}{SVM Cost 0.15 Gamma 0.002. This option for Cost and Gamma is chosen.\relax }{table.caption.89}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.85}{\ignorespaces SVM Cost 0.15 Gamma 0.001\relax }}{32}{table.caption.90}}
\newlabel{tab:SVM-Cp15-Gp001}{{\M@TitleReference {3.85}{SVM Cost 0.15 Gamma 0.001\relax }}{32}{SVM Cost 0.15 Gamma 0.001\relax }{table.caption.90}{}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Results and Analysis}{33}{chapter.4}}
\newlabel{chap:aenied}{{\M@TitleReference {4}{Results and Analysis}}{33}{Results and Analysis}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}SVM and LogReg Classifier Performance}{33}{section.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces LogReg entire dataset results after parameter tuning\relax }}{33}{table.caption.91}}
\newlabel{tab:LogRegAll-Wt23}{{\M@TitleReference {4.1}{LogReg entire dataset results after parameter tuning\relax }}{33}{LogReg entire dataset results after parameter tuning\relax }{table.caption.91}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces SVM entire dataset results after parameter tuning\relax }}{33}{table.caption.92}}
\newlabel{tab:SVM-All}{{\M@TitleReference {4.2}{SVM entire dataset results after parameter tuning\relax }}{33}{SVM entire dataset results after parameter tuning\relax }{table.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The Fine default threshold occurs at a point on the PR curve associated with a higher F-measure score compared to the Coarse curves.\relax }}{34}{figure.caption.93}}
\newlabel{fig:LogRegThreshPr}{{\M@TitleReference {4.1}{The Fine default threshold occurs at a point on the PR curve associated with a higher F-measure score compared to the Coarse curves.\relax }}{34}{The Fine default threshold occurs at a point on the PR curve associated with a higher F-measure score compared to the Coarse curves.\relax }{figure.caption.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Fine has a higher accuracy than coarse at the default threshold for the LogReg classifier.\relax }}{35}{figure.caption.94}}
\newlabel{fig:LogRegThreshAcc}{{\M@TitleReference {4.2}{Fine has a higher accuracy than coarse at the default threshold for the LogReg classifier.\relax }}{35}{Fine has a higher accuracy than coarse at the default threshold for the LogReg classifier.\relax }{figure.caption.94}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces SVM results for PR curves and F-measure have Coarse and Fine picking different parts of the curves for their respective thresholds. This results in a slight advantage for Fine at the default threshold, similar to the results for the LogReg classifier.\relax }}{35}{figure.caption.95}}
\newlabel{fig:SVMThreshPr}{{\M@TitleReference {4.3}{SVM results for PR curves and F-measure have Coarse and Fine picking different parts of the curves for their respective thresholds. This results in a slight advantage for Fine at the default threshold, similar to the results for the LogReg classifier.\relax }}{35}{SVM results for PR curves and F-measure have Coarse and Fine picking different parts of the curves for their respective thresholds. This results in a slight advantage for Fine at the default threshold, similar to the results for the LogReg classifier.\relax }{figure.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces SVM accuracy results are similar between Coarse and Fine.\relax }}{36}{figure.caption.96}}
\newlabel{fig:SVMThreshRoc}{{\M@TitleReference {4.4}{SVM accuracy results are similar between Coarse and Fine.\relax }}{36}{SVM accuracy results are similar between Coarse and Fine.\relax }{figure.caption.96}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Active vs Passive curves}{36}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Plots for Logistic Regression Active vs Passive curves}{37}{subsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The PR-AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with Active Fine having the highest performance, and Active outperforming Passive for both Coarse and Fine classifier types.\relax }}{37}{figure.caption.97}}
\newlabel{fig:runActPassLogReg_pr}{{\M@TitleReference {4.5}{The PR-AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with Active Fine having the highest performance, and Active outperforming Passive for both Coarse and Fine classifier types.\relax }}{37}{The PR-AUC curves for rounds with the Logistic Regression classifier conforms to expectations, with Active Fine having the highest performance, and Active outperforming Passive for both Coarse and Fine classifier types.\relax }{figure.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The ROC-AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both Coarse and Fine. Note that Active Fine ROC curve doesn't converge to the Active Coarse ROC curve until round 40. This is contrasted to a dominance of the Active Fine PR curve after round 10.\relax }}{38}{figure.caption.98}}
\newlabel{fig:runActPassLogReg_roc}{{\M@TitleReference {4.6}{The ROC-AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both Coarse and Fine. Note that Active Fine ROC curve doesn't converge to the Active Coarse ROC curve until round 40. This is contrasted to a dominance of the Active Fine PR curve after round 10.\relax }}{38}{The ROC-AUC curves for rounds with the Logistic Regression classifier. The active curves beat out the passive curves for both Coarse and Fine. Note that Active Fine ROC curve doesn't converge to the Active Coarse ROC curve until round 40. This is contrasted to a dominance of the Active Fine PR curve after round 10.\relax }{figure.caption.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces The accuracy of the classifiers stays at roughly the same rate throughout the rounds, this is due to an effective weighting scheme. Both curves show a dominance of Fine over Coarse and Active over Passive.\relax }}{39}{figure.caption.99}}
\newlabel{fig:ActiveVsPassiveAccFmesLR}{{\M@TitleReference {4.7}{The accuracy of the classifiers stays at roughly the same rate throughout the rounds, this is due to an effective weighting scheme. Both curves show a dominance of Fine over Coarse and Active over Passive.\relax }}{39}{The accuracy of the classifiers stays at roughly the same rate throughout the rounds, this is due to an effective weighting scheme. Both curves show a dominance of Fine over Coarse and Active over Passive.\relax }{figure.caption.99}{}}
\citation{DavisRocPr}
\citation{DavisRocPr}
\citation{DavisRocPr}
\@writefile{brf}{\backcite{DavisRocPr}{{40}{4.2.1}{figure.caption.99}}}
\@writefile{brf}{\backcite{DavisRocPr}{{40}{4.2.1}{figure.caption.99}}}
\@writefile{brf}{\backcite{DavisRocPr}{{40}{4.2.1}{figure.caption.99}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces PR curves for each fold at Round 20\relax }}{41}{figure.caption.100}}
\newlabel{fig:rnd20LogRegPr}{{\M@TitleReference {4.8}{PR curves for each fold at Round 20\relax }}{41}{PR curves for each fold at Round 20\relax }{figure.caption.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces ROC curves for each fold at Round 20\relax }}{41}{figure.caption.101}}
\newlabel{fig:rnd20LogRegRoc}{{\M@TitleReference {4.9}{ROC curves for each fold at Round 20\relax }}{41}{ROC curves for each fold at Round 20\relax }{figure.caption.101}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Plots for SVM Active vs Passive curves}{42}{subsection.4.2.2}}
\newlabel{fig:ActiveVsPassivePRSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassivePRSVM}{ on input line 2046}}{Plots for SVM Active vs Passive curves}}{42}{Plots for SVM Active vs Passive curves}{figure.caption.102}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces The PR AUC curves for SVM show a slight advantage for Active Fine, similar to the LogReg results.\relax }}{42}{figure.caption.102}}
\newlabel{fig:ActiveVsPassiveROCSVM}{{\M@TitleReference {\caption@xref {fig:ActiveVsPassiveROCSVM}{ on input line 2056}}{Plots for SVM Active vs Passive curves}}{43}{Plots for SVM Active vs Passive curves}{figure.caption.103}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The ROC AUC curves for SVM match the LogReg results, the convergence of Active Fine to Active Coarse takes slightly longer, round 60 commpared to round 40.\relax }}{43}{figure.caption.103}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Plots for Fine Fixed Ratio experiments}{44}{section.4.3}}
\newlabel{ffrSection}{{\M@TitleReference {4.3}{Plots for Fine Fixed Ratio experiments}}{44}{Plots for Fine Fixed Ratio experiments}{section.4.3}{}}
\newlabel{fig:ParamsFFR_PR_Cost1_rnds0_180}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost1_rnds0_180}{ on input line 2112}}{Plots for Fine Fixed Ratio experiments}}{45}{Plots for Fine Fixed Ratio experiments}{figure.caption.104}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces For this curve the fine and coarse grain labels both have a cost of 1. The purple 1.0 curve shows that if only fine-grained labels are purchased, the highest performing PR-AUC can be obtained. All FFR ratios end at the same round since the cost of the Fine and Coarse instances is the same the budget\relax }}{45}{figure.caption.104}}
\newlabel{fig:ParamsFFR_PR_Cost2_rnds0_180}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost2_rnds0_180}{ on input line 2125}}{Plots for Fine Fixed Ratio experiments}}{46}{Plots for Fine Fixed Ratio experiments}{figure.caption.105}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces At fine cost 2, advantage of the higher FFR values decreases but the ordering of the curves remains unchanged.\relax }}{46}{figure.caption.105}}
\newlabel{fig:ParamsFFR_PR_Cost4_rnds0_180}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost4_rnds0_180}{ on input line 2137}}{Plots for Fine Fixed Ratio experiments}}{47}{Plots for Fine Fixed Ratio experiments}{figure.caption.106}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces At fine cost 4, the highest FFR 1.0 is no longer preferred, the cost is to high for Fine instances PR-AUC utility to overcome the PR-AUC increase gained by purchasing more Coarse instances.\relax }}{47}{figure.caption.106}}
\newlabel{fig:ParamsFFR_PR_Cost8_rnds0_180}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost8_rnds0_180}{ on input line 2149}}{Plots for Fine Fixed Ratio experiments}}{48}{Plots for Fine Fixed Ratio experiments}{figure.caption.107}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces At fine cost 8 the middle FFR values outperform the extreme values for rounds 0 to 180.\relax }}{48}{figure.caption.107}}
\newlabel{fig:ParamsFFR_PR_Cost8_rnds0_500}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost8_rnds0_500}{ on input line 2159}}{Plots for Fine Fixed Ratio experiments}}{49}{Plots for Fine Fixed Ratio experiments}{figure.caption.108}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces This shows the iterations continuing through round 500, the curves with the higher fine rates eventually settle to the same end point that the curves with the high rates of coarse labels purchased achieved at previous iterations.\relax }}{49}{figure.caption.108}}
\newlabel{fig:ParamsFFR_PR_Cost8_rnds20_60}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost8_rnds20_60}{ on input line 2171}}{Plots for Fine Fixed Ratio experiments}}{50}{Plots for Fine Fixed Ratio experiments}{figure.caption.109}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces The fine cost 8 curves shown expanding the rounds 20-60. If a round budget of 40 occurs than the recommended Fine Fixed Ratio would be 0.2\relax }}{50}{figure.caption.109}}
\newlabel{fig:ParamsFFR_PR_Cost16_rnds0_180}{{\M@TitleReference {\caption@xref {fig:ParamsFFR_PR_Cost16_rnds0_180}{ on input line 2182}}{Plots for Fine Fixed Ratio experiments}}{51}{Plots for Fine Fixed Ratio experiments}{figure.caption.110}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces The fine cost is increased to 16. The cost is to high for the fine label advantage to offset the decreased number of instances purchased.\relax }}{51}{figure.caption.110}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,nuthesis}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Conclusions and Future Work}{52}{chapter.5}}
\newlabel{chap:math}{{\M@TitleReference {5}{Conclusions and Future Work}}{52}{Conclusions and Future Work}{chapter.5}{}}
\bibcite{mitchell}{1}
\bibcite{scikit-learn}{2}
\bibcite{DavisRocPr}{3}
\bibcite{lsu-lsal-13}{4}
\bibcite{Dasgupta2008}{5}
\bibcite{Merialdo2001}{6}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{53}{section*.112}}
\bibcite{Ling2012fine}{7}
\bibcite{sklearn-api}{8}
\bibcite{bioPoster}{9}
\citation{*}
\memsetcounter{lastsheet}{60}
\memsetcounter{lastpage}{54}
